\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
%\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=scriptsize,labelfont=sf,textfont=rm]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
% \usepackage{tabularx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{makecell}

\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{ragged2e}
\usepackage{latexsym, amssymb, verbatim, amsmath}
\usepackage{amsmath,bm}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{subeqnarray}
\usepackage{cases}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{color}
\usepackage{cite}
\usepackage[]{chapterbib}
\usepackage{enumerate}
\usepackage{threeparttable}


\usepackage[dvipsnames, svgnames, x11names]{xcolor}
\usepackage{algorithm,algpseudocode,float}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\begin{document}

\title{SecFFT: Safeguarding Federated Fine-tuning for Large Vision Language Models against Covert Backdoor Attacks in IoRT Networks}
 
\author{Zan Zhou, %~\IEEEmembership{Student Member,~IEEE}, 
  Changqiao Xu, Bo Wang, Tengfei Li, Sizhe Huang, Shujie Yang$^{\ast}$\thanks{*Corresponding authors}, Su Yao$^{\ast}$
% <-this % stops a space
\thanks{Z. Zhou, B. Wang, T. Li, S. Huang, C. Xu, and S. Yang are with the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, P.R. China. E-mail: \{zan.zhou, cqxu, bowang, tf, swhsz, sjyang\}@bupt.edu.cn. 
	}  % <-this % stops a space
%\thanks{Y. Zhuang is with the Research Institute of China Telecom, Ave Zhongshan, Guangzhou 510630, China. E-mail: 13316094433@chinatelecom.cn.}% <-this % stops a space
% \thanks{L. Zhong is with the Information Engineering College, Capital Normal University, Beijing 100048, China. E-mail: zhonglj@cnu.edu.cn (Corresponding author).}% <-this % stops a space
\thanks{Su Yao is with the Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing 100190, China. E-mail: yaosu@tsinghua.edu.cn}% <-this % stops a space
%\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
%随着大模型与具身智能机器人网络的迅速发展，以及在智慧城市、电网、工厂、交通运输等领域的应用，视觉感知理解成为了突破具身智能节点性能瓶颈的基础性要素之一。而由于通用的预训练模型并不能很好应对特定任务，联邦微调技术由于其能够最大限度利用离散节点数据、算力的优势，成为了提升视觉感知模型能力的热带技术。然而，随着多种针对性高级持续性威胁的涌现，现有的污染防治方案，并不能有效应对这一新场景下的隐蔽后门攻击。本文因此提出SecFFT方法，其核心在于针对攻击隐蔽性高与攻击策略复杂的问题，分别设计了基于频域分布一致性提取的瞬时攻击行为识别，和基于隐藏攻击意图检测的长效安全聚合机制。从最终目的出发，有效限缩了智能化攻击者通过隐藏自身行为来绕过防御方案的可行域。在公共数据集上的实验表明，我们所提出的方法能够显著提升面对后门攻击时的防御成功率、模型性能和识别精度，尤其是面对具有多轮次复杂策略的高隐蔽后门范式。
As the large vision language models and embodied intelligent robotic networks continue to advance at a remarkable pace, particularly in applications spanning smart cities, power grids, factories, and transportation, visual perception and understanding have emerged as foundational elements to overcoming performance limitations in such intelligent systems. However, since general pre-trained models are not well-suited to specific tasks, federated fine-tuning (FFT) has gained attention as a promising technique for enhancing the performance of vision-based perception models by leveraging data and computational power distributed across nodes. The rise of advanced persistent threats has revealed significant vulnerabilities in existing defense mechanisms, which struggle to mitigate sophisticated backdoor attacks toward FFT for LVLMs. To address these challenges, this paper proposes the SecFFT method, which tackles both the stealthiness and complexity of backdoor strategies. The approach incorporates instantaneous attack behavior detection based on frequency-domain distribution consistency and introduces a long-term secure aggregation mechanism aimed at identifying hidden attack intentions. These strategies effectively limit the feasibility of adversaries attempting to bypass defense measures by concealing their behaviors. Experiments conducted on public datasets demonstrate that SecFFT significantly improves defense success rates, model performance, and detection accuracy, particularly in response to highly covert, multi-round backdoor attacks.



\end{abstract}

\begin{IEEEkeywords}
Vision Language Model, Federated learning, Fine-tuning, Security, Backdoor, Internet of Robotic Things.
\end{IEEEkeywords}

\section{Introduction}
%\IEEEPARstart{T}{his} file is intended to serve as a ``sample article file'' for IEEE journal papers produced under \LaTeX\ using IEEEtran.cls version 1.8b and later. The most common elements are covered in the simplified and updated instructions in ``New\_IEEEtran\_how-to.pdf''. For less common elements you can refer back to the original ``IEEEtran\_HOWTO.pdf''. It is assumed that the reader has a basic working knowledge of \LaTeX. Those who are new to \LaTeX \ are encouraged to read Tobias Oetiker's ``The Not So Short Introduction to \LaTeX ,'' available at: \url{http://tug.ctan.org/info/lshort/english/lshort.pdf} which provides an overview of working with \LaTeX.
%FFL很重要
%作为的一种，

%随着具身智能技术的迅猛进步和在智慧城市、智能交通、无人机等领域的广泛应用，机器人节点从执行等单一任务的agent，逐步向集合了感知交互和自主决策的智能体进行了演进，这也成为了未来IORT网络的重要发展趋势之一。而视觉大模型作为一种新兴热门的图像感知和物体识别技术，能够成为支撑智能机器人对周围环境进行认识和理解不可或缺的基础。
With the rapid advancements of embodied intelligence technology and its widespread application in multiple areas such as smart cities, intelligent transportation, and unmanned aerial vehicles, robotic nodes have evolved from agents performing singular tasks to intelligent entities that integrate perception, interaction, and autonomous decision-making. This evolution has become one of the critical developmental trends for Internet of Robotic Things (IoRT) networks\cite{andronie2023big}. Meanwhile, the Large Vision Language Model (LVLM)\cite{liu2024survey}, as an emerging and popular image perception and object recognition technology, has the potential to support intelligent robots in understanding and interpreting their surrounding environments. 

%视觉大模型极大地增强了机器人的视觉语义理解能力，使其能够对多模态任务进行更加智能化的决策响应，然而在真实应用环境中，仍存在以下问题：1）由于真实世界的多样性和异构性，通用大模型在特定任务上往往无法较好迁移通用知识，需要进行微调才能显著提升识别精度（例如，针对动目标检测任务，交通机器人主要针对车辆、行人，而工厂机器人则主要针对机床，两者轮廓、行为模式范围均存在较大差异）；2）此外，将来自不同源头的离散化、碎片化分布的海量数据进行传输并汇聚于主服务器，将会导致巨大的通信开销和时间延迟，无法满足具身智能业务的需求，且存在潜在的隐私风险。
Although LVLMs significantly enhance the visual semantic understanding capabilities of robots, enabling them to make more intelligent decisions in multi-modal tasks, there are still challenges in real-world applications: (1) Due to the diversity and heterogeneity of the real world\cite{dai2023tackling}, general foundation models often cannot effectively transfer their general knowledge to specific tasks, necessitating fine-tuning\cite{fu2023effectiveness} to significantly improve recognition accuracy (for example, traffic robots primarily focus on vehicles and pedestrians for moving object detection tasks, while factory robots mainly target machine tools, with significant differences in their contours and behavioral patterns); (2) Additionally, transmitting and aggregating massive amounts of discretized and fragmented data from different sources to a central server can result in significant communication overhead and latency\cite{zhao2023towards}, failing to meet the demands of embodied intelligence applications and posing potential privacy risks\cite{wen2023survey, khan2023federated}. 

%因此，如图1所示，在pretrained的通用视觉大模型的基础上，不同于核心节点汇聚所有数据而后训练垂域模型的集中式微调，通过联邦微调技术，驱使大量机器人节点利用私有数据和算力计算微调梯度而后聚合，能够在不交互原始数据的前提下，充分利用整个IORT网络中各个节点的知识和算力资源，成为了一种非常具有前景的解决方案。
As illustrated in Figure \ref{fig1:scene}, a promising solution is to employ federated fine-tuning (FFT) technology rather than adopting a centralized fine-tuning approach where core nodes aggregate all data before training domain-specific models. This approach drives numerous robotic nodes to compute fine-tuning gradients using private data and computational resources and then aggregate these gradients, effectively leveraging the knowledge and computational resources of each node in the IoRT network without exchanging raw data.

%不幸的是，FFT的分布式特性为多种后门攻击提供了存续的空间。尽管多种防御手段相继提出，但是尽我们所知，目前尚未有针对LVM的FFT过程进行污染防治的后门对抗技术提出。现有技术大多是针对普通联邦学习场景或集中式的VLM训练进行设计，这暴露出两大核心问题：（1）高隐蔽的攻击行为感知困难：攻击者在设计攻击行为时，尽量在行为特征上拟合正常用户，perception的区分度越来越narrow；(2)复杂攻击策略的检测困难：为了规避防御，攻击者可能采用限制角度、大小、符号等方式，将原有单次攻击转化为多轮次长时间尺度的持续性威胁，攻击识别更加困难。
Unfortunately, the distributed nature of FFT provides a persistent space for various backdoor attacks. While several defense strategies have been proposed, to the best of our knowledge, no countermeasures specifically address backdoor attacks in the FFT process for LLVMs. Most existing techniques are designed for conventional federated learning scenarios or centralized VLM training, exposing two critical issues: (1) \textbf{The high stealthy nature of attack behavior:} Attackers increasingly design their actions to mimic normal users, narrowing the perception’s distinguishing capability\cite{xu2024shadowcast, zhang2024a3fl}; and (2) \textbf{The complexity of detecting sophisticated attack strategies:} To evade defenses, attackers may employ techniques such as limiting angle, magnitude, or sign, transforming a single attack into a multi-round, long-term threat, further complicating attack detection\cite{geisler2024attacking, chen2024optimal, sagliano2024powered, dong2023adaptive,wan2023average}.

% 因此，在本文中，我们针对LVLM的FFT过程，提出了一种新型的抗backdoor防御方案。针对攻击行为感知难题，我们创新地从频域分布一致性入手，将隐性的梯度更新分布差异而非显性的更新离群性作为新的鉴别角度，从而实现对高隐蔽攻击的感知；在此基础上，进一步针对智能化攻击者可能采取的复杂多轮次策略，我们创新地提出了基于意图识别的长效评估架构，有效分离高级持续性威胁，并据此设计安全聚合方法。
% 主要包括以下几方面贡献：
Therefore, in this paper, we propose a novel backdoor defense scheme tailored for the FFT process of LVLMs. To address the challenge of detecting attack behaviors, we introduce a groundbreaking approach based on consistency in frequency domain distributions. Rather than focusing on explicit gradient outliers, we leverage the implicit differences in gradient update distributions as a new identification criterion, enabling the detection of highly covert attacks. Building on this, we further address the issue of complex multi-round strategies employed by intelligent attackers by proposing an innovative long-term evaluation framework based on intent recognition. This framework effectively isolates advanced persistent threats and informs the design of a secure aggregation method.

The main contributions of this scheme include:
\begin{enumerate}
    \item Based on an in-depth analysis of emerging covert backdoor attacks, we first define two core challenges: detecting highly covert attack behaviors and identifying complex attack strategies. We then provide a case study of these issues within the FFT process of large vision models. To the best of our knowledge, this is the first framework capable of recognizing deep-level attack intentions during the fine-tuning process in IoRT networks, effectively defending against covert attacks.
    \item In response to the potential threat of intelligent attackers fragmenting their behaviors to evade detection, we propose an innovative method for constructing attack intentions based on historical behavior sequences. By building an approximate minimal covering hypersphere from these behaviors, the malicious intent hidden behind seemingly disordered, benign actions is fully exposed. 
    \item SecFFT also adopts a novel nonlinear defense strategy, dynamically adjusting aggregation weights using local outlier factors while ensuring fairness for benign nodes and effectively eliminating malicious ones. A series of experiments on public datasets demonstrate that our approach significantly improves model integrity, effectively suppresses covert backdoor attacks, and maintains a low false positive rate.
\end{enumerate}
% 基于对现有新型隐蔽后门攻击的分析，我们首先定义了高隐蔽攻击行为感知和复杂攻击策略识别这两大核心问题，并基于视觉大模型的FFT过程进行实例分析。据我们所知，这是首个在IORT网络中能够实现对微调过程中深层次攻击目的识别，从而抵御高隐蔽攻击的方案。
% 针对智能攻击者可能将攻击行为切片化拆解从而规避防御检测的潜在威胁，我们创新地设计了基于历史行为序列进行攻击意图构建的方法。通过构建历史行为的近似最小覆盖超球，将智能攻击者隐藏在看似杂乱无序、良性无害的攻击行为背后的恶意意图充分暴露。
% 基于意图识别架构，SecFFT能够设计一种非线性的新型防御策略，利用局部离群因子动态调整聚合权重，同时保障良性用户公平性和恶意用户的消除。公有数据集上的系列实验表明，我们的方法能够显著提升模型完整性，有效遏制隐蔽后门攻击，并同时保证极低的误判率。

The remainder of this paper is organized as follows: Section \uppercase\expandafter{\romannumeral2} reviews related works. Section \uppercase\expandafter{\romannumeral3} and \uppercase\expandafter{\romannumeral4} present the system and threat models, respectively. Section \uppercase\expandafter{\romannumeral6} elaborates on the design philosophy of the proposed SecFFT solution, providing comprehensive details on the availability, security, and integrity of FFT in IoRT networks. Section \uppercase\expandafter{\romannumeral7} presents the experimental results. Finally, Section \uppercase\expandafter{\romannumeral8} concludes this paper and briefly explores future directions.

%攻击识别
% Alongside the conti
% Spectral Distribution
% Consistency Verification
% Poisoning with 
% Detection

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/SecFFT-scenario.pdf}
    \caption{FFT process for large vision language models in IoRT network}
    \label{fig1:scene}
    %\vspace{-0.7cm}
\end{figure}

\section{Related Works}
\subsection{Covert backdoor attacks}
%后门攻击对深度学习模型构成了严重的安全威胁，它旨在通过对模型输入添加特殊扰动来诱导目标模型产生不当行为，例如图像分类器中的错误分类等。我们将现有针对模型的后门攻击分为两种，即空间隐蔽性后门攻击和时间隐蔽性后门攻击。
Backdoor attacks pose a significant security threat to deep learning models, aiming to induce improper behavior in the target model by adding specific perturbations to the input, such as misclassification in image classifiers. We classify existing backdoor attacks on models into two types: spatially covert backdoor attacks and temporally covert backdoor attacks.

% 空间隐蔽性后门攻击是指攻击者通过微调模型参数、修改输入特征或操纵模型训练过程等方式，使得现有防御方法无法在特征空间中有效区分恶意攻击者与良性参与者，从而有效逃避检测。EDGE CASE\cite{EDGE_CASE}提出了一种针对联邦学习的边缘案例后门攻击方法，通过在训练和测试数据集中通常不存在但自然存在的输入数据点（即边缘样本）来注入后门。Shadowcast\cite{xu2024shadowcast}通过在特征空间中引入微小、不可察觉的扰动，操纵视觉语言模型（VLMs）在特定视觉输入下生成误导性的文本输出。Flip\cite{jha2023label}通过修改训练数据的标签，使得模型训练轨迹接近于带有后门的专家模型，从而在特征空间中达到隐蔽攻击的效果，即使在图像干净的情况下也能逃避现有检测方法。
\textbf{Spatially Covert Backdoor Attacks} refer to attacks where adversaries fine-tune model parameters, modify input features, or manipulate the model training process in a way that existing defense cannot effectively distinguish between malicious attackers and benign participants in the feature space, thereby evading detection. EDGE CASE\cite{EDGE_CASE} proposes an edge-case backdoor attack method for federated learning by injecting backdoors using input data points that are naturally occurring but typically absent in both training and testing datasets (i.e., edge-case samples). Shadowcast \cite{xu2024shadowcast} manipulates VLMs to produce misleading textual outputs for specific visual inputs by introducing subtle, imperceptible perturbations in the feature space. Flip \cite{jha2023label} modifies the labels of training data so that the model's training trajectory approximates that of an expert model with a backdoor. This hidden process completes covert attacks in the feature space, thus evading existing detection methods even when the images are clean.

% 时间隐蔽性后门攻击是指攻击者通过调整模型训练过程或优化后门植入等方式，使后门在恶意攻击者结束攻击后仍能长期存在，不被后续良性更新所覆盖，从而持续影响模型行为。 Neurotoxin\cite{zhang2022neurotoxin}提出了一种针对联邦学习的更持久的后门攻击方法，通过攻击在训练过程中变化较小的模型参数来增加后门的耐久性，避免与良性用户梯度的冲突，使其不易在后续的模型更新中被清除。ImgTrojan\cite{tao2024imgtrojan}通过在视觉语言模型的训练数据中注入少量恶意样本，使得这些后门即使在模型后续的良性训练中仍能保持有效，从而持续影响模型的行为。\cite{gu2023gradient}提出了一种新的更新控制方法，通过跨层更新幅度归一化和层内更新方向投影，解决了参数高效调优（PET）过程中后门攻击的遗忘问题，从而维持攻击的持久性和隐蔽性。A3FL\cite{zhang2024a3fl}通过对抗性适应损失函数来优化触发器的方式，使得后门能够在全局训练动态中持久存在，从而在联邦学习模型中实现高效且持久的攻击效果。
% Chameleon \cite{dai2023chameleon} focuses on enhancing the spatial covert nature of backdoor attacks by adjusting the relationships between poisoned samples, interference samples, and promoting samples in contrastive learning.
\textbf{Temporally Covert Backdoor Attacks} involve adjusting the model training process or optimizing backdoor implantation such that the backdoor persists even after the malicious attacker has stopped the attack, remaining undetected by subsequent benign updates, and continuing to affect the model’s behavior. NEUR\cite{zhang2022neurotoxin} proposes a more persistent backdoor attack method for federated learning by targeting model parameters that change less during training. This approach increases the durability of the backdoor, avoids conflicts with the gradients of benign nodes, and makes it less likely to be removed in subsequent model updates. ImgTrojan \cite{tao2024imgtrojan} injects a small number of malicious samples into the training data of vision-language models, ensuring that these backdoors remain effective even after subsequent benign training, thereby continuously affecting the model's behavior. \cite{gu2023gradient} proposes a novel update control method that normalizes cross-layer update magnitudes and projects intra-layer update directions to address the issue of backdoor forgetting during parameter-efficient tuning (PET), thus maintaining the persistence and concealment of the attack. A3FL \cite{zhang2024a3fl} optimizes the trigger by using an adversarial adaptation loss function, enabling the backdoor to persist throughout global training dynamics, thereby achieving efficient and persistent attack effects.
% 总之，现有的后门攻击策略在空间隐蔽性和时间隐蔽性方面都有了显著的进展。空间隐蔽性后门攻击通过优化特征空间中的触发器或更新，使得恶意行为难以被现有的检测方法发现，从而实现了更高效、更难以检测的攻击。而时间隐蔽性后门攻击则专注于提高后门的持久性，使得即使在恶意攻击结束后，这些后门依然能够在模型中长期存在并继续影响其行为。通过这些攻击方式的结合，攻击者能够在各种应用场景中有效地躲避检测，并确保攻击效果的持续性，这对现有的防御机制提出了更高的挑战，强调了未来在开发更加鲁棒的防御方法方面的迫切需求。

%总的来看，现有的后门攻击方法已展现出显著的隐蔽性和持久性，能够有效规避当前的检测机制。这些攻击不仅在特征空间中巧妙隐藏恶意意图，还能在长期训练过程中保持攻击效果，对模型行为产生持续而深远的影响，也对模型安全构成了严峻挑战。这对现有的防御机制提出了更高的挑战，强调了未来在开发更加鲁棒的防御方法方面的迫切需求。
Overall, existing backdoor attack methods have demonstrated significant advances in both spatial and temporal concealment, effectively circumventing current detection mechanisms. These attacks not only cleverly hide malicious intent in the feature space but also maintain their effects throughout long-term training, posing a sustained and profound impact on model behavior, thus presenting a severe challenge to model security. This underscores the need for developing more robust defense methods.

\subsection{Backdoor countermeasures}
%在当前深度学习模型的安全性研究中，针对后门攻击的防御措施主要可以分为两大类：后门检测和后门缓解。% Backdoor Mitigation or Elimination
In current research on defense technologies, defense measures against backdoor attacks can be also broadly categorized into two major types: %backdoor detection and backdoor mitigation.

% 后门检测主要旨在识别模型中已存在的后门攻击。在这一类中，SEER\cite{zhu2024seer}提出了一种用于视觉-语言模型的后门检测算法，通过在图像和文本模态的特征空间中联合搜索目标文本和图像触发器，成功实现了在多种场景下的后门检测。FoolsGold\cite{foolsgold}提出了一种通过检测各客户端更新的相似性来识别攻击者方法，该方法利用攻击者的更新通常更为相似这一特征自适应调整每个客户端的学习率，从而减少攻击者对模型训练的影响，有效防御投毒攻击，即使在存在大量恶意客户端的情况下也能显著降低攻击成功率。ASSET\cite{pan2023asset}提出了一种通过主动诱导后门样本和干净样本在模型行为上的差异，从而实现跨多种深度学习范式的鲁棒后门数据检测方法，并在端到端监督学习、自监督学习和迁移学习中表现出色。DECREE\cite{feng2023detecting}提出了一种用于检测自监督学习中预训练图像编码器后门的创新方法，该方法无需依赖标签数据或下游分类器，且在多种数据集和攻击类型下表现出极高的检测准确率。
\textbf{Backdoor Detection} primarily aims to identify existing backdoor attackers. In this category, FoolsGold\cite{foolsgold} proposes a method to identify attackers by detecting the similarity of updates from each client. This method adaptively adjusts the learning rate of each nodes based on the observation that the updates from attackers are usually more similar. It effectively defends against poisoning attacks and significantly lowers the attack success rate even with a large number of malicious nodes. ASSET \cite{pan2023asset} introduces a robust backdoor data detection method across multiple deep learning paradigms by actively inducing behavioral differences between backdoor and clean samples, demonstrating excellent performance in end-to-end supervised learning, self-supervised learning, and transfer learning. DECREE \cite{feng2023detecting} presents an innovative method for detecting backdoors in pre-trained image encoders used in self-supervised learning, achieving high detection accuracy across various datasets and attack types without relying on label data or downstream classifiers.

% 后门缓解则侧重于通过各种技术手段来消除或减轻后门攻击的影响。FLAME\cite{FLAME}提出了一种防御联邦学习中后门攻击的方法，通过动态聚类和权重剪裁来识别并移除可能的恶意模型更新，同时注入差分隐私噪声以有效消除后门的影响，从而在保持全局模型性能的前提下，确保后门攻击的防御效果，即使面对多样的攻击者模型和数据分布也能显著降低攻击成功率。MCLDef\cite{yue2023model}提出了一种基于模型对比学习的两阶段后门防御方法，通过收缩或破坏中毒数据在特征空间中的聚类，并将中毒数据的特征拉向其干净对应物，从而有效去除深度神经网络中的后门，同时在不显著降低模型准确性的情况下提升防御效果。DPoE\cite{liu2023shortcuts}提出了一种基于端到端集成的后门防御框架，通过结合浅层模型和主模型来捕捉和抑制后门触发器，从而有效应对各种显性和隐性的后门攻击，同时减轻噪声标签对模型性能的影响。此外，PSIM\cite{zhao2024defending}提出了一种基于参数高效微调的防御模块，通过利用样本的置信度来识别被后门攻击污染的样本，显著增强了模型抵御权重中毒后门攻击的能力，并在不影响模型准确性的情况下有效过滤出被污染的样本。CleanCLIP\cite{bansal2023cleanclip}通过在视觉和文本编码器上进行无监督微调，独立调整每种模态的表示，以削弱视觉-语言对比学习模型中由后门攻击引入的错误关联，从而有效减少后门攻击的影响，同时保持模型在正常数据上的性能。
\textbf{Backdoor Mitigation} focuses on eliminating or reducing the impact of backdoor attacks through various technical means. FLAME\cite{FLAME} proposes a method to defend against backdoor attacks in federated learning by dynamically clustering and pruning weights to identify and remove potential malicious model updates. It also injects differential privacy noise to effectively eliminate the impact of backdoors, thereby ensuring the defense against backdoor attacks while maintaining global model performance. This approach significantly reduces the attack success rate even when facing diverse attacker models and data distributions. MCLDef \cite{yue2023model} proposes a two-stage backdoor defense method based on model contrastive learning, effectively removing backdoors in deep neural networks by disrupting the clustering of poisoned data in the feature space and pulling the features of poisoned data toward their clean counterparts, while improving defense effectiveness without significantly reducing model accuracy. DPoE \cite{liu2023shortcuts} introduces an end-to-end integrated backdoor defense framework that captures and suppresses backdoor triggers by combining a shallow model and a main model, effectively countering various explicit and covert backdoor attacks while mitigating the impact of noisy labels on model performance. Additionally, PSIM \cite{zhao2024defending} proposes a defense module based on parameter-efficient fine-tuning, significantly enhancing the model's ability to resist weight poisoning backdoor attacks by identifying samples polluted by backdoor attacks using sample confidence, effectively filtering out polluted samples without affecting model accuracy. CleanCLIP \cite{bansal2023cleanclip} reduces the impact of backdoors in vision-language contrastive learning models by adjusting the representations of each modality through unsupervised fine-tuning of the visual and text encoders while maintaining model performance on clean data.

%尽管这些方法在应对后门攻击方面展现了显著效果，但在面对更隐蔽或复杂的后门攻击时，效果依然可能不够理想。某些方法还依赖于外部知识或置信度的设定，可能导致误判或性能瓶颈。此外，这些方法在实际应用中也增加了计算开销和训练时间。因此，未来的研究仍需在提升整体性能与效率的同时，着力解决隐蔽或复杂后门攻击的防御挑战。
Despite the effectiveness of these methods in countering explicit backdoors, their performance may still be insufficient when faced with more covert or complex backdoor attacks. Some methods rely on external knowledge or confidence settings, which may lead to misjudgments or performance bottlenecks. Moreover, these methods increase computational overhead and training time in practical applications. %Therefore, future research must continue to focus on addressing the challenges posed by covert or complex backdoor attacks while enhancing overall performance and efficiency.

\section{System model \& Preliminaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%symbols 表格
\begin{table}[!t]
{
\renewcommand{\arraystretch}{1.2}
\caption{Summary of Main Notation}
\label{notations}
\vspace{0em}
\centering
\begin{tabular}{| c || p{6cm}  |}
\hline
\textbf{Symbol} & \multicolumn{1}{c |}{\textbf{Description}}\\
    \hline
    $ N $ & The number of nodes participating in each round.\\ \hline
    $R_i$ & The $i$-th robot node. \\ \hline
    $D_i$ & The private local dataset of node $R_i$.\\ \hline
    $\theta_g^t, \theta_i^t$ & The global model and local model of $R_i$ at round $t$. \\ \hline
    $\triangledown \theta_i^t$ & Local update of $R_i$ during round $t$. \\ \hline
    $G_i^t $ & Frequency vector of $R_i$'s update at round $t$. \\ \hline
    $m$ & The length of the low-frequency vectors. \\ \hline
    $H^t$ & The matrix formed by stacking $G_i^t$. \\ \hline
    $\tilde{G}^t $ & The extracted global clean ingredient at round $t$. \\ \hline
    $\lambda_{max}^t$ & The largest singular value of matrix $H^t$ at round $t$. \\ \hline
    $\xi_{max}^t$ & The left singular vector corresponding to $\lambda_{max}^t$. \\ \hline
    $Chi_i^t$ & The difference between $\triangledown \theta_i^t$ and $\tilde{G}^t $ at round $t$. \\ \hline
    $t'$ & The round number within total $T$ rounds. \\ \hline
    $O_i^{t'}$ & The intention point of $R_i$ at round $t'$. \\ \hline
    \( {v}_{i}^{t'} \) & The vector of the behavior ray for $R_i$ at round $t'$. \\ \hline
    $lof_i$ & The outlier level of $R_i$.\\ \hline
    $cre_i$ & The credit score of $R_i$.\\ \hline
\end{tabular}
    }
 \vspace{-1.5em}
 \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[htbp]
%     \centering
%     % \includegraphics[width=0.5\textwidth]{figure}
%     \label{fig:scenario}
% \end{figure}

%如图\ref{fig:scenario}所示，考虑在机器人物联网（IoRT）网络中对视觉大模型进行联邦微调的场景下，整个系统模型包括多个机器人节点和一个中心服务器。每轮，每个机器人节点在本地对视觉大模型进行微调并只上传微调部分的参数更新，随后中心服务器对所有更新进行聚合，然后下发聚合后的参数，开启新一轮的联邦微调。以下是系统模型的详细描述。
As illustrated in Figure \ref{fig1:scene}, \textcolor{red}{consider the scenario of federated fine-tuning (FFT) for large vision language models (LVLMs) in an Internet of Robotic Things (IoRT) network. The entire system model consists of multiple robot nodes and a central server. In each round, each robot node locally fine-tunes the vision model and only uploads the updates of the fine-tuned part. The central server then aggregates all updates and distributes the aggregated parameters to initiate a new round of federated fine-tuning. Below is a detailed description of the system model.}


%\textbf{参与者和数据集}：假设系统模型中存在 $N$ 个机器人节点，每个节点记作 $R_i$, 其中$i \in \{1, 2, \ldots, N\}$。 每个机器人节点 $R_i$ 拥有本地数据集 $D_i = \{x_{i,j}, y_{i,j}\}_{j=1}^{|D_i|}$，其中 $x_{i,j}$ 表示训练样本，$y_{i,j}$ 表示对应的标签数据，$|D_i|$ 为数据集的大小，所有节点本地数据集的并集$D = \bigcup_{i=1}^{N} D_i$构成整体数据集。
\textbf{Participants and Datasets}:
\textcolor{red}{Assume there are $N$ robot nodes in the system model, with each node denoted as $R_i$, where $i \in {1, 2, \ldots, N}$. Each robot node $R_i$ possesses a local dataset $D_i = {x_{i,j}, y_{i,j}}{j=1}^{|D_i|}$, where $x{i,j}$ represents a training sample, $y_{i,j}$ represents the corresponding label, and $|D_i|$ denotes the size of the dataset. The union of all local datasets across the nodes, $D = \bigcup_{i=1}^{N} D_i$, constitutes the overall dataset.}

% In this system model, there are $N$ robotic nodes, each denoted as $R_i$, where $i \in {1, 2, \ldots, N}$. Each robotic node $R_i$ has a local dataset $D_i = \{x_{i,j}, y_{i,j}\}_{j=1}^{|D_i|}$, where $x{i,j}$ represents the input data, $y_{i,j}$ represents the corresponding label, and $|D_i|$ is the size of the dataset. The union of the local datasets across all nodes, $D = \bigcup_{i=1}^{N} D_i$, forms the overall dataset.

%\textbf{LoRA}： 在LoRA(Low-Rank Adaptation) \cite{lora}方法中，每个参数矩阵 $W$ 被表示为：
\textbf{Fine-tuning}:
Taking Low-Rank Adaptation (LoRA) \cite{lora} as an example, model parameter matrix $W$ can be expressed as:
\begin{equation}
    W = W_0 + \Delta W, \quad \Delta W = BA,
\end{equation}
    %其中，$W_0$ 为模型的原始权重矩阵，$\Delta W$ 是低秩增量矩阵，$B$ 和 $A$ 是低秩分解矩阵，$r$ 为矩阵的秩。在对视觉大模型进行联邦微调的场景中，全局模型原始参数为$\Theta_p = \sum {W_0}$，而LoRA引入的增量参数为$\theta_p = \sum {W}$，因此全局模型的整个参数集为$\Theta = \Theta_p + \theta_g$。在整个联邦微调过程中，仅对全局模型的增量参数$\theta_g$ 进行微调训练，保持原始参数$\Theta_p$不变。
\textcolor{red}{where $W_0$ represents the original weight matrix of the model, $\Delta W$ denotes the low-rank increment matrix, $B$ and $A$ are the low-rank decomposition matrices, and $r$ is the rank of the matrix. In the scenario of FFT for LVLMs, the fixed parameters of the global model are given by $\Theta_p = \sum {W_0}$, and the incremental parameters introduced by LoRA are denoted as $\theta_g = \sum {W}$. Therefore, the entire parameter of the global model is $\Theta = \Theta_p + \theta_g$. Throughout the FFT process, only the parameters $\theta_g$ of the global model are fine-tuned, while the parameters $\Theta_p$ remain unchanged.}
    
    % In the federated fine-tuning scenario of the large-scale vision model, the global model's original parameters are $\Theta_p = \sum {W_0}$, while the incremental parameters introduced by LoRA are $\theta_p = \sum {W}$. Therefore, the global model's full parameter set is $\Theta = \Theta_p + \theta_g$. Only the incremental parameters $\theta_g$ are fine-tuned throughout the federated fine-tuning, while the original parameters $\Theta_p$ remain unchanged.
    
%\textbf{本地模型训练}：
\textbf{Local training}:
%在每轮联邦训练的开始时，中心服务器将全局模型的增量参数 $\theta_g^{t-1}$ 发送给每个机器人节点$R_i$。每个节点在获得增量参数$\theta_g^{t-1}$后，基于本地数据集 $D_i$ 进行微调训练。
At the beginning of each FFT round, the central server sends the global incremental parameters $\theta_g^{t-1}$ to each robot node $R_i$. Upon receiving $\theta_g^{t-1}$, each robot node fine-tunes the local incremental parameters based on its local dataset $D_i$.
% 算法\ref{alg:local_update}展示了节点进行本地模型每轮训练的主要过程。
%假设本地训练的轮次为$E_l$，批次大小为$b$，学习率为$\eta$。对于每一轮，节点$R_i$首先从其本地数据集$D_i$中随机抽取一个批次数据$D_i^b \sim Sample(D_i, b)$，然后利用随机梯度下降等优化算法，对增量参数进行更新，
Assume the number of local training epochs is $E_l$, the batch size is $b$, and the learning rate is $\eta$. For each epoch, the node $R_i$ randomly samples a batch of data $D_i^b \sim Sample(D_i, b)$ from $D_i$ and updates the $\theta_g^{t-1}$ using an optimization procedure as:
\begin{equation}
    (\theta_i^{t})^e = (\theta_i^{t})^{e-1} - \eta \nabla f(D_i^b, (\theta_i^{t})^{e-1}),
\end{equation}
%其中$(\theta_i^{t})^e$表示在第$e$轮微调后的参数，初始时$(\theta_i^{t})^0 = \theta_g^{t-1}$。
where $(\theta_i^{t})^e$ denotes the local incremental parameters after $e$ training epoch, with the initial value $(\theta_i^{t})^0 = \theta_g^{t-1}$.
%本地训练的目标是最小化损失函数 $f(D_i^b, (\theta_i^{t})^e)$： 
The local training objective to minimize the loss $f(D_i^b, (\theta_i^{t})^e)$:
\begin{equation}
    f(D_i^b, (\theta_i^{t})^e) = \frac{1}{|D_i^b|} \sum_{j=1}^{|D_i^b|} \ell(x_{i,j}, y_{i,j}; \Theta_p + (\theta_i^{t})^e),
\end{equation}
%其中$\Theta$是模型的原始参数， $\ell$表示损失函数（如交叉熵损失）。本地训练完成后，节点计算其增量参数更新 $\triangledown \theta_i^t = (\theta_i^t)^{E_l} - (\theta_i^t)^0$ 并上传至中心服务器。
where $\ell$ denotes the loss function (e.g., cross-entropy loss). After local training is completed, the robot node computes the update of the incremental parameters $\triangledown \theta_i^t = (\theta_i^t)^{E_l} - (\theta_i^t)^0$ and uploads it to the central server.

%\item \textbf{联邦聚合}：当中心服务器接收到所有机器人节点的本地更新$\triangledown \theta_i^t$后，利用相应的聚合算法进行聚合得到新一轮的全局增量参数$\theta_g^t$：
\textbf{Federated Aggregation}:
\textcolor{red}{After the central server receives the updates $\triangledown \theta_i^t$ from all robot nodes, it applies the appropriate aggregation algorithm to generate the next round's global incremental parameters $\theta_g^t$:
% The central server collects the local updates $\triangledown \theta_i^t$ from each robot node $R_i$ and aggregates them using a specified aggregation algorithm to update the global incremental parameters $\theta_g^t$:   
\begin{equation}
    \theta_g^t = \theta_g^{t-1} + Agg(\triangledown \theta_1^t, \triangledown \theta_2^t, \dots, \triangledown \theta_n^t),
\end{equation}
%其中，$Agg$表示中心服务器使用到的聚合算法，例如FedAvg、FLTrust\cite{cao2020fltrust}、RoseAgg\cite{yang2024roseagg}等，$\theta_g^t$ 是第 $t$ 轮更新的全局增量参数，更新后的全局增量参数 $\theta_g^t$ 将在下一轮训练前下发给所有机器人节点，用于进一步的本地微调。
where $Agg$ denotes the aggregation algorithm employed by the central server, such as FedAvg, FLTrust \cite{cao2020fltrust}, or RoseAgg \cite{yang2024roseagg}.}


\section{Threat Model}
% 在对视觉语言模型进行联邦微调的场景中，我们假设存在一个或者多个由攻击者操纵的恶意节点，并且恶意节点的数量小于全部参与节点的一半。攻击者的目标是对全局模型$\Theta$ 进行后门植入，使得模型能够将任何带有预定义触发器 $\delta$ 的测试输入分类为目标类别，即$\Theta(x+\delta) = y'$，其中$x$代表正常输入，$y'$表示目标类别。同时尽可能保持对干净输入的分类准确性，即$\Theta(x) = y$，其中$y$为输出$x$的原始类别。
\textcolor{red}{In the scenario of FFT for LVLMs, we assume the presence of one or more malicious nodes controlled by attackers, with the number of malicious nodes being less than half of all participating nodes.}

\textbf{Attacker's Goal:} The attacker's goal is to implant a backdoor into the global model $\Theta$, such that the model classifies any input with a predefined trigger $\delta$ as the target class, i.e., $\Theta(x + \delta) = y'$, where $x$ represents a normal input and $y'$ denotes the target class. Simultaneously, the attacker seeks to maintain the classification accuracy for clean inputs, i.e., $\Theta(x) = y$, where $y$ is the original class corresponding to $x$.

%Attacker's Ability: 攻击者对恶意节点的本地训练过程具有完全的控制权，意味着攻击者可以任意操控本地数据、触发器模式、优化策略和本地更新。此外，攻击者可以采用“智能”攻击策略，这种策略的特点是攻击效果不会在单轮训练中显现，而是通过多轮训练的累积效应逐步将后门植入全局模型中。这种场景在实际中是可能存在的，因为中央服务器只能接收到客户端上传的训练后模型更新，而无法知晓每个客户端的具体训练过程。然而，攻击者无法对中央服务器的操作进行干扰，例如，无法更改全局模型的聚合规则或篡改其他良性客户端的本地更新。
\textbf{Attacker's Ability:} 
\textcolor{red}{The attacker has complete control over the local training process of the malicious robot nodes, meaning they can manipulate local data, trigger patterns, optimization strategies, and local updates at will. Additionally, the attacker may adopt a stealthy attack strategy, where the backdoor is not implanted in a single training round but gradually embedded into the global model through the cumulative effect of multiple rounds of training. This scenario is plausible in practice, as the central server only receives the updates uploaded by robot nodes and has no insight into the specific training process of each node. However, the attacker cannot interfere with the aggregation process of the central server (i.e., altering the global model aggregation rules) or tamper with the local updates of benign nodes.}


% In the FFT processes of LVLMs, it is assumed that the attacker has complete control over the local training process of malicious nodes. This means the attacker can manipulate the local dataset, trigger patterns, optimization strategies, and the local update procedure at will. Additionally, the attacker can employ stealthy attack strategies, where the backdoor is not immediately apparent in a single training round but is gradually embedded into the global model through the cumulative effect of multiple training rounds. Such a scenario is feasible in practice since the central server only receives the model updates uploaded by clients after training and has no insight into the specific local training process of each client. However, the attacker is unable to interfere with the central server's operations, such as modifying the global model aggregation rules or tampering with the model updates from other benign clients.


\section{Methodology}
Our SecFFT method consists of two major modules: Instantaneous Attack Behavior Perception and Long-term Attack Intention Detection, which are elaborated, respectively.

\subsection{Instantaneous Attack Behavior Perception}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig2.png}
    \caption{SecFFT's instantaneous attack behavior perception module with frequency-domain deep outlier feature semantics extraction}
    \label{fig2:instant}
\end{figure}
%^\textbf{Motivation:} 目前的防御策略主要集中于利用攻击的“浅层语义特征”来进行恶意节点识别，所谓浅层语义是指模型上传更新的方向\cite{foolsgold}，范数\ref{sun2019can}以及几何中心\cite{pillutla2022robust}等几何或者统计特性。然而，这些基于浅层特征的防御方法在高维空间中存在显著的局限性。首先，这些防御方式只能在特定假设条件下对某些类型的攻击起作用。例如，余弦距离能够检测出具有较大方向偏差的恶意更新，而欧几里得距离则用于检测通过扩大$L_2$范数影响全局模型的恶意更新。同时最新的研究表明\cite{huang2023multi}，基于欧几里得距离等传统的距离度量方法在高维空间中无法有效区分恶意和良性更新，因为这些方法往往受到维度灾难的影响。此外攻击策略也在不停的演变，现在的攻击者已经能够通过精心调整，使恶意更新在方向、范数等浅层特征上与良性更新相似，从而巧妙地避开这些基于浅层语义特征的防御措施，最终在全局模型中植入后门。但尽管攻击者的攻击策略变得越来越复杂，值得注意的是，后门攻击的本质在于让模型建立起预定义触发器和目标之间的联系，因此无论攻击者如何调整自己的攻击方式或策略，其上传的模型更新相较于良性用户始终在某种分布上具有偏移。
\textbf{Motivation:} 
Current defense strategies primarily focus on identifying malicious nodes by leveraging shallow semantic features of the attack, such as the direction \cite{foolsgold}, magnitude \cite{sun2019can}, and geometric or statistical properties like geometric centers \cite{pillutla2022robust} of updates. However, these defense methods based on shallow semantic features face significant limitations in high-dimensional spaces. First, these methods are only effective against certain types of attacks under specific assumptions. For example, cosine distance can detect malicious updates with large directional deviations, while Euclidean distance is used to detect malicious updates that affect the global model by increasing the $L_2$ norm. Furthermore, recent studies \cite{huang2023multi} have shown that traditional distance metrics, such as Euclidean distance, cannot effectively distinguish between malicious and benign updates in high-dimensional spaces due to the curse of dimensionality. Additionally, attack strategies are constantly evolving, and attackers can now carefully adjust their malicious updates to resemble benign updates in terms of direction, magnitude, and other shallow features. This allows them to bypass defenses based on shallow semantic features and eventually implant backdoors into the global model. \textcolor{red}{Despite the increasingly sophisticated strategies employed by attackers, it is important to note that the essence of backdoor attacks lies in establishing a link between the predefined trigger and the target within the model. Thus, regardless of how attackers adjust their attack strategies, their updates will always exhibit some distributional shift compared to benign nodes.}

% Current defense strategies primarily focus on identifying malicious nodes using "shallow semantic features" of attacks, such as geometric or statistical characteristics like the direction\cite{foolsgold}, magnitude\cite{sun2019can}, and geometric center\cite{pillutla2022robust} of node updates. However, these shallow feature-based defenses have significant limitations in high-dimensional spaces. For instance, these methods often function effectively only under specific assumptions and for certain types of attacks. For example, cosine distance can detect malicious updates with significant directional deviations, while Euclidean distance is typically employed to identify malicious updates that inflate the $L_2$ norm and affect the global model. Recent studies have also shown \cite{huang2023multi} that traditional distance metrics like Euclidean distance are inadequate for distinguishing between malicious and benign updates in high-dimensional spaces due to the curse of dimensionality. Moreover, attack strategies continue to evolve, with attackers now able to fine-tune their updates to resemble benign ones in terms of direction, norm, and other shallow features, effectively bypassing these shallow semantic-based defenses while still implanting backdoors into the global model. It is important to note, however, that the core of a backdoor attack lies in establishing an association between a predefined trigger and the target. Regardless of how the attacker adjusts their attack methods or strategies, the uploaded updates will always display a distributional shift compared to benign nodes.


%\textbf{Overview:} 在神经网络中，每个权重代表两个神经元之间连接的强度，权重分布及其相关的能量在训练过程中会经历动态演变，以缩小模型预测结果与真实目标类别之间的差距。当后门攻击迫使神经网络模型学习一种在正常数据中不存在的关联模式时，根据后门触发器的设计和实现方式，这种模式可能是微妙的，也可能是相当明显的，但无论如何，它都代表了与模型从正常数据中学习到的模式的偏离。因此，为解决基于浅层语义特征的防御方法在高维空间中面临的局限性，我们从后门攻击的本质出发\cite{zeng2021rethinking}，提出了一种基于深层语义特征的防御策略，重点分析模型更新在频域空间上的差异。通过将本地模型更新转换到频域，我们能够更准确地捕捉恶意更新在低频成分上的异常表现，这些异常通常难以被传统的浅层特征防御方法发现。通过对频域上的相应特征进行分析，我们可以有效地区分出恶意节点与良性节点，从而增强联邦学习系统在面对复杂后门攻击时的鲁棒性和检测精度。
\textbf{Overview:} 
In neural networks, each weight represents the strength of the connection between two neurons, and the distribution of these weights, along with their associated energy, undergoes dynamic evolution during training to reduce the gap between the model's predicted results and the true classes. nonetheless, When a backdoor attack forces the neural network to learn an association that does not exist in normal data, which may be subtle or quite obvious depending on the design and implementation of the backdoor trigger, it invariably represents a deviation from the association learned by the model from normal data. \textcolor{red}{To address the limitations faced by defense methods based on shallow semantic features in high-dimensional spaces, we propose a new defense strategy rooted in the essence of backdoor attacks \cite{zeng2021rethinking}, which leverages deep semantic features by focusing on the differences in model updates in the frequency domain. According to our experimental results, by transforming local model updates into the frequency domain, we can more accurately capture the abnormal behavior of malicious updates which is typically difficult to detect using traditional shallow feature-based defenses in low-frequency components. By analyzing the relevant features in the frequency domain, we can effectively distinguish between malicious and benign nodes, thereby enhancing the robustness and accuracy of federated learning in the face of sophisticated backdoor attacks.}

% In deep neural networks, each weight signifies the strength of the connection between two neurons. The distribution of these weights and their corresponding energy dynamically evolves during training to minimize the gap between the model's predictions and the ground truth in the training data. When a backdoor attack compels the neural network to learn an association pattern that does not exist in normal data, this pattern depending on the design and implementation of the backdoor trigger can be subtle or quite evident. Nonetheless, it always signifies a deviation from the patterns learned from normal data. To address the limitations of shallow feature-based defenses in high-dimensional spaces, we propose a defense strategy grounded in the essence of backdoor attacks\cite{zeng2021rethinking}, focusing on deeper semantic features by analyzing differences in the frequency distribution of model updates. By transforming local model updates into the frequency domain, we can more effectively capture the anomalous behavior of malicious updates in low-frequency components, which are typically challenging to detect using traditional shallow-feature detection methods. Through frequency domain feature analysis, we can effectively distinguish malicious nodes from benign ones, thereby enhancing the robustness and detection accuracy of federated learning systems when confronted with sophisticated backdoor attacks. Figure \ref{fig2:instant} formalizes this method we proposed.

\subsubsection{Frequency domain transformation}
%受高维空间中维度灾难的影响，传统的基于距离度量的方法已无法有效识别恶意节点。相反，将模型更新转换到频率域后，其差异将主要集中在低频成分上。因此，专注于低频成分的分析，在提升识别准确率的同时，将显著的降低计算开销。假设每个节点上传的更新为 \(\triangledown \theta_i^t \in \mathbb{R}^d\)，其中 \(i\) 表示机器人节点，\(t\) 表示通信轮次，\(d\) 为本地模型上传更新的维度。对于每个更新 \(\triangledown \theta_i^t\)，对其进行归一化，然后通过一维的DCT-II\cite{ahmed1974discrete}变化将其转换为对应的频率分布，并提取$m$个低频分量，例如5000，得到低频向量\(G_i^t\)。
Due to the curse of dimensionality in high-dimensional spaces, traditional methods based on shallow semantic
feature can no longer effectively identify malicious nodes. In contrast, once model updates are transformed into the frequency domain, their differences are primarily concentrated in the low-frequency components. Focusing on the analysis of these low-frequency components not only improves identification accuracy but also significantly reduces computational overhead. \textcolor{red}{For each update \(\triangledown \theta_i^t\), we apply one-dimensional DCT-II \cite{ahmed1974discrete} to obtain its corresponding frequency distribution. Then we extract $m$ low-frequency components\cite{fereidooni2023freqfed}, such as 5000, yielding the low-frequency vector $G_i^t$.}

%%___________________________________________________________________________________________________________________________
\begin{equation}
G_i^t = Trunc(DCT(Flatten(\triangledown \theta_i^t)), m)
\label{eq:DCT}
\end{equation}

% 频域转换的主要过程如公式\ref{eq:DCT}所示，这里的 $Trunc$ 操作表示仅保留前$m$个低频分量。
The main process of frequency domain transformation is shown in Equation \ref{eq:DCT}, where the $Trunc$ operation indicates retaining only the first $m$ low-frequency components.

\subsubsection{Clean ingredient extraction}
% 在提取到节点更新对应的低频向量\(G_i^t\)后，我们使用流行的聚类算法例如HDBSCAN\cite{mcinnes2017hdbscan}对这些更新进行聚类，得到多个簇$C_0^t, C_1^t, \ldots, C_{\kappa}^t$，其中$\kappa$表示簇的数量。在大多数联邦学习场景中，良性节点的数量通常多于恶意节点的数量，且良性节点的更新通常更接近全局模型的方向并表现出更高的聚集性。基于这一观察，我们选择节点数量最多的簇$C_{max} = \{R_{i_0}, R_{i_1}, \ldots, R_{i_n}\}$作为主要分析对象。为了进一步提取该簇的主要特征或模式，我们对其中所有节点组成的矩阵进行奇异值分解\cite{wang2023scfl}。由于，SVD分解的左奇异向量$\tilde{G}^t$对应于数据的主要方向，因此，$\tilde{G}^t$可以作为良性节点在低频域内的主要特征。
After extracting the corresponding low-frequency vectors \(G_i^t\) from updates, we employ popular clustering algorithms such as HDBSCAN \cite{mcinnes2017hdbscan} to cluster these vectors, resulting in multiple clusters \(C_0^t, C_1^t, \ldots, C_{\kappa}^t\), where \(\kappa\) denotes the number of clusters. In most federated learning scenarios, the number of benign nodes typically exceeds that of malicious nodes, which is consistent with the assumptions outlined in our threat model. Moreover, the updates from benign nodes are generally closer to the direction of the global model and exhibit a higher degree of aggregation. This behavior is also evident in the frequency domain, where it becomes even more pronounced. 
\textcolor{red}{Based on this observation, we select the largest cluster \(C^t_{max} = \{G^t_{i_0}, G^t_{i_1}, \ldots, G^t_{i_n}\}\) as the primary subject for analysis. To further extract the main characteristics of this cluster, we perform singular value decomposition (SVD)\cite{wang2023scfl} on the matrix $H^t = (G^t_{i_0}, G^t_{i_1}, \ldots, G^t_{i_n}) \in \mathbb{R}^{m \times n}$ formed by all nodes in this cluster. Since the left singular vectors of the SVD, denoted as \(\tilde{G}^t\), represent the principal directions of these vectors, \(\tilde{G}^t\) can be regarded as the principal ingredient of benign nodes in the low-frequency domain.}


\begin{equation}
\tilde{G}^t = \frac{H^t \xi^t_{max}}{\sqrt{\lambda^t_{max}}}
\label{eq:singular}
\end{equation}


Equation \ref{eq:singular} illustrates the main process of computing the left singular vector $\tilde{G}^t$. $\lambda^t_{max}$ and $\xi^t_{max}$ correspond to the largest eigenvalue and its eigenvector of the matrix $\hat{H}^t = (H^t)^{T} H^t$, respectively. In this way, we effectively extract the principal ingredient that reflect the updates of benign nodes in the frequency domain, so $\tilde{G}^t$ can be used as the clean ingredient for subsequent malicious node detection.

\subsubsection{Chi-square distance calculation}
% 我们利用每个节点的低频向量与“干净成分”之间的差异度来作为衡量恶意节点和良性节点的依据。对于每个节点的低频向量 \(G_i^t\) 以及上一步中求得的“干净成分” \(\tilde{G}^t\)，通过卡方距离来计算两者之间的差异度，得到距离差异集合$S^t = Chi^t_i, Chi^t_2, \ldots, Chi^t_N$。卡方距离的计算过程为：
We use the divergence between each node's low-frequency vector and the clean ingredient as a measureto distinguish between malicious and benign nodes. For each  low-frequency vector \(G_i^t\) and the clean ingredient \(\tilde{G}^t\) we calculate their divergence using the Chi-square distance, yielding \(S^t = \{Chi^t_1, Chi^t_2, \ldots, Chi^t_N\}\). The calculation process for the Chi-square distance is:

\textcolor{red}{
\begin{equation}
Chi^t_i = \sqrt{ \sum_{k=0}^{m-1}\frac{(G_i^t[k] - {\tilde G}^t[k])^2}{{|\tilde G}^t[k]| + \epsilon} }
\label{equation:KL}
\end{equation}}
% 其中 \(m\) 是低频向量的维度，\(\epsilon\) 是一个小的常数，用于避免分母为零。相比于传统的欧氏距离，卡方距离根据两个向量对应分量的大小进行加权，使得特征值较大的部分对距离的影响更大，从而能够更准确地反映低频向量中重要特征之间的差异，同时忽略不重要的噪声成分。
where \(G_i^t[k]\) denotes the k-th element of vector \(G_i\) and \(\epsilon\) is a small constant to avoid division by zero. 


\textcolor{red}{
Chi-square distance is usually used to measure whether a certain observed distribution conforms to a typical theoretical distribution, which is very consistent with our method because we use the calculated clean ingredient as a criterion for measuring benign and malicious nodes. Compared with the traditional Euclidean distance, the chi-square distance assigns weights according to the clean ingredient, giving greater importance to smaller components. On the other hand, cosine similarity measures the angle between two vectors and is usually used to evaluate their directional consistency, ignoring magnitude differences. However, in our method, we are concerned with the degree of deviation of each user's low-frequency vector $G_i^t$ from the clean ingredient $\tilde{G}^t$, including magnitude differences, so the chi-square distance is more suitable for capturing hidden malicious attacks than cosine similarity.
}
% 为什么要用卡方距离，不用欧式距离？实验论证？

\subsubsection{Single-round malicious behavior perception}
% 为了准确地识别恶意节点，我们利用良性节点的低频成分与干净成分之间的差异度相比于恶意节点与干净成分之间更为相似这一特性，对差异集合$S^t$进一步进行聚类分析。具体而言，我们采用流行的KMeans聚类算法\cite{wan2023four}对距离差异集合$S^t$进行聚类，并将类别数设置为2，得到聚类\(C'_1, C'_2\)，将用户数量较大的簇\(C'_{max}\)中的所有节点标记为良性节点， 记作为\(U_{nor}\)，将另一簇\(C'_k\) \(k \neq {max}\)中的所有节点标记为恶意节点，记作\(U_{mal}\)。  
To accurately identify malicious nodes, we leverage the characteristic that the divergence between benign nodes' low-frequency vectors and the clean ingredient is more similar compared to that of malicious nodes. Based on this, we further perform cluster analysis on the divergence set \(S^t\). Specifically, we employ the popular clustering algorithm KMeans \cite{wan2023four} to \(S^t\), setting the number of clusters to 2, yielding two clusters \(C'_1, C'_2\). Based on the assumption before,  we label all nodes in the cluster with the larger number of nodes \(C'_{max}\) as benign nodes, denoted by \(U_{nor}\). while all nodes in the other cluster \(C'_k\) (\(k \neq {max}\)), are labeled as malicious nodes, denoted as \(U_{mal}\).

% 算法\ref{alg:malicious-node-detection}展示了单轮恶意用户识别的整体过程，其中3-6行代表了频率转换的计算过程，7-12行代表了“干净分量”$\tilde G^t$的计算过程，13-16行代表了每个用户与“干净分量”间的卡方差异度的计算过程，最后17-18行代表了通过KMeans聚类算法进行恶意用户检测的计算过程。
Algorithm \ref{alg:malicious-node-detection} outlines the overall process of Instantaneous Attack Behavior Perception. 
Lines 3-6 describe the computation process of frequency domain transformation, lines 7-12 detail the extraction process of clean ingredient, lines 13-16 cover the calculation of the Chi-square divergence between each low-frequency vector and the clean ingredient, and finally, lines 17-18 outline the process of single-round malicious behavior perception using KMeans.

\begin{algorithm}
\caption{Instantaneous Attack Behavior Perception}
\label{alg:malicious-node-detection}
\begin{algorithmic}[1]
\State \textbf{Input:} $d$, $N$, $(\triangledown \theta_0^t, \triangledown \theta_1^t, \ldots, \triangledown \theta_N^t) \in \mathbb{R}^{N \times d}$, $m$ \Comment{$d$ is the dimension of each update; $N$ is the number of nodes participating during each round; $(\triangledown \theta_0^t, \triangledown \theta_1^t, \ldots, \triangledown \theta_N^t) \in \mathbb{R}^{N \times d}$ is the local updates from robot nodes during the $t$-th round; $m$ is the length of low-frequency vector}
\State \textbf{Output:} $U_{nor}$, $U_{mal}$ \Comment{benign nodes, malicious nodes}
\State{\color{CadetBlue}/* \textbf{Frequency Domain Transformation */}} 
% \State Step 1: Frequency Domain Transformation
\For {$R_i \in \textcolor{red}{\{R_1, \ldots, R_N\}}$}
    \State \textcolor{red}{$G_i^t \gets Trunc(DCT(Flatten(\triangledown \theta_i^t))), m)$}
\EndFor

% \State
\State{\color{CadetBlue}/* \textbf{Clean Ingredient Extraction */}} 
% \State Step 2: Clean Ingredient Extraction
\State $(C_0^t, C_1^t, \ldots, C_{\kappa}^t) \gets Clustering\textcolor{red}{ (G_0^t, G_1^t, \ldots, G_N^t)}$ \Comment{$\kappa$ denotes the number of clusters}
\State \textcolor{red}{$ C^t_{max} \gets \underset{C^t_j}{\arg\max} \, |C^t_j|$ \Comment{$|C^t_j|$ denotes the number of nodes in cluster $C^t_j$, $j = 1, 2, \ldots, \kappa $}}
\State $H^t \gets (G^t_{i_0}, G^t_{i_1}, \ldots, G^t_{i_n}) \in \mathbb{R}^{m \times n} $ \Comment{Stacking to form Matrix $H^t$, where $R_{i_0}, R_{i_1}, \ldots, R_{i_n} \in C^t_{max}$.}

\State $\hat{H}^t \gets (H^t)^{T} H^t$
\State $\lambda_{max}^t, \xi_{max}^t \gets eig(\hat{H}^t)$ \Comment{Calculating the maximum singular value and its corresponding eigenvector}
\State $\tilde{G}^t \gets \frac{H^t \xi_{max}^t}{\sqrt{\lambda_{max}^t}}$ \Comment{The clean ingredient}

% \State
\State{\color{CadetBlue}/* \textbf{Chi-square distance calculation */}} 
% \State Step 3: Chi-square distance calculation
% \State $P_i^t \gets Softmax(G_i^t), \, Q^t \gets Softmax(\tilde{G}^t)$
\For {$R_i \in \textcolor{red}{\{R_1, \ldots, R_N\}}$}
    \State \textcolor{red}{$Chi^t_i \gets \sqrt{\sum_{k=0}^{m-1}\frac{(G_i^t[k] - {\tilde G}^t[k])^2}{|{\tilde G}^t[k]| + \epsilon}}$}
\EndFor
\State \textcolor{red}{$S^t \gets \{Chi^t_1, Chi^t_2, \ldots, Chi^t_N\}$} \Comment{The Distance differences calculated by Chi-square distance.}

% \State
\State{\color{CadetBlue}/* \textbf{Single-round malicious behavior perception */}} 
% \State Step 4: Single-round malicious behavior perception
\State $\textcolor{red}{\{{C^t_1}', {C^t_2}'\}} \gets KMeans(S^t, 2)$ \Comment{Cluster $S$ into 2 clusters using KMeans.}
\State \textcolor{red}{$U_{nor} \gets C'_{max}$, $U_{mal} \gets \{C'_i \, | \, i \neq max\}$}

\end{algorithmic}
\end{algorithm}


\subsection{Long-term Attack Intention Detection}

% \textbf{Motivation}: 为了规避异常检测，有很多通过多轮次的组合攻击来植入后门的攻击。这些攻击很难在单轮次中检测出来，大致分为三类：1)限制大小的攻击，如\cite{geisler2024attacking, chen2024optimal}，这些攻击方法会限制单次攻击的大小，使得攻击并不是很明显，从而变得难以检测；2)限制角度的攻击，如\cite{you2023three, sagliano2024powered, dong2023adaptive}，这些攻击会限制每次攻击的角度，使其与正常的攻击类似从而变得难以识别；3)限制符号的攻击，如\cite{wan2023average, zhu2023boosting}，这些攻击通过梯度缩放或者梯度合成，控制恶意攻击的梯度与正常客户端梯度的正负维持一致，从而增加识别难度。本部分结合每个客户端的历史记录，通过识别攻击者最终的模型引导意图点来识别恶意节点。
\textbf{Motivation}: To evade anomaly detection systems, many backdoor attacks employ multi-round composite strategies that are difficult to detect in a single round. These attacks can be broadly classified into three types: (1) \textbf{Size-Limited Attacks} (e.g., \cite{geisler2024attacking, chen2024optimal}), which constrain the magnitude of each attack to make it subtle and less conspicuous, thereby reducing its detectability; (2) \textbf{Angle-Limited Attacks} (e.g., \cite{you2023three, sagliano2024powered, dong2023adaptive}), which restrict the direction of each attack so that it appears similar to legitimate updates, making them harder to recognize; and (3) \textbf{Sign-Limited Attacks} (e.g., \cite{wan2023average, zhu2023boosting}), which use techniques such as gradient scaling or gradient composition to ensure that the signs of the malicious gradients align with those of legitimate clients, thereby increasing the difficulty of detection. This section presents an approach that leverages each client's historical records to identify malicious nodes by analyzing their ultimate intent to manipulate the global model.



% 前面的防御已经可以有效地针对单轮次攻击进行识别，本部分主要结合客户端的历史记录对客户端进行有效的识别。本部分将保留历史$T$轮次的全局模型$\theta_g^{t'}$，其中$\max\{t-T+1, 1\}\leq  t'\leq t$，并将其Flatten后视为高维空间中的一个点；同时保留每个客户端的历史更新变化，将其视为高维空间中的一个向量，将其延伸后视为一个射线。对于每个客户端，使用最小覆盖球算法计算得到一个最小的超球，覆盖$\zeta$比例的射线。最终球心的位置可以被视为对应客户端对全局模型的意图引导点，球心的半径可以视为对应客户端的置信度。

\textbf{Overview}: The previous defense algorithms have effectively identified single-round attacks; this section focuses on leveraging clients' historical records for effective identification. We maintain the global models $\theta_g^{t'}$ from the past $T$ rounds, where $\max\{t-T+1, 1\}\leq t'\leq t$, and consider their flattened versions as points in a high-dimensional space. Simultaneously, we keep track of each client's historical updates, viewing them as vectors in the high-dimensional space and extending them as rays. For each client, we use the Minimum Enclosing Ball algorithm to compute a minimal hypersphere that covers a $\zeta$ proportion of the ray. The final position of the hypersphere's center can be regarded as the intended steering point of the global model for the corresponding node, and the radius of the hypersphere can be regarded as the reliability level.



This section mainly consists of four steps:

\subsubsection{Construction of weights updates and model databases}

% 像正常的联邦学习过程一样，中央服务器每次下发一个全局模型，每个客户端得到全局模型后使用本地数据进行训练，并将更新变化上传到中央服务器中。我们将第$t$轮次的全局模型记为$\theta_t$，将客户端$i$第$t$轮次训练后的模型记为$\theta^t_i$。客户端$i$将训练后的模型$\theta^t_{i}$减去训练前中央服务器下发的全局模型$\theta^t$，就得到了$t$轮次的更新变化$\triangledown \theta^t_{i}=\theta^t_{i}-\theta^t$。客户端将更新变化$\triangledown \theta^t_{i}$上传到中央服务器，中央服务器在聚合的同时，记录下每个客户端当前轮次的更新$\triangledown \theta^t_{i}$展平后的结果$\bar{\theta_i^t}$，同时存下中央服务器上轮次下发的全局模型$\theta^{t-1}$。中央服务器最多保留$T$轮次的模型和更新历史记录。“Keep T-rounds History”的主要算法过程如算法\ref{alg:malicious-node-detection-history}的3-13行所示。
As in a typical federated learning process, the central server distributes a global model in each round. Upon receiving the global model, each client trains on its local data and uploads the updates to the central server. Let the global model in the $t$-th round be denoted as $\theta_t$, and the model of client $i$ after training in the $t$-th round be denoted as $\theta^t_i$. Client $i$ computes its update in the $t$-th round as the difference between the trained model $\theta^t_{i}$ and the global model received before training $\theta^t$, i.e., $\triangledown \theta^t_{i} = \theta^t_{i} - \theta_g^t$. The client uploads the update $\triangledown \theta^t_{i}$ to the central server. While aggregating, the central server records the flattened result of each client's update in the current round, $\bar{\theta_i^t}$, as well as the global model $\theta^{t-1}$ distributed by the central server in the previous round. The central server retains the models and weights update histories for up to $T$ rounds.% The main algorithm process for "Keep T-rounds History" is shown in lines 3-13 of Algorithm \ref{alg:malicious-node-detection-history}. 

\subsubsection{Construction of attack intention}
% 这个问题可以抽象为高维空间中的一些具有起点的射线。射线的起点代表上轮次的全局模型，射线的方向代表本轮次展平后的更新变化。问题的优化目标是：找到一个最小的超球，至少覆盖$\zeta$比例的射线。
This problem can be abstracted as rays in a high-dimensional space with specific starting points. The starting point of a ray represents the global model from the previous round, and the direction of the ray represents the flattened update in the current round. The optimization goal of the problem is to find a minimum hypersphere that covers at least a $\zeta$ proportion of the rays.
% 我们使用符号$O_i$代表客户端$i$在高维空间中超球的球心，使用符号$r_i$代表这个超球的半径。假设当前轮次为$t$，$t'$是最近$T$轮次中的其中一轮，使用符号$\tilde O_i^{t'}$代表球心到第$t'$对应射线的最近点。则可以定义优化问题：
We use the symbol $O_i$ to represent the center of the hypersphere for client $i$ in the high-dimensional space and $r_i$ to represent the radius of this hypersphere. Suppose the current round is $t$, and $t'$ is one of the recent $T$ rounds. Let $\tilde O_i^{t'}$ denote the closest point from the hypersphere center to the ray corresponding to round $t'$. Then, the optimization problem can be defined as:

\begin{equation}
    \min_{O_i, r_i} \quad r_i
\end{equation}

\begin{equation} 
\text{s.t.} \quad \left| \left\{ t' \mid \| O_i - \tilde{O}_{i}^{t'} \| \leq r_i \right\} \right| \geq \zeta T,
\end{equation}
\begin{equation} 
    \quad\quad\max\{t-T+1, 1\}\leq  t'\leq t.
\end{equation}
% “Obtain the Purpose Intention”的主要算法过程如算法\ref{alg:malicious-node-detection-history}的14-18行所示。此部分又可以分为“Construct Ray Model”和“Minimum Enclosing Hypersphere Calculation”两部分。
%The main algorithm process for "Obtain the Purpose Intention" is shown in lines 14-18 of Algorithm \ref{alg:malicious-node-detection-history}. This section can be further divided into two parts: "Construct Ray Model" and "Minimum Enclosing Hypersphere Calculation."
% “Construct Ray Model”的主要过程如下。对于每个客户端 \(R_i\)，中央服务器保留了最近 \(T\) 轮的全局模型历史以及每轮次的更新变化历史。设 \( \theta_g^{t-T}, \theta_g^{t-T+1}, \dots, \theta_g^{t-1} \) 是最近 \(T\) 轮次的全局模型，且这些全局模被“展平”到了高维空间中的一个点。类似地，设 \( \overline{\triangledown\theta_{i}^{t-T}}, \overline{\triangledown\theta_{i}^{t-T+1}}, \dots, \overline{\triangledown\theta_{i}^{t-1}} \) 是最近 \(T\) 轮次客户端 \(i\) 上传的展平后的梯度变化向量。

%The main process of "Construct Ray Model" is as follows. 
For each client \(R_i\), the central server retains the global model history and the update history for the most recent \(T\) rounds. Let \( \theta_g^{t-T}, \theta_g^{t-T+1}, \dots, \theta_g^{t-1} \) represent the global models of the most recent \(T\) rounds, which have been "flattened" to a point in a high-dimensional space. Similarly, let \( \overline{\triangledown\theta_{i}^{t-T}}, \overline{\triangledown\theta_{i}^{t-T+1}}, \dots, \overline{\triangledown\theta_{i}^{t-1}} \) represent the flattened gradient update vectors uploaded by client \(i\) in the most recent \(T\) rounds.
% 对于每一轮 \( t' \) (\( \max\{t-T+1, 1\}\leq  t'\leq t \))，我们将每个射线的起点表示为展平后的全局模型 \( \theta_g^{t'} \)，并将每个射线的方向向量表示为展平后的梯度变化 \( \overline{\triangledown\theta_{i}^{t'}} \)。因此，射线模型可以构建如下：
For each round \( t' \) (\( \max\{t-T+1, 1\}\leq  t'\leq t \)), the starting point of each ray is represented by the flattened global model \( \theta_g^{t'} \), and the direction vector of each ray is represented by the flattened gradient update \( \overline{\triangledown\theta_{i}^{t'}} \). Thus, the ray model can be constructed as follows:

\begin{equation}
    \left\{\begin{matrix}
    \widetilde{\theta_{i}^{t'-1}} = Flatten(\theta_g^{t'-1})\\
    {v}_{i}^{t'} = \overline{\triangledown\theta_{i}^{t'}}
    \end{matrix}\right.,
\end{equation}
% 其中，\( \widetilde{\theta_{i}^{t'-1}} \) 是射线的起点，\( {v}_{i}^{t'} \) 是射线的方向向量。则射线方程为：
where \( \widetilde{\theta_{i}^{t'-1}} \) is the starting point of the ray, and \( {v}_{i}^{t'} \) is the direction vector of the ray. The ray equation is then given by:

\begin{equation}
l_i^{t'}=\widetilde{\theta_{i}^{t'-1}}+\alpha {v}_{i}^{t'},
\end{equation}
% 其中，$\alpha\in [0, +\infty)$为射线的自变量参数。
where $\alpha\in [0, +\infty)$ is the parameter of the ray.
% “Minimum Enclosing Hypersphere Calculation”的主要过程如下。基于构建的射线模型，使用最小覆盖球算法找到一个能够覆盖至少 \( \zeta \) 比例的射线的最小超球。首先，设定初始球心 \( O_{i,0} \) 为所有射线起点的几何中心，计算公式如下：
Therefore, we approximate the attack intention hiddden behind the series of behaviors based on Minimum Enclosing Hypersphere Ball (MEHB) construction as follows. Based on the constructed ray model, the MEHB model is adopted to find the smallest hypersphere that can cover at least a \( \zeta \) proportion of the rays, which denotes the intention of the current node. First, the initial center of the hypersphere \( O_{i,0} \) is set to the geometric center of all ray starting points, calculated as follows:

\begin{equation}
    O_{i,0} = \frac{1}{\min\{T, t\}} \sum_{t'=\max\{t-T+1, 1\}}^{t} \widetilde{\theta_g^{t'-1}}.
\end{equation}

% 初始半径 \( r_{i,0} \) 设定为从初始球心 \( O_{i,0} \) 到所有射线起点的最大距离：
The initial radius \( r_{i,0} \) is set as the maximum distance from the initial center \( O_{i,0} \) to all ray starting points:

\begin{equation}
r_{i,0} = \max_{t'=\max\{t-T+1, 1\}}^{t}  \|O_{i,0} - \widetilde{\theta_{g}^{t'-1}}\|.
\end{equation}
% 接下来，采用迭代方法来更新球心和半径：

% 对于每次迭代 \( k \)，计算当前球心 \( O_{i,k} \) 到所有射线的最近点 \( \tilde{O}_{i,k}^{t'} \)。对于每个节点$R_i$的第$t'(\max\{t-T+1, 1\}\leq  t'\leq t)$轮，对于射线 \( l_i^{t'} \)，找到球心 \( O_{i,k} \) 与当前射线的投影点 \( \hat O_{i,k}^{t'} \)，并计算参数 \( \alpha_k \)：
Then, the center and radius are iteratively updated. 
For each iteration \( k \), compute the nearest points \( \tilde{O}_{i,k}^{t'} \) from the current center \( O_{i,k} \) to all rays. For the $t'$-th round of each node $R_i$ (\(\max\{t-T+1, 1\}\leq  t'\leq t\)), for the ray \( l_i^{t'} \), find the projection point \( \hat O_{i,k}^{t'} \) of the center \( O_{i,k} \) onto the current ray, and compute the parameter \( \alpha_k \):

\begin{equation}
\alpha_k = \frac{(O_{i,k} - \widetilde{\theta_{i}^{t'-1}}) \cdot {v}_{i}^{t'}}{{v}_{i}^{t'}\cdot {v}_{i}^{t'}},
\end{equation}
% 则最近点 \( \tilde O_{i,k}^{t'} =\widetilde{\theta_{i}^{t'-1}}+\max \left\{ 0, \alpha_k\right\} {v}_{i}^{t'} \)。
% 计算当前球心 \( O_{i,k} \) 到这些最近点 \( \tilde O_{i,k}^{t'} \) 的距离集合 \( Dis_{i,k}= \{ \| O_{i,k}-\tilde O_{i,k}^{t'} \|, \max\{t-T+1, 1\}\leq  t'\leq t\} \)，并按从小到大的顺序进行排序，然后暂时舍弃$1-\zeta$比例的射线来忽略离群点对计算过程的影响，得到集合${Dis}_{i,k}'$，保留射线对应轮次的集合记为$\tilde T$。将球心向着被保留的最近点中距离最远的点$\tilde{O}_{i,k}^{t_{max}'}$移动：

Thus, the nearest point is given by \( \tilde O_{i,k}^{t'} =\widetilde{\theta_{i}^{t'-1}}+\max \left\{ 0, \alpha_k\right\} {v}_{i}^{t'} \). 
Calculate the set of distances from the current center \( O_{i,k} \) to these nearest points \( \tilde O_{i,k}^{t'} \), denoted as \( Dis_{i,k}= \{ \| O_{i,k}-\tilde O_{i,k}^{t'} \|, \max\{t-T+1, 1\}\leq  t'\leq t\} \). Sort these distances in ascending order and temporarily discard a proportion of \(1-\zeta\) of the rays to ignore the impact of outliers, resulting in the set ${Dis}_{i,k}'$. The rounds corresponding to the retained rays are denoted as $\tilde T$. Move the center towards the farthest point $\tilde{O}_{i,k}^{t_{max}'}$ among the retained nearest points:

\begin{equation}
O_{i,k+1} = O_{i,k} + \eta' (\tilde{O}_{i,k}^{t_{max}'}-O_{i,k}),
\end{equation}
% 其中$\eta'$为学习率。移动后的球心半径为：
where $\eta'$ is the learning rate. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/fig3.png}
    \caption{Long-term attack intention detection module and secure aggregation}
    \label{fig3:longterm}
\end{figure}

The radius of the moved center is:

\begin{equation}
r_{i,k+1} =  \{ \max\{\| O_{i,k+1}-\tilde O_{i,k+1}^{t'} \|\}, t'\in \tilde T\} ,
\end{equation}



% 其中$\tilde O_{i,k+1}^{t'}$为更新后的球心$O_{i,k+1}$到被保留射线的最近点。当更新后的半径 \( r_{i,k+1} \) 与上一次的半径 \( r_{i,k} \) 相差小于一个预设的阈值 \( \lambda \)，或者达到预设的最大迭代次数$k_{max}$时，停止迭代。
where $\tilde O_{i,k+1}^{t'}$ is the nearest point from the updated center $O_{i,k+1}$ to the retained rays. When the difference between the updated radius \( r_{i,k+1} \) and the previous radius \( r_{i,k} \) is smaller than a predefined threshold \( \lambda \), or when the maximum number of iterations $k_{max}$ is reached, the iteration stops.

\textcolor{red}{Figure \ref{fig3:longterm} illustrates the first two steps of the Long-term Attack Intention Detection. The figure comprises three rows overall. The first row demonstrates the process of federated learning, where many robot nodes independently train their local models \( \theta^t_{l_1} \) based on the distributed global model \( G_{t-1} \). Each participating client then uploads its gradient updates to the central server's global model, which are aggregated to form the next global model iteration. The second row is divided into two sections: the left section shows the flattened gradients updated across multiple rounds for clients. The first column represents the gradient update of a specific round, the second column shows the flattened gradient updates from \( n \) clients in one round, and the third column displays the flattened updates across \( T \) rounds for \( n \) clients. The right section of the image represents the global model in each round. The third row depicts how a single client's intention point is calculated based on multi-round historical data, where each ray signifies the flattened results of the model updates, and the hypersphere's center represents the current client's intention point.}

\subsubsection{LOF-driven malicious intention detection}
% “Abnormal Detection”的主要过程基于局部离群因子（Local Outlier Factor, LOF）得到异常点。LOF算法是一种基于密度的异常检测方法，算法的核心思想是通过计算一个数值$lof_i$来反应节点$i$的异常程度。$lof_i$大致意思是节点$i$目的意图点$O_i$周围意图点所处位置的平均密度比上$O_i$所在位置的密度。比值越大于$1$，则$O_i$所在位置的密度越小于周围其他意图点所在位置的密度，即$O_i$越有可能是异常点。首先定义意图点$O_i$的$q$距离$\widetilde{dis_i^q}$，假设高维空间中存在节点$j$的意图点$O_j$，则有$O_j$与$O_i$之间的距离为$\|O_i-O_j\|$，如果满足以下两个条件，我们就认为$\widetilde{dis_i^q}=\|O_i-O_j\|$：
The main process of Abnormal Detection identifies outliers based on the Local Outlier Factor (LOF). The LOF algorithm is a density-based anomaly detection method. Its core idea is to calculate a value $lof_i$ to reflect the degree of abnormality of node $i$. The value of $lof_i$ represents the ratio of the average density of the intention points around the intention point $O_i$ to the density at the location of $O_i$. The greater this ratio is than $1$, the lower the density around $O_i$ compared to the surrounding intention points, indicating that $O_i$ is more likely to be an outlier. First, we define the $q$-distance $\widetilde{dis_i^q}$ of the intention point $O_i$. Assume there is an intention point $O_j$ for node $j$ in the high-dimensional space, and the distance between $O_j$ and $O_i$ is $\|O_i-O_j\|$. If the following two conditions are satisfied, we have $\widetilde{dis_i^q} = \|O_i-O_j\|$:

% \begin{itemize}
%     \item 在样本空间中，至少存在$q$个意图点$O_j'$，使得$\|O_i-O_j'\|\leq \|O_i-O_j\|$, 其中$j'\neq i$；
%     \item 在样本空间中，至多存在$q-1$个意图点$O_j'$，使得$\|O_i-O_j'\|< \|O_i-O_j\|$, 其中$j'\neq i$。
% \end{itemize}

\begin{itemize}
    \item In the sample space, there are at least $q$ intention points $O_j'$ such that $\|O_i-O_j'\| \leq \|O_i-O_j\|$, where $j' \neq i$;
    \item There are at most $q-1$ intention points $O_j'$ such that $\|O_i-O_j'\| < \|O_i-O_j\|$, where $j' \neq i$.
\end{itemize}

% 总的来说，$O_i$的$q$距离$\widetilde{dis_i^q}$表示高维空间中距离第$q$远的点。之后定义意图点$O_i$的第$q$距离邻域$Nei_i$为到$O_i$距离不超过$\widetilde{dis_i^q}$的所有意图点的集合。由于可能同时存在多个第$q$距离的数据，所以$|Nei_i|\geq q$。可以想象，离群度越大的意图点的$q$距离往往较大，离群度越小的意图点的$q$距离往往较小。之后定义意图点$O_i$相对于意图点$O_j$的可达距离：
In summary, the $q$-distance $\widetilde{dis_i^q}$ of $O_i$ represents the distance to the $q$-th farthest point in the high-dimensional space. Then, we define the $q$-distance neighborhood $Nei_i$ of the intention point $O_i$ as the set of all intention points whose distance to $O_i$ does not exceed $\widetilde{dis_i^q}$. Since multiple data points at the $q$-distance may exist simultaneously, $|Nei_i| \geq q$. It can be imagined that the $q$-distance of intention points with greater outlier degrees is often larger, while those with smaller outlier degrees tend to have smaller $q$-distances. Next, we define the reachable distance of intention point $O_i$ relative to point $O_j$:

\begin{equation}
rea_{i,j}=\max\{\widetilde{dis_i^q}, \|O_i-O_j\|\},
\end{equation}

% 也就是说，如果意图点$O_j$远离意图点$O_i$，则两者之间的可达距离就是他们之间的实际距离$\|O_i-O_j\|$；而如果二者距离足够近，则可达距离用意图点$O_i$的$q$距离$\widetilde{dis_i^q}$代替。之后定义意图点$O_i$的局部可达密度$lrd_i$为其$Nei_i$所有意图点的平均可达距离的倒数，即：
In another word, if the intention point $O_j$ is far from the intention point $O_i$, the reachable distance between them is their actual distance $\|O_i-O_j\|$; if they are close enough, the reachable distance is replaced by the $q$-distance $\widetilde{dis_i^q}$ of the intention point $O_i$. Next, we define the local reachable density $lrd_i$ of the intention point $O_i$ as the reciprocal of the average reachable distance of all intention points in its $Nei_i$:

\begin{equation}
lrd_i=1/(\frac{\sum_{j\in Nei_i} rea_{i,j}}{|Nei_i|})
\end{equation}

% 此时，若有重复点，则可能导致$lrd$变为无限大。$lrd_i$的可以理解为意图点$O_i$所处位置的密度，密度越高则意图点$O_i$越有可能属于同一簇，密度越低则意图点$O_i$越有可能是离群点。也就是说，如果意图点$O_i$和周围邻域点是同一簇，则可达距离可能为较小的$\widetilde{dis_i^q}$，导致可达距离之和较小，密度值较高；如果$O_i$和周围邻居意图点较远，则可达距离可能会取较大的$\|O_i-O_j\|$，导致可达距离之和较大，密度值较低，越有可能是离群点。最后，我们定义意图点$O_i$的局部离群因子$lof_i$为其$Nei_i$所有意图点的局部可达密度与其自身局部可达密度的比值的平均值，即：
At this point, if there are duplicate points, $lrd$ may become infinite. $lrd_i$ can be understood as the density at the location of intention point $O_i$. The higher the density, the more likely intention point $O_i$ belongs to the same cluster; the lower the density, the more likely $O_i$ is an outlier. In other words, if intention point $O_i$ and its surrounding neighborhood points are in the same cluster, the reachable distance is likely to be the smaller $\widetilde{dis_i^q}$, resulting in a smaller sum of reachable distances and a higher density value; if $O_i$ is far from its neighboring intention points, the reachable distance is likely to take the larger $\|O_i-O_j\|$, resulting in a larger sum of reachable distances and a lower density value, making it more likely to be an outlier. Finally, we define the local outlier factor $lof_i$ of intention point $O_i$ as the average of the ratio of the local reachable densities of all intention points in its $Nei_i$ to its own local reachable density:

\begin{equation}
    lof_i=\frac{\sum_{j\in Nei_i}\frac{lrd_j}{lrd_i}}{|Nei_i|}=\frac{\sum_{j\in Nei_i} lrd_j}{|Nei_i|\cdot lrd_i}
\end{equation}

% 如果$lof_i$比较接近$1$，则说明意图点$O_i$与其邻域点密度差不多，$O_i$可能和邻域属于同一簇；如果$lof_i$小于$1$，则说明意图点$O_i$的密度高于其邻域点的密度，$O_i$为密集点；如果$lof_i$大于$1$，则说明意图点$O_i$的密度低于其邻域点的密度，$O_i$可能是离群点。总之，LOF算法主要通过比较每个意图点$O_i$和其邻域点的密度来判断$O_i$是否为离群点，密度越低，则越有可能是离群点。而密度主要是通过点之间的距离来计算的，点之间的距离越远密度越低，距离越近密度越高。计算每个节点$i$的局部离群因子$lof_i$，将$lof_i$大于$1$的客户端视为异常客户端，并将其梯度丢弃。
If $lof_i$ is close to $1$, it indicates that the density of intention point $O_i$ is similar to its neighboring points, suggesting that $O_i$ and its neighborhood belong to the same cluster. If $lof_i$ is less than $1$, it means that the density of intention point $O_i$ is higher than its neighboring points, indicating that $O_i$ is a dense point. If $lof_i$ is greater than $1$, it means that the density of intention point $O_i$ is lower than its neighboring points, suggesting that $O_i$ may be an outlier. In conclusion, the LOF algorithm mainly determines whether $O_i$ is an outlier by comparing the density of each intention point $O_i$ with that of its neighboring points. The lower the density, the more likely it is an outlier. Density is primarily calculated based on the distance between points: the greater the distance between points, the lower the density; the closer the distance, the higher the density. After calculating the local outlier factor $lof_i$ for each node $i$, clients with $lof_i$ greater than $1$ are regarded as abnormal clients, and their gradients are discarded.
% “Abnormal Detection”的主要过程如算法\ref{alg:malicious-node-detection-history}的19-30行所示。
% The main process of "Abnormal Detection" is shown in lines 19-30 of Algorithm \ref{alg:malicious-node-detection-history}.

\subsubsection{Secure Aggregation}
% 剔除了异常节点后，依据正常节点的置信度$cre$进行聚合。其中定义：
After removing abnormal nodes, the aggregation is performed based on the confidence $cre$ of normal nodes. The definition is as follows:

\begin{equation}
    cre_i=\frac{1}{r_i+\rho},
\end{equation}
% $\rho$是一个很小的正数，以防分母为$0$。

% 如图\ref{fig:before-agg}所示，使用LOF算法识别恶意客户端可以剔除潜在的恶意节点，但是在正常的节点中，同样存在一些置信度很低的节点。这些节点的最小覆盖球甚至可能和异常节点有一定交集。因此，在聚合的过程中，这些置信度较低的点所占的权重就应该越低。

% 试想，假设有两个节点的最小覆盖超球的半径都很小，但是相差倍数很高，那么它们计算出来的置信度相差倍数也会很高。但其实它们的意图点都十分明确，因此权重应该都比较高且相差不应很大。所以我们可以使用激活函数Tanh来对置信度进行加权处理：

where $\rho$ is a small positive number to prevent the denominator from being zero.

As shown in Figure \ref{fig:before-agg}, the LOF algorithm can identify and remove potential malicious clients, but among the normal nodes, there are still some nodes with very low confidence. The minimum enclosing hypersphere of these nodes may even overlap with the abnormal nodes. Therefore, in the aggregation process, nodes with lower confidence should have lower weights.

Consider the following scenario: suppose two nodes have very small minimum enclosing hypersphere radii, but their radii differ by a significant factor. Consequently, their computed confidences would also differ significantly. However, their intention points are quite clear, so their weights should both be relatively high and not differ too much. Therefore, we can use the Tanh activation function to adjust the confidence weights:

\begin{equation}
    cre_i'=\tanh(cre_i),
\end{equation}

% 激活函数Tanh在$0$到$\infty$范围内是一个上升又快到慢的单调递增函数。当$cre_i\to 0$时$cer_i'\to 0$，也就是说节点$i$的最小覆盖球半径$r_i$很大时该节点在聚合时的权重很小；当$cre_i\to \infty$时$cer_i'\to 1$，也就是说节点$i$的最小覆盖球半径$r_i$很小时该节点在聚合时的权重较大。这样，我们就可以对每个加权处理过的置信度$cer_i'$进行归一化处理，得到权重$w_i$。其中：
The Tanh activation function is a monotonic increasing function that rises quickly at first and then slowly within the range of $0$ to $\infty$. When $cre_i \to 0$, $cer_i' \to 0$, which means that when the minimum enclosing hypersphere radius $r_i$ of node $i$ is large, its weight during aggregation is small. Conversely, when $cre_i \to \infty$, $cer_i' \to 1$, which indicates that when the radius $r_i$ is small, the weight of node $i$ in the aggregation is larger. This allows us to normalize each adjusted confidence $cer_i'$ to obtain the weight $w_i$. The formula is:

\begin{equation}
    w_i=\frac{cre_i'}{\sum_{i=1}^{N}cre_i'},
\end{equation}

% 因此第$t$轮模型的梯度聚合公式为：
Thus, the secure aggregation formula for the model in the $t$-th round is:

\begin{equation}
    \theta_g^t = \theta_g^{t-1}+\sum_{i=1}^{N}w_i\triangledown \theta_i^t,
\end{equation}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{results/prob-history-Visualization of Client Intentions and Anomalies.png}
%     \centering \caption{Results obtained using simulated data}
%     \label{fig:result-of-histry}
% \end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig4.jpg}
    \caption{Benign and Malicious Nodes Before Aggregation}
    \label{fig:before-agg}
\end{figure}
% 如图\ref{fig:before-agg}所示展示了异常检测和梯度聚合的计算流程示意图。图中每个球都是某个节点的意图范围。蓝色的球代表良性节点，红色的球代表异常节点。在进行异常检测时我们只考虑每个节点的意图点，即图中的球心位置。在计算可达距离时，由于意图点$O_{j_1}$距离意图点$O_i$较近，故二者之间的可达距离为意图点$O_i$的$q$距离$\widetilde{dis_i^q}$；由于意图点$O_{j_2}$距离$O_i$较远，所以二者之间的可达距离为二者之间的实际距离$\|O_i-O_{j_2}\|$。意图点$O_i$的密度高于其邻域意图点的密度，$lof_i<1$；而意图点$O_{j_2}$的密度低于其邻域意图点的密度，因此$lof_{j_2} > 1$。
As shown in Figure \ref{fig:before-agg}, the diagram illustrates the computational process of anomaly detection and gradient aggregation. Each sphere in the figure represents the intention range of a specific node. The blue spheres represent benign nodes, while the red spheres represent anomalous nodes. During anomaly detection, we only consider the intention point of each node, which is represented by the center of each sphere in the figure. When calculating the reachable distance, since intention point $O_{j_1}$ is close to intention point $O_i$, the reachable distance between them is the $q$-distance $\widetilde{dis_i^q}$ of intention point $O_i$. Since intention point $O_{j_2}$ is far from $O_i$, the reachable distance between them is the actual distance $\|O_i-O_{j_2}\|$ between them. The density of intention point $O_i$ is higher than that of its neighboring intention points, so $lof_i < 1$. On the other hand, the density of intention point $O_{j_2}$ is lower than that of its neighboring intention points, so $lof_{j_2} > 1$.

% 将异常节点剔除，在聚合良性节点的梯度时，依据正常节点的置信度进行加权处理。图中可以看出，虽然节点$j_1$被划分为了良性节点，但其置信度非常低，其超球甚至已经与异常节点$j_2$有所重合。因此在梯度聚合的时候，其权重较小。
After removing anomalous nodes, the gradients of benign nodes are aggregated with a weighting based on the confidence of the normal nodes. As shown in the figure, although node $j_1$ is classified as a benign node, its confidence is very low, and its hypersphere even overlaps with the anomalous node $j_2$. Therefore, its weight is small during gradient aggregation.

% 我们将算法总结成了伪代码，如算法\ref{alg:malicious-node-detection-history}所示。其中第3-7行是步骤1，表示了保留T轮次历史记录的方法；第8-15行是步骤2，表示了使用最小覆盖超球算法求每个机器人节点意图点和置信度的方法；第16-27行是步骤3，表明了如何通过LOF算法求出正常节点和异常节点；第28-32行是步骤4，展示了依据每个机器人节点置信度进行加权聚合的方法。

We summarize the algorithm in the form of pseudocode, as shown in Algorithm \ref{alg:malicious-node-detection-history}. Lines 3 to 7 correspond to Step 1, which outlines the method for retaining historical records for \(T\) rounds. Lines 8 to 15 correspond to Step 2, which describes the use of the minimum enclosing hypersphere algorithm to determine each robot node's intent point and confidence level. Lines 16 to 27 correspond to Step 3, which explains how to identify normal and abnormal nodes using the LOF algorithm. Finally, lines 28 to 32 correspond to Step 4, which illustrates the method for performing weighted aggregation based on each robot node's confidence level.


\begin{algorithm}
\caption{Attack intention detection and secure aggregation}
\label{alg:malicious-node-detection-history}
\begin{algorithmic}[1]
\State \textbf{Input:} Global model history $\{\theta_g^{t-T}, \dots, \theta_g^{t-1}\}$, historical gradient updates for each client $\{\triangledown \theta_{i}^{t-T}, \dots, \triangledown \theta_{i}^{t-1}\}$
\State \textbf{Output:} Sets of normal clients $U_{nor}$ and malicious clients $U_{mal}$


% \For {Client $i$ \in 55}
\State{\color{CadetBlue}/* \textbf{Construction of updates and model databases */}} 
    \State Record flattened gradients $\bar{\theta_i^t}$ and models $\theta_g^{t-1}$
    % \State $\widetilde{\theta_{i}^{t'-1}}  \gets Flatten(\theta_g^{t'-1})$, ${v}_{i}^{t'}  \gets \overline{\triangledown\theta_{i}^{t'}}$ for $t'  \gets \max\{t-T+1, 1\}, \ldots, t$ \Comment{Construct rays}
    \State Construct rays for each $'$, $\max\{t-T+1, 1\}\leq t' \leq t$


\State{\color{CadetBlue}/* \textbf{Construction of attack intention */}} 

    \State Initialize center $O_{i,0}$ and radius $r_{i,0}$
    \While {Not Converged}
        \State Update center $O_{i,k+1}$ and radius $r_{i,k+1}$
    \EndWhile
    \State  $cre_i  \gets \frac{1}{r_i + \rho}$ \Comment{Calculate confidence}


\State{\color{CadetBlue}/* \textbf{LOF-driven malicious intention detection*/}} 

    \For {Each point $O_j \neq O_i$}
        \State Calculate distance $\|O_i - O_j\|$
    \EndFor
    \State Determine $q$-distance $\widetilde{dis_i^q}$ for $O_i$
    \State Find neighbors $Nei_i$ within $\widetilde{dis_i^q}$
    \State $rea_{i,j}  \gets \max\{\widetilde{dis_i^q}, \|O_i - O_j\|\}$ for $O_j \in Nei_i$  \Comment{Compute reachability distance}
    \State  $lrd_i  \gets \frac{|Nei_i|}{\sum_{j \in Nei_i} rea_{i,j}}$ \Comment{Calculate reachability density}
    \State $lof_i  \gets \frac{\sum_{j \in Nei_i} \frac{lrd_j}{lrd_i}}{|Nei_i|}$

% \For {Client $i$}
%     \State Compute $lof_i = \frac{\sum_{j \in Nei_i} \frac{lrd_j}{lrd_i}}{|Nei_i|}$
%     \If {$lof_i > 1$}
%         \State Mark client $i$ as malicious, add to $U_{mal}$
%     \Else
%         \State Mark client $i$ as normal, add to $U_{nor}$
%     \EndIf
% \EndFor
% \State $U_{mal} = \{i \,|\, lof_i > 1\}$
% \State $U_{nor} = \{i \,|\, lof_i \leq 1\}$
\State $U_{mal}, U_{nor}  \gets \{i \,|\, lof_i > 1\}, \{i \,|\, lof_i \leq 1\}$

\State{\color{CadetBlue}/* \textbf{Secure aggregation of global FFT model*/}} 
        \State $cre_i'  \gets \tanh(cre_i)$\Comment{Compute adjusted confidence}
        \State $w_i  \gets \frac{cre_i'}{\sum_{i \in U_{nor}} cre_i'}$\Comment{Normalize weights}
\EndFor

\State \textbf{Output:} $\theta_g^t \gets \theta_g^{t-1} + \sum_{i \in U_{nor}} w_i \cdot \triangledown \theta_i^t$\Comment{Aggregate global model}
\end{algorithmic}
\end{algorithm}

% \begin{figure}[!t]
%     \centering
%     % \includegraphics[width=\linewidth]{figures/fig2.pdf}
%     \caption{Benign and Malicious Nodes Before Aggregation}
%     \label{fig2:single}
% \end{figure}

% \subsection{Multi}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/fig3.jpg}
%     \caption{Benign and Malicious Nodes Before Aggregation}
%     \label{fig3:multi}
% \end{figure}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/fig4.jpg}
%     \caption{Benign and Malicious Nodes Before Aggregation}
%     \label{fig4:before-agg}
% \end{figure}



\section{Experimental evaluation}
% This section presents a thorough evaluation of SecFFT on two benchmark datasets, aiming to illustrate its effectiveness. 本实验在一台Windows操作系统的服务器上进行，28核心128GB内存，以及2块NVIDIA GeForce RTX 3090 GPU(24G)。
This section thoroughly evaluates SecFFT on the benchmark dataset to illustrate its effectiveness, which begins by detailing the experimental setup and environment, followed by the analysis of the experimental results.

\subsection{Experimental Setup}
% 在本实验中，我们采用了常用的联邦学习场景进行评估。具体设置如下：
We used the widely adopted FedAvg framework for evaluation. The experiments were conducted on a server running the Windows operating system with 28 cores, 128GB of memory, and two NVIDIA GeForce RTX 3090 GPUs (24GB). The specific settings are as follows:



%\textbf{数据集}：我们使用FMNIST\cite{minist}作为实验数据集，该数据集在联邦学习场景中被广泛使用。FMNIST数据集包含60,000张28x28像素的灰度时尚商品图像，共分为10个类别，其中50,000张为训练集，10,000张为测试集。


\textbf{Dataset:} We employ the FMNIST dataset \cite{minist} as our experimental data, which is widely utilized in federated learning research. The FMNIST dataset consists of 60,000 grayscale images of fashion items, each with a resolution of 28x28 pixels, categorized into 10 distinct classes. Of these, 50,000 images form the training set, while the remaining 10,000 images constitute the test set.





 % \textbf{微调模型}：我们采用CLIP的CLIP-ViT-B/32\cite{huggingface_clip}模型作为微调所使用的预训练模型。CLIP是一种通过对比学习在文本和图像数据上进行预训练的模型，能将自然语言和视觉信息映射到同一特征空间中。CLIP-ViT-B/32基于ViT\cite{vit}架构，包含12层Transformer编码器，隐藏层维度为768，多头注意力头数为12，使用32x32的图像块作为输入。在微调过程中，我们使用LORA\cite{lora}方案仅在Vision Encoder模块插入低秩矩阵，保持Textual Encoder模块不变。我们设定低秩矩阵的秩为16，学习率为$10^{-4}$，以在有限计算资源下高效微调模型，同时符合我们设定的高维场景。
\textbf{Models:} 
% We adopted the \textbf{CLIP-ViT-B/32}\cite{huggingface_clip} model from \cite{clip} as the pre-trained model for federated fine-tuning. CLIP is a vision language model pre-trained on both text and image data through contrastive learning, capable of mapping natural language and visual information into the same feature space. CLIP-ViT-B/32 is based on the ViT\cite{vit} architecture and contains 12 layers of Transformer encoders, with a hidden dimension of 768 and 12 attention heads, using 32x32 image patches as input. During fine-tuning, we used the \textbf{LORA}\cite{lora} method to insert low-rank matrices only in the Vision Encoder module, keeping the Textual Encoder module unchanged. We set the rank of the low-rank matrices to 16 and the learning rate to \textbf{$10^{-4}$} to efficiently fine-tune the model under limited computational resources, allowing it to better adapt to the data distribution of specific tasks.
We adopt the \textbf{CLIP-ViT-B/32}\cite{huggingface_clip} model from \cite{clip} as the base model for federated fine-tuning. CLIP is a vision-language model pre-trained on both text and image data through contrastive learning, capable of mapping natural language and visual information into a shared feature space. CLIP-ViT-B/32 is based on the ViT\cite{vit} architecture and consists of 12 layers of Transformer encoders, with a hidden dimension of 768 and 12 attention heads, processing 32x32 image patches as input. Meanwhile, we employ the \textbf{LORA}\cite{lora} method to introduce low-rank matrices only into the Vision Encoder module, while keeping the Textual Encoder module unchanged during federated fine-tuning. We set the rank of the low-rank matrices to 16 and the learning rate to \textbf{$10^{-4}$} to efficiently fine-tune the model under limited computational resources, aligning with the high-dimensional setting defined in our experiment.

% \textbf{攻击方式}：对于单轮恶意用户检测，我们使用包括MR\cite{MR}、EDGE CASE\cite{EDGE_CASE}、以及NEUR\cite{zhang2022neurotoxin}在内的三种针对联邦学习场景的最新攻击。MR攻击通过将中毒图像与正常图像混合来创建恶意数据集，并使用投影梯度下降法进行本地训练来达到植入后门的效果；EDGE CASE攻击利用罕见的“边缘情况”样本，通过数据或模型投毒在联邦学习模型中插入后门；Neurotoxin攻击则尝试通过识别在正常节点训练中不常更新的参数，并利用这些参数插入后门，以提高后门的持久性。For Instantaneous Attack Behavior Perception,?
\textbf{Attack paradigms:} we employed three state-of-the-art attacks targeting federated learning scenarios, namely \textbf{MR}\cite{MR}, \textbf{EDGE CASE}\cite{EDGE_CASE}, and \textbf{NEUR}\cite{zhang2022neurotoxin}. The MR attack generates a malicious dataset by combining poisoned images with normal ones and employing the projected gradient descent method for local training to implant backdoors; the EDGE CASE attack employs rare "edge-case" samples to inject backdoors into the federated learning model through data or model poisoning; the NEUR attack seeks to identify parameters that are seldom updated during normal node training and uses these parameters to insert backdoors, thereby enhancing the persistence of the backdoor.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/lab1-combined.pdf}
    \caption{ASR and TSR of various defense mechanisms (FedAVG, FLTrust, Foolsgold, Flame, SecFFT) under three attacks (MR, NEUR, EDGE CASE).}
    \label{fig:lab1}
\end{figure*}

% \textbf{对比算法}：对于单轮恶意用户检测算法，我们对比了三种聚合算法：FedAVG\cite{FedAVG}、FLTrust\cite{FLTrust}和Flame\cite{FLAME}。其中FedAVG根据每个节点数据集的大小对其上传的更新进行加权聚合；FLTrust利用一个干净的数据集，仅聚合那些上传更新与该干净数据集上训练所得更新余弦相似度较高的节点；而Flame则通过模型聚类和权重剪裁来估计并注入适量噪声，有效消除联邦学习中的后门攻击。
\textbf{Baseline defense mechanisms:} 
% For Instantaneous Attack Behavior Perception, we compared four aggregation algorithms: \textbf{FedAVG}\cite{FedAVG}, \textbf{Foolsgold}\cite{foolsgold}, \textbf{FLTrust}\cite{FLTrust}, and \textbf{Flame}\cite{FLAME}. FedAVG performs weighted aggregation of the updates uploaded by each node based on the size of their dataset; FoolsGold distinguishes between benign and malicious clients by assessing pairwise similarity among model updates, subsequently down-weighting abnormal updates on the FL server; FLTrust uses a clean dataset and only aggregates updates from nodes whose updates have a high cosine similarity with the updates obtained from the clean dataset; Flame estimates and injects an appropriate amount of noise through model clustering and weight clipping to effectively eliminate backdoor attacks in federated learning.
we compared four aggregation mechanisms: \textbf{FedAVG}\cite{FedAVG}, \textbf{Foolsgold}\cite{foolsgold}, \textbf{FLTrust}\cite{FLTrust}, and \textbf{Flame}\cite{FLAME}. FedAVG conducts weighted aggregation of updates uploaded by each node based on the size of its dataset. FoolsGold differentiates between benign and malicious clients by evaluating pairwise similarity among model updates and subsequently down-weighting anomalous updates on the FL server. FLTrust employs a clean dataset and aggregates only those updates from nodes that exhibit high cosine similarity with the updates derived from the clean dataset. Flame estimates and injects an appropriate amount of noise via model clustering and weight clipping to mitigate backdoor attacks in federated learning effectively.
    
\textbf{Evaluation metrics:}
% 就像其他文章中的评价指标一样，我们使用两个关键指标来评估投毒攻击与防御方法的效果，即ASR（攻击成功率）和 TSR（测试成功率）。ASR表示恶意节点成功诱导全局模型成功预测指定目标的概率。攻击者的目标是最大化ASR，而有效的防御措施则旨在防止ASR上升。TSR表示全局模型在测试集上的预测正确率。攻击者的目标是尽量维持模型的TSR不变，以避免被轻易检测到，而有效的防御措施则不应显著降低全局模型的TSR。
% Similar to evaluation metrics used in other works\cite{tang2023port, jin2024learning, rong2023special}, we use two key metrics to assess the effectiveness of poisoning attacks and defense methods: \textbf{ASR} (Attack Success Rate) and \textbf{TSR} (Test Success Rate). ASR represents the probability that malicious nodes successfully induce the global model to predict the specified target. The goal of the attacker is to maximize ASR, while effective defense measures aim to prevent an increase in ASR. TSR represents the accuracy of the global model on the test set. The attacker's goal is to maintain the TSR of the model as unchanged as possible to avoid easy detection, while effective defense measures should not significantly reduce the TSR.
\textcolor{red}{For single round recognition,} similar to the evaluation metrics employed in other studies\cite{tang2023port, jin2024learning, rong2023special}, we utilize two key metrics to assess the effectiveness of poisoning attacks and defense mechanisms: \textbf{ASR} (Attack Success Rate) and \textbf{TSR} (Test Success Rate). ASR denotes the probability that malicious nodes successfully manipulate the global model to predict a specified target. The attacker aims to maximize the ASR, whereas effective defense mechanisms strive to prevent an increase in ASR. TSR indicates the accuracy of the global model on the test set. The attacker aims to keep the TSR of the model as stable as possible to avoid easy detection, whereas effective defense mechanisms should not significantly degrade the TSR. \textcolor{red}{For multi round recognition, ``Acc" refers to accuracy, which measures the proportion of correct predictions;``Rec" stands for recall, representing the model's ability to correctly identify positive instances;``FPR" denotes the false positive rate, indicating the proportion of negative instances incorrectly predicted as positive;``FNR" refers to the false negative rate, which represents the proportion of positive instances incorrectly predicted as negative;``AUC" stands for the area under the curve, reflecting the model's overall ability to distinguish between positive and negative instances; and``MCC" represents the Matthews correlation coefficient, which provides a balanced evaluation of the model's predictions.}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/lab2-combined.pdf}
    \caption{Visualization of node identification results under three attack scenarios (MR, EDGE CASE, NEUR) for four defense mechanisms (SecFFT, Flame, FLTrust, FoolsGold) using t-SNE dimensionality reduction. The red dots represent misidentified nodes, including both malicious nodes incorrectly identified as benign and benign nodes incorrectly identified as malicious. }
    \label{fig:lab2}
\end{figure*}

\subsection{Model integrity and poisoning resistance}
% 为了验证我们所提方法“Instantaneous Attack Behavior Perception”的有效性，我们分别在MR、EDGE CASE、NEUR三种攻击下进行实验，并与FedAVG(Baseline)、Foolsgold、FLTrust、Flame进行对比，比较不同防御算法下相应的TSR以及ASR。图\ref{fig:lab1}分别展示了攻击相应的实验结果。
To validate the effectiveness of our proposed method SecFFT, we conducted experiments under three types of attack scenarios: MR, EDGE CASE, and NEUR. We compared our method with FedAVG (Baseline), FoolsGold, FLTrust, and Flame to evaluate the corresponding TSR and ASR under different defense mechanisms. The experimental results are shown in Figure \ref{fig:lab1}.




% 从图中可以看出，SecFFT在面对三种攻击场景（MR、NEUROTOXIN、EDGE_CASE）时展现了最强的鲁棒性和出色的模型性能。在所有场景下，SecFFT的攻击成功率（ASR）始终接近于零，测试准确率（TSR）依然维持在高位，这意味着该算法能够有效防御攻击，能够成功识别并防御恶意用户，同时保持全局模型的性能和准确性。相比之下，其他防御算法表现则较为逊色。虽然Flame在降低ASR方面同样表现优异，能有效识别出恶意用户，但其在识别过程中引入了较多噪声，导致全局模型的准确率（TSR）下降，特别是在MR攻击下，性能损失更为显著。FLTrust在应对攻击时几乎完全失效，在面对MR攻击时，尽管刚开始ASR较低，但经过多轮迭代后，ASR迅速上升，未能有效防御，在面对NEUR和EDGE_CASE攻击时，ASR从开始就展现出较高值，也无法有效防御。Foolsgold在以上三种攻击时的防御效果均差，从开始就展现出较高的ASR。总体而言，对比的防御算法要么无法有效的抵抗攻击，要么在抵御攻击时未能同时兼顾模型的性能，进一步突显了SecFFT在联邦微调抗后门攻击中的显著优势，成为最优的防御选择。
% As shown in the figure, SecFFT demonstrates the strongest robustness and excellent model performance when faced with three types of attack scenarios (MR, NEUR, EDGE CASE). In all scenarios, SecFFT's ASR remains close to zero, while the TSR stays high, indicating that the algorithm can effectively defend against attacks by successfully identifying and mitigating malicious users while maintaining the performance and accuracy of the global model. In contrast, other defense mechanisms perform less favorably. Although Flame also performs well in reducing ASR and effectively identifying malicious users, it introduces a significant amount of noise during the identification process, which leads to a decrease in the global model's accuracy (TSR), particularly under the MR attack, where the performance loss is more pronounced. FLTrust nearly fails to defend against MR attacks, with ASR rising rapidly, rendering it ineffective. While its performance improves under NEUR and EDGE CASE scenarios, the defense effectiveness remains unstable. FoolsGold exhibits poor defense performance under MR and NEUR attacks, with ASR still high, showing some effectiveness only in the EDGE CASE scenario, but not significantly. Overall, these mechanisms fail to balance model performance while defending against attacks, further highlighting SecFFT's significant advantage, making it the optimal choice for defense.
As depicted in the figure, in all attack scenarios, SecFFT consistently achieves an attack success rate (ASR) close to zero, while the test success accuracy (TSR) remains high. \textcolor{red}{This indicates that SecFFT effectively defends against attacks by accurately identifying and neutralizing malicious users, all while preserving the performance and accuracy of the global model. In contrast, other defense mechanisms perform less effectively. Although Flame also shows strong performance in reducing ASR and mitigating malicious attack, it introduces considerable noise in the identification process, leading to a decrease in TSR, especially under MR attacks, where the performance degradation is more pronounced. FLTrust fails to defend effectively, with ASR initially low in MR attacks but quickly rising after several iterations, ultimately proving ineffective; against NEUR and EDGE CASE attacks, ASR remains high from the outset, demonstrating inadequate defense. Foolsgold also shows poor performance in defending against all three types of attacks, maintaining a high ASR from the beginning. Overall, the comparison highlights that other defense mechanisms either fail to effectively resist attacks or compromise model performance during defense. This further underscores SecFFT's significant advantage in federated fine-tuning for backdoor attack resistance, making it the optimal defense choice.}
% SecFFT exhibits the highest robustness and superior model performance when confronted with three types of attack scenarios (MR, NEUR, EDGE CASE). Across all scenarios, SecFFT's ASR remains near zero, whereas the TSR remains highest, indicating that the algorithm effectively defends against attacks by accurately identifying and mitigating malicious nodes while preserving the performance and accuracy of the global model. Conversely, other defense algorithms perform less effectively. Although Flame is also effective in reducing ASR and identifying malicious nodes, it introduces substantial noise during the identification process, leading to a decline in the global model's accuracy (TSR), especially under the MR attack, where the performance degradation is more pronounced. FLTrust almost fails to defend against MR attacks, with ASR rising rapidly, rendering it ineffective. While its performance improves under NEUR and EDGE CASE scenarios, its defense effectiveness remains inconsistent. FoolsGold demonstrates weak defense performance under MR and NEUR attacks, with ASR remaining high, showing limited effectiveness only in the EDGE CASE scenario. Overall, these algorithms fail to balance model performance and defense against attacks, further underscoring SecFFT's substantial advantage, establishing it as the optimal choice for defense.



\subsection{Performance of Attack Behavior Perception}
% 为了验证我们所提方法在恶意用节点识别方面具有较高的准确度，我们总结了在三种攻击(MR, EDGE CASE, NEUR)下所有轮次中SecFFT对恶意节点的识别情况，并与FoolsGold, FLTrust, and Flame三种防御算法进行了对比。具体对比结果如图\ref{fig:lab2}所示，我们利用t-sne算法将所有轮次中节点的更新降维到了二维平面，图中红色点表示被错误识别的节点，包括恶意节点被识别成良性节点以及良性节点被错误识别成恶意节点。对于无法直接获得节点识别情况的Foolsgold防御算法，我们通过利用KMeans聚合算法将该防御算法过程中分配给每个用户的聚合权重聚成两类，并取最大类作为良性节点其余作为恶意节点的结果作为统计标准。
% To validate the high accuracy of our proposed method in identifying malicious nodes, we summarized the detection results of SecFFT under three types of attacks (MR, EDGE CASE, NEUR) across all rounds and compared them with three defense mechanisms: FoolsGold, FLTrust, and Flame. The specific comparison results are shown in Figure \ref{fig:lab2}. We used the t-SNE algorithm to reduce the dimensionality of the nodes' updates from all rounds to a two-dimensional plane. In the figure, red dots represent misidentified nodes, which include malicious nodes incorrectly identified as benign and benign nodes incorrectly identified as malicious. For the Foolsgold defense algorithm, which does not directly provide node identification results, we used the KMeans clustering algorithm to classify the aggregation weights assigned to each user during the defense process into two clusters, considering the larger cluster as benign nodes and the remaining as malicious nodes for evaluation purposes.
To validate the high accuracy of our proposed method in identifying malicious nodes, we summarized the detection results of SecFFT under three types of attacks (MR, EDGE CASE, NEUR) across all rounds and compared them with three defense mechanisms: FoolsGold, FLTrust, and Flame. The specific comparison results are illustrated in Figure \ref{fig:lab2}. We employed the t-SNE algorithm to reduce the dimensionality of the nodes' updates across all rounds to a two-dimensional plane. In the figure, red dots represent misidentified nodes, which include malicious nodes mistakenly identified as benign and benign nodes mistakenly identified as malicious. For the FoolsGold defense algorithm, which does not directly provide node identification results, we employed the KMeans clustering algorithm to classify the aggregation weights assigned to each user during the defense process into two clusters, considering the larger cluster as benign nodes and the remaining as malicious nodes for evaluation purposes.


% 如图所示，SecFFT 的错误识别率最低，仅为 3%，表明其在各种攻击模式下具有更强的防御能力和鲁棒性，能够准确区分恶意节点和良性节点。而 Flame、FLTrust 和 FoolsGold 的错误识别率分别为 12%、19.5% 和 37.5%，错误识别的节点分布较为广泛，尤其是 FoolsGold 的错误识别率最高，显示出其在应对复杂攻击场景时的防御效果较差。相比之下，SecFFT 在应对多种攻击类型时表现最为优异，能有效降低错误识别率，展示了更好的泛化能力和准确性。
% As shown in the figure, SecFFT has the lowest misidentification rate, at only 3\%, indicating its stronger defense capability and robustness in various attack scenarios, accurately distinguishing between malicious and benign nodes. In contrast, the misidentification rates of Flame, FLTrust, and FoolsGold are 12\%, 19.5\%, and 37.5\%, respectively, with misidentified nodes more widely distributed. Notably, FoolsGold has the highest misidentification rate, demonstrating poor defense performance in handling complex attack scenarios. Comparatively, SecFFT performs the best in dealing with various attack types, effectively reducing the misidentification rate and showing better generalization ability and accuracy.
As illustrated in the figure, comparatively, SecFFT outperforms other mechanisms in handling various attack. SecFFT achieves the lowest misidentification rate, at only 0.2\%, indicating its superior defense capability and robustness across various attack scenarios, accurately distinguishing between malicious and benign nodes. Conversely, the misidentification rates of Flame, FLTrust, and FoolsGold are 8\%, 3.9\%, and 9.5\%, respectively, with misidentified nodes more broadly distributed. Notably, FoolsGold exhibits the highest misidentification rate, demonstrating suboptimal defense performance in managing complex attack scenarios. 

\vspace{-0.28cm}
\subsection{Performance of Attack intention detection}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/lab3-combined.pdf}
    \caption{Detection performance of different defense technologies against intelligent attackers with sophisticated multi-round strategies}
    \label{fig:lab3}
\end{figure*}

%为了验证SecFFT面对具有复杂攻击策略的高级隐蔽后门攻击时候的攻击意图构建和有效防御能力，我们在NEUR攻击的基础之上，分别加上了Size约束跟Angle约束等多轮次策略。如图\ref{fig:lab3}所示，通过对比Foolsgold、Fltrust、Flame等仅基于行为的典型防御机制可以看出，基于意图的SecFFT能够显著提升攻击识别能力。即便部分防御手段如Flame能够对一些高隐蔽攻击行为进行有效识别，在叠加了复杂的多轮次攻击策略后，防御机制的识别能力迅速被绕过。而与之相反，我们的方法即便在面对最复杂的“size+angle”策略时，虽然绝对值变小，但是差距依然显著，因而依旧能够区分出攻击者与正常节点之间意图的不同。
%表\ref{tab:detect} 则详细列出了四种防御手段面对不同策略时的分类量化指标情况，可见，SecFFT不仅能够保持较好的抵御后门攻击的能力，同时也维持了相对最好的判别精度，误判率得到很好限缩，可靠性有保障。
To validate the capability of SecFFT in constructing attack intent and effectively defending against advanced covert backdoor attacks employing complex strategies, we added Size and Angle constraints to the NEUR attack over multiple rounds. As shown in \textcolor{red}{Figure \ref{fig:lab3}}, by comparing with typical behavior-based defense mechanisms such as Foolsgold, FLTrust, and Flame, it is evident that the intent-based SecFFT significantly enhances attack detection capabilities. Even though certain defense methods like Flame can effectively identify some highly covert attack behaviors, their detection performance is quickly bypassed when more complex multi-round attack strategies are introduced. In contrast, our method, even when facing the most sophisticated "size+angle" strategy, maintains a notable difference, enabling the continued distinction between the attacker's intent and that of benign nodes, despite a decrease in absolute values.

Table \ref{tab:detect} provides a detailed quantitative analysis of the classification metrics for the four defense mechanisms against different strategies. It is clear that SecFFT not only consistently preserves its strong backdoor attack mitigation capability but also achieves the best detection accuracy, with significantly reduced false positive rates, ensuring reliable and robust performance.

% \begin{table}[h!]
% \small
% \centering
% \caption{Metrics results with different attack and defense methods.}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{c|c|c|c|c|c|c|c}
% \hline
% Strategy & Defense & Acc. & Rec. & FPR & FNR & AUC & MCC \\ \hline

% \multirow{4}{*}{None} & Foolsgold & 0.90 & 0.67 & 0.00 & 0.33 & 0.83 & 0.76 \\
%                       & FLTrust   & 0.82 & 0.40 & 0.00 & 0.60 & 0.70 & 0.56 \\
%                       & Flame     & 0.90 & 0.67 & 0.00 & 0.33 & 0.83 & 0.76 \\
%                       & SecFFT    & \textbf{1.00} & \textbf{1.00} & 0.00 & \textbf{0.00} & \textbf{1.00} & \textbf{1.00} \\ \hline

% \multirow{4}{*}{Size} & Foolsgold & 0.92 & 0.80 & 0.03 & 0.20 & 0.88 & 0.72 \\ 
%                       & FLTrust   & 0.82 & 0.53 & 0.06 & 0.47 & 0.74 & 0.61 \\ 
%                       & Flame     & 0.92 & 0.73 & \textbf{0.00} & 0.27 & 0.87 & 0.78 \\ 
%                       & SecFFT    & \textbf{0.98} & \textbf{0.93} & \textbf{0.00} & \textbf{0.07} & \textbf{0.96} & \textbf{0.91} \\ \hline

% \multirow{4}{*}{Angle} & Foolsgold & 0.88 & 0.73 & 0.06 & 0.27 & 0.84 & 0.68 \\ 
%                        & FLTrust   & 0.86 & 0.60 & 0.03 & 0.40 & 0.80 & 0.65 \\ 
%                        & Flame     & 0.90 & 0.67 & \textbf{0.00} & 0.33 & 0.83 & 0.76 \\ 
%                        & SecFFT    & \textbf{0.96} & \textbf{0.87} & \textbf{0.00} & \textbf{0.13} & \textbf{0.94} & \textbf{0.88} \\ \hline
% \end{tabular}
% }
% \vspace{1em}
% \begin{tablenotes}
% \scriptsize
% \item ``Acc." stands for Accuracy, ``Rec." for Recall, ``FPR" for False Positive Rate, ``FNR" for False Negative Rate, ``AUC" for Area Under Curve, and ``MCC" for Matthews Correlation Coefficient. The strategies are abbreviated as ``None" (No strategy), ``Size" (Size-limited strategy), and ``Angle" (Angle-limited strategy).
% \end{tablenotes}
% \label{tab:detect}
% \end{table}



\begin{table}[h!]
\small
\centering
\caption{\textcolor{red}{Metrics results with different attack and defense methods.}}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c}
\hline
\textcolor{red}{Strategy} & \textcolor{red}{Defense} & \textcolor{red}{Acc.} & \textcolor{red}{Rec.} & \textcolor{red}{FPR} & \textcolor{red}{FNR} & \textcolor{red}{AUC} & \textcolor{red}{MCC} \\ \hline

\multirow{4}{*}{\textcolor{red}{None}} & \textcolor{red}{Foolsgold} & \textcolor{red}{0.90} & \textcolor{red}{0.67} & \textcolor{red}{0.00} & \textcolor{red}{0.33} & \textcolor{red}{0.83} & \textcolor{red}{0.76} \\
                      & \textcolor{red}{FLTrust}   & \textcolor{red}{0.82} & \textcolor{red}{0.40} & \textcolor{red}{0.00} & \textcolor{red}{0.60} & \textcolor{red}{0.70} & \textcolor{red}{0.56} \\
                      & \textcolor{red}{Flame}     & \textcolor{red}{0.90} & \textcolor{red}{0.67} & \textcolor{red}{0.00} & \textcolor{red}{0.33} & \textcolor{red}{0.83} & \textcolor{red}{0.76} \\
                      & \textcolor{red}{SecFFT}    & \textbf{\textcolor{red}{1.00}} & \textbf{\textcolor{red}{1.00}} & \textcolor{red}{0.00} & \textbf{\textcolor{red}{0.00}} & \textbf{\textcolor{red}{1.00}} & \textbf{\textcolor{red}{1.00}} \\ \hline

\multirow{4}{*}{\textcolor{red}{Size}} & \textcolor{red}{Foolsgold} & \textcolor{red}{0.92} & \textcolor{red}{0.80} & \textcolor{red}{0.03} & \textcolor{red}{0.20} & \textcolor{red}{0.88} & \textcolor{red}{0.72} \\ 
                      & \textcolor{red}{FLTrust}   & \textcolor{red}{0.82} & \textcolor{red}{0.53} & \textcolor{red}{0.06} & \textcolor{red}{0.47} & \textcolor{red}{0.74} & \textcolor{red}{0.61} \\ 
                      & \textcolor{red}{Flame}     & \textcolor{red}{0.92} & \textcolor{red}{0.73} & \textbf{\textcolor{red}{0.00}} & \textcolor{red}{0.27} & \textcolor{red}{0.87} & \textcolor{red}{0.78} \\ 
                      & \textcolor{red}{SecFFT}    & \textbf{\textcolor{red}{0.98}} & \textbf{\textcolor{red}{0.93}} & \textbf{\textcolor{red}{0.00}} & \textbf{\textcolor{red}{0.07}} & \textbf{\textcolor{red}{0.96}} & \textbf{\textcolor{red}{0.91}} \\ \hline

\multirow{4}{*}{\textcolor{red}{Angle}} & \textcolor{red}{Foolsgold} & \textcolor{red}{0.88} & \textcolor{red}{0.73} & \textcolor{red}{0.06} & \textcolor{red}{0.27} & \textcolor{red}{0.84} & \textcolor{red}{0.68} \\ 
                       & \textcolor{red}{FLTrust}   & \textcolor{red}{0.86} & \textcolor{red}{0.60} & \textcolor{red}{0.03} & \textcolor{red}{0.40} & \textcolor{red}{0.80} & \textcolor{red}{0.65} \\ 
                       & \textcolor{red}{Flame}     & \textcolor{red}{0.90} & \textcolor{red}{0.67} & \textbf{\textcolor{red}{0.00}} & \textcolor{red}{0.33} & \textcolor{red}{0.83} & \textcolor{red}{0.76} \\ 
                       & \textcolor{red}{SecFFT}    & \textbf{\textcolor{red}{0.96}} & \textbf{\textcolor{red}{0.87}} & \textbf{\textcolor{red}{0.00}} & \textbf{\textcolor{red}{0.13}} & \textbf{\textcolor{red}{0.94}} & \textbf{\textcolor{red}{0.88}} \\ \hline
\end{tabular}
}
\vspace{1em}
\begin{tablenotes}
\scriptsize
\item ``Acc." stands for Accuracy, ``Rec." for Recall, ``FPR" for False Positive Rate, ``FNR" for False Negative Rate, ``AUC" for Area Under Curve, and ``MCC" for Matthews Correlation Coefficient. The strategies are abbreviated as ``None" (No strategy), ``Size" (Size-limited strategy), and ``Angle" (Angle-limited strategy).
\end{tablenotes}
\label{tab:detect}
\end{table}


\section{Conclusion}
%。本文聚焦于Covert backdoor attacks对于IoRT网络中进行视觉大模型联邦微调过程的威胁，针对性地提出了SecFFT防御架构。不同于现有的浅层信息捕捉的防御方法，我们抓住了频域分布差异中隐含的深层语义进行攻击感知，在此基础上创新地重构出攻击者的意图，从而在保障微调业务模型精度的同时，有效识别和阻断后门攻击，提升可靠性和完整性。在公有数据集上的系列实验表明，我们的方案相比现有技术，在性能提升、误判控制和污染防治方面均取得明显提升。未来方向方面，尽管SecFFT实现了对攻击意图的良好捕捉，但是目前仅仅是较为单一的网络场景。真实IoRT网络中的诸多特性，比如异构、多模态等因素并未纳入考虑，可能对防御性能产生影响。因此，未来将会更加贴近真实网络环境，设计实现异构容忍、抗噪声扰动的强鲁棒意图识别和防御方案。
This paper focuses on the threat posed by covert backdoor attacks during the federated fine-tuning of large vision language models within IoRT networks and proposes the SecFFT defense framework. Unlike existing defense methods that capture only shallow-level information, our approach leverages the latent deep semantic differences in frequency-domain distributions to enhance attack detection. Building on this, we innovatively reconstruct the attacker’s intent, enabling effective backdoor attack detection while preserving the fine-tuning model's accuracy and improving its reliability and integrity. 
A series of experiments demonstrate that our approach significantly outperforms existing techniques. %in terms of performance improvement, false-positive control, and backdoor mitigation. 

Regarding future directions, although SecFFT demonstrates effective intent detection, it currently targets relatively homogeneous network scenarios. Real-world IoRT networks exhibit characteristics such as heterogeneity and multimodality, which are not yet fully considered and may impact defense performance. Hence, future work will focus on designing and implementing robust intent recognition and defense strategies that are tolerant to heterogeneity and resilient to noise disturbances, closely aligning with real network environments.



\section*{Acknowledgments}
This work is partially supported by China Postdoctoral Science Foundation (grant No. 2024M750259), Beijing Natural Science Foundation (grant No. 4244084), National Natural Science Foundation of China (grant No. 62401075, 62394322).


\bibliographystyle{IEEEtran}
\bibliography{ref}

% {\appendix[Proof of the Zonklar Equations]
% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}



% \section{References Section}
% You can use a bibliography generated by BibTeX as a .bbl file.
%  BibTeX documentation can be easily obtained at:
%  http://mirror.ctan.org/biblio/bibtex/contrib/doc/
%  The IEEEtran BibTeX style support page is:
%  http://www.michaelshell.org/tex/ieeetran/bibtex/
 
 % argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% \section{Simple References}
% You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
%  (used to reserve space for the reference number labels box).

% \begin{thebibliography}{1}
% \bibliographystyle{IEEEtran}

% \bibitem{ref1}
% {\it{Mathematics Into Type}}. American Mathematical Society. [Online]. Available: https://www.ams.org/arc/styleguide/mit-2.pdf

% \bibitem{ref2}
% T. W. Chaundy, P. R. Barrett and C. Batey, {\it{The Printing of Mathematics}}. London, U.K., Oxford Univ. Press, 1954.

% \bibitem{ref3}
% F. Mittelbach and M. Goossens, {\it{The \LaTeX Companion}}, 2nd ed. Boston, MA, USA: Pearson, 2004.

% \bibitem{ref4}
% G. Gr\"atzer, {\it{More Math Into LaTeX}}, New York, NY, USA: Springer, 2007.

% \bibitem{ref5}M. Letourneau and J. W. Sharp, {\it{AMS-StyleGuide-online.pdf,}} American Mathematical Society, Providence, RI, USA, [Online]. Available: http://www.ams.org/arc/styleguide/index.html

% \bibitem{ref6}
% H. Sira-Ramirez, ``On the sliding mode control of nonlinear systems,'' \textit{Syst. Control Lett.}, vol. 19, pp. 303--312, 1992.

% \bibitem{ref7}
% A. Levant, ``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proc. 45th IEEE Conf. Decis.
% Control}, San Diego, CA, USA, 2006, pp. 5585--5590. DOI: 10.1109/CDC.2006.377165.

% \bibitem{ref8}
% M. Fliess, C. Join, and H. Sira-Ramirez, ``Non-linear estimation is easy,'' \textit{Int. J. Model., Ident. Control}, vol. 4, no. 1, pp. 12--27, 2008.

% \bibitem{ref9}
% R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez, ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proc. Amer. Control Conf.}, Chicago, IL, USA,
% 2000, pp. 2245--2249.

% \end{thebibliography}


\newpage

%\section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/Zan Zhou.jpg}}]{Zan Zhou} received his Ph.D. degree in Computer Science from the Beijing University of Posts and Telecommunications (BUPT) in 2022. 
From 2021 to 2022, he was a visiting student at Nanyang Technology University (NTU), Singapore. 
He is currently a Postdoc researcher with the State Key Laboratory of Networking and Switching Technology, BUPT, Beijing, China. His research interests include data privacy, active defense, and federated learning. %He is Student member of IEEE.
\end{IEEEbiography}

\vspace{11pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/changqiao xu.jpg}}]{Changqiao Xu}
(Senior Member, IEEE) received the Ph.D. degree from the Institute of Software, Chinese Academy of Sciences (ISCAS) in Jan. 2009. He was a researcher at Athlone Institute of Technology and Joint Training PhD at Dublin City University, Ireland during 2007-2009. %He joined BUPT in Dec. 2009. 
Currently, he is a Professor with the State Key Laboratory of Networking and Switching Technology, and Director of the Network Architecture Research Center at BUPT. His research interests include Network Security, Mobile Networking, Multimedia Communications, and Future Internet Technology. He has edited two books and published over 200 technical papers in prestigious international journals and conferences, including IEEE Comm. Magazine, IEEE/ACM ToN, IEEE TMC, INFOCOM, ACM Multimedia, etc. He has served a number of international conferences and workshops as a Co-Chair and TPC member. He is currently serving as the Editor-in-Chief of Transactions on Emerging Telecommunications Technologies (Wiley).
\end{IEEEbiography}

\vspace{11pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/bowang.jpg}}]{Bo Wang} received his B.E. degree from the School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China, in 2023. He is currently pursuing the Ph.D. degree at the same institution. His major research interests include network security and artificial intelligence.
\end{IEEEbiography}

\vspace{11pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/tengfeili.jpg}}]{Tengfei Li} received his B.E. degree from Beijing University of Chemical Technology, Beijing, China, in 2023. He is currently pursuing the M.E. degree at the School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China. His major research interests include network security and artificial intelligence.
\end{IEEEbiography}

\vspace{11pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/Sizhe Huang.jpg}}]{Sizhe Huang} is currently pursuing a Bachelor's degree in the School of Computer Science, BUPT. He is also a research assistant with the State Key Laboratory of Networking and Switching Technology, BUPT. His research interests include network security and active defense.  
\end{IEEEbiography}

\vspace{11pt}



\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/Shujie Yang.jpg}}]{Shujie Yang} received the Ph.D. degree from the Institute of Network Technology, Beijing University of Posts and Telecommunications, Beijing, China, in 2017. He is currently a lecturer with the State Key Laboratory of Networking and Switching Technology. His major research interests include network security and artificial intelligence.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in, clip,keepaspectratio]{photos/Su Yao.jpg}}]{Su Yao} received the Ph.D. degree from the National Engineering Laboratory for Next Generation Internet Interconnection Devices, Beijing Jiaotong University. Currently, he is with the Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, as an Assistant Research Fellow. His research interests include future network architecture, IoT security, and artificial intelligence for network systems.
\end{IEEEbiography}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}



\vfill

% \newpage
% \onecolumn
% \begin{longtable}{|c|p{3cm}|c|p{12cm}|} 
%     \caption{This is the caption for the long table}
%     \label{tabl:attack} \\ \hline
%     Title & Conference & Year & Main Content \\ \hline
%     \endfirsthead \hline
%     Title & Conference & Year & Main Content \\ \hline 
%     \endhead \hline
%    \cite{jha2023label} & neurips & 2023 &  本文提出了一种新颖的标签投毒攻击方法，称为FLIP。与传统的后门攻击不同，FLIP仅通过修改训练数据的标签即可实现对模型的控制，而无需更改图像本身。这种方法特别适用于当训练标签可能来自潜在恶意的第三方（如众包标注或知识蒸馏）的场景。本文通过实验展示了FLIP在多个数据集和模型架构上的高效性，证明了在仅污染少量标签的情况下，FLIP能够显著影响模型的预测结果。 \\ \hline
%    \cite{yang2023data} & International Conference on Machine Learning & 2023 & 本文首次研究了针对多模态模型的投毒攻击，包括视觉和语言两种模态。研究的主要问题是：（1）语言模态是否也容易受到投毒攻击？（2）哪种模态更易受攻击？本文提出了三种针对多模态模型的投毒攻击，并通过在不同数据集和模型架构上的广泛评估，表明这些攻击可以在保持模型实用性的同时实现显著的攻击效果。为缓解这些攻击，本文还提出了预训练和后训练的防御措施，并证明这些防御措施能够显著降低攻击效果，同时保持模型的效用。\\ \hline
%    \cite{dai2023chameleon} & International Conference on Machine Learning & 2023 & 这篇文章的主要贡献在于提出了Chameleon攻击方法，这是一种通过利用正常图像与被污染图像之间的关系，来增强后门在联邦学习（FL）系统中持久性的策略。通过对比学习调整图像嵌入距离，Chameleon成功延长了后门的存续时间，使其在多种数据集、后门类型和模型架构下的耐久性提高了1.2至4倍，显著优于现有方法。 \\ \hline
%    \cite{gu2023gradient} & ACL & 2023 & 这篇文章提出了一种新的梯度控制方法，旨在解决参数高效调优（PET）过程中后门攻击的遗忘问题。通过将后门注入过程视为多任务学习，文章引入了跨层梯度幅度归一化和层内梯度方向投影两种策略，以减少不同任务之间的梯度冲突和梯度大小的不平衡，从而增强后门攻击在用户微调模型后的效果。实验结果表明，该方法在情感分类和垃圾邮件检测任务中显著提高了后门攻击的持久性和有效性。 \\ \hline
%    \cite{zhang2024a3fl} & neurips & 2024 & 这篇文章的主要贡献是提出了一种新的后门攻击方法A3FL，它通过对抗性自适应策略优化后门触发器，使其在联邦学习的全局训练动态中更持久、更难被检测到。与现有方法相比，A3FL显著提高了攻击的成功率和隐蔽性，并在多种防御机制下展现了出色的效果，揭示了现有防御方法的不足，强调了开发新防御策略的必要性。\\ \hline
%    \cite{xu2024shadowcast} & arxiv  & 2024 & 这篇文章主要介绍了一种针对视觉语言模型（VLMs）的隐蔽数据投毒攻击方法，称为Shadowcast。该方法通过向模型的训练数据中注入视觉上与正常图像几乎无法区分的投毒样本，从而误导模型在推理时生成错误或误导性的信息。文章探讨了两种攻击类型：标签攻击（Label Attack）和说服攻击（Persuasion Attack），前者旨在让模型错误识别图像类别，而后者通过生成具有说服力但错误的文本，改变用户对图像的认知。实验结果表明，Shadowcast攻击在多种VLM架构下都非常有效，且在不同的提示词和数据增强条件下依然保持攻击效果。\\ \hline
%    \cite{liang2024badclip} & CVPR & 2024 & 这篇文章提出了一种针对多模态对比学习模型（如CLIP）的双嵌入引导后门攻击方法，称为BadCLIP。BadCLIP通过优化视觉触发模式，使其在嵌入空间中接近目标文本语义，从而在不显著改变模型参数的情况下，植入难以被检测到的后门。此外，该方法通过对抗性训练，增强了中毒样本的视觉特征，使得后门在模型经过清洁数据微调后仍能保持有效。实验结果显示，BadCLIP在多种防御机制下都表现出显著的攻击成功率，展示了其对现有防御方法的强大威胁。\\ \hline

    
%    \cite{attack01} & arXiv preprint & 2024 & 本篇文章次揭示了在联邦学习环境下对大语言模型（LLM）进行指令调优时存在的安全漏洞。文章提出了一种简洁但有效的安全攻击方法，恶意客户端通过使用未对齐的数据来训练本地模型，从而大幅度削弱了全球模型的安全对齐性（“未对齐的数据”，是指那些与预期的安全或伦理规范不一致的数据。例如，未对齐的数据可能包含有害的、误导性的或是不道德的信息，而这些信息在普通情况下不会被用于训练模型）。实验表明，该攻击方法能够将模型的安全性降低高达70\%，而现有的防御方法在应对此类攻击时几乎无效，仅能提高4\%的安全性。为了解决这一问题，作者进一步提出了一种新的事后防御方法，即通过服务器端生成对齐数据并进一步对全局模型进行微调，从而增强模型的安全性（中央服务器在接收到各个客户端的更新后，会主动生成一组对齐的数据。这些对齐的数据是预先定义好的，确保与预期的安全和伦理规范一致。这些数据可能包含严格筛选过的内容，如道德上中立或积极的文本片段）。实验结果显示，这种防御方法能够将模型的安全性提高最多69\%，且不显著降低模型的有效性。（这篇看完了发现不是视觉大模型） \\ \hline
%    \cite{attack02} & arXiv preprint & 2024 & 本篇文章探讨了一种针对大语言模型（LLMs）的新型训练方法，称为目标潜在对抗性训练（Targeted Latent Adversarial Training, LAT），文章提出了通过在模型的潜在表示（latent representations）中引入针对性的扰动，来更有效地消除模型中顽固的不良行为（如后门攻击和模型“越狱”）。(潜在表示是指在神经网络的中间层中，数据通过多层非线性变换后所形成的特征表示。这些表示通常处于更高的抽象层次，与原始输入相比，能够捕捉到数据的深层次特征。在视觉大模型中，这些潜在表示可能包括图像的边缘、形状、纹理等更抽象的特征，而不再是具体的像素值。潜在表示在模型中扮演着至关重要的角色，因为它们是模型用来进行预测和决策的核心特征。)(针对性的扰动是指在训练或评估过程中，特意对模型的输入或潜在表示进行细微的修改或扰动，以诱导模型产生特定的（通常是不希望的）行为。通过这种方法，可以测试和增强模型在面对各种攻击时的鲁棒性。在本文中，作者使用潜在空间中的针对性扰动来模拟攻击，目的是强化模型的防御能力，使其能够抵抗类似的实际攻击，如后门攻击或越狱行为。)研究表明，与传统的对抗性训练相比，目标潜在对抗性训练可以显著提高模型抵抗这些攻击的能力，同时对模型的整体性能影响较小。文章通过实验验证了该方法在增强模型鲁棒性方面的有效性，尤其是在面对未知触发条件的后门攻击时，表现出色。  \\ \hline
%    \cite{attack03} & arXiv preprint & 2024 & 本文探讨了开放权重大语言模型（LLMs）在面对篡改攻击时的脆弱性，并提出了一种名为TAR（Tampering Attack Resistance）的方法，旨在增强这些模型的抗篡改能力。文章指出，现有的安全防护措施，如拒绝机制和偏好训练，容易在少量微调步骤后被攻击者绕过，导致模型被恶意修改。为此，TAR方法通过对抗性训练和元学习，设计了一种新的防护机制，使得即使在经历数千步的微调攻击后，模型仍能保持其原有的安全防护功能。实验结果显示，与现有方法相比，TAR显著提高了模型的抗篡改能力，同时保留了模型的正常功能。研究还通过大量红队评估验证了TAR方法的有效性，展示了其在应对各种复杂攻击时的鲁棒性。(红队评估是一种在网络安全和机器学习领域常用的测试方法，它通过模拟攻击者的行为来评估系统或模型的安全性和防御能力。红队通常扮演“敌方”角色，主动寻找和利用系统的漏洞，以测试系统在真实攻击场景下的表现。这种方法帮助识别和修复安全漏洞，使系统在面对潜在的实际攻击时更加稳健。这篇文章中研究人员通过设计多个测试对手，这些对手模拟了各种可能的攻击策略，试图篡改或破坏大语言模型的功能。文章中提到进行了28个不同的红队评估测试，每个测试都旨在突破TAR的防护机制。)文章中的攻击方式涉及通过微调大语言模型的权重来篡改其行为。攻击者可以在模型的开放权重上进行少量微调，使其在特定情况下产生不希望的输出。例如，攻击者可能会在输入特定触发词时，让模型生成有害内容或偏离其正常功能。\\ \hline
%    \cite{attack04} & arXiv preprint & 2024 & 这是一篇综述。本文主要介绍了以下攻击方式：\begin{itemize}
%     \item 对抗性攻击：通过对输入数据进行微小的扰动，这些扰动虽然对人类几乎不可见，但会导致模型产生显著错误的输出，例如在图像分类中，可能会使模型将一个正常的图像误分类为完全不同的类别；
%     \item 后门攻击：和之前咱做的一样；
%     \item 数据中毒攻击：攻击者向模型的训练数据中注入恶意样本，这些样本会导致模型在遇到类似数据时输出错误结果，例如在物体识别任务中，中毒数据可能会导致模型误将无害物体识别为威胁；
%     \item 模型逃逸：攻击者通过调整输入或模型参数，试图找到绕过模型防御机制的方法，使模型输出不受控制的内容，这种攻击常用于测试模型的防御效果；
%     \item 多模态攻击：针对处理多种类型输入（如文本和图像）的模型，攻击者通过操纵一种模态的输入来影响另一种模态的输出，例如在多模态对话系统中，通过改变图像输入可能会影响系统的文本回应；
%     \item 跨语言攻击：在多语言任务中，攻击者通过在一种语言中引入扰动来影响模型在另一种语言中的表现，这类攻击特别针对多语言翻译或生成模型，可能导致不同语言间的翻译不准确或失真。
%     \end{itemize}\\ \hline
%    \cite{attack05} &  Advances in Neural Information Processing Systems 34 (NeurIPS 2021) & 2021 & 本篇文章讨了如何保护通过“彩票假设”（Lottery Ticket Hypothesis, LTH）找到的稀疏子网络（即“中奖票”）的所有权。文章提出了一种新的基于稀疏结构信息的验证方法，通过在网络的稀疏结构中嵌入签名来进行所有权验证。这种方法能够在白盒和黑盒场景下保护模型的知识产权，并且对细微调整（如微调和剪枝）具有很强的鲁棒性。研究还通过大量实验验证了该方法在多种模型（如ResNet-20、ResNet-18、ResNet-50）和数据集（如CIFAR-10和CIFAR-100）上的有效性，展示了其在应对移除攻击和模糊攻击时的坚韧性。具体攻击方式有：细微调整（Fine-tuning）攻击：对模型进行微调来改变模型的权重值，同时希望不改变网络的稀疏结构。这种攻击旨在通过调整权重，试图使嵌入的签名信息变得不可辨认或无效。然而，由于嵌入的信息是基于网络的稀疏结构（即被剪枝后的模型结构），细微调整难以改变这一基础结构，从而无法有效移除签名。剪枝（Pruning）攻击：攻击者尝试通过进一步剪枝来移除嵌入的签名信息。这种攻击的目的是通过减少模型的非零参数，使得嵌入的结构信息丢失。然而，文章中提出的嵌入方法确保了签名信息在极端稀疏的情况下仍能保留，即使剪枝比例达到一定程度，签名依然可以从稀疏结构中提取出来。模糊攻击（Ambiguity Attacks）：攻击者试图通过制造伪签名或模糊原有签名的信息来混淆所有权验证。这种攻击可能包括添加噪声、篡改稀疏结构等手段，旨在使得验证机制无法区分真实的所有权签名和伪造的信息。然而，文章中的验证方法通过设计稳健的结构嵌入机制，使得这种模糊攻击难以成功。(“签名”指的是嵌入到神经网络稀疏结构中的一种独特的标识信息。这种签名通过在模型的剪枝过程中，利用网络的稀疏性来实现。具体而言，当模型被剪枝后，一部分神经元和连接被移除，剩余的结构会呈现出一种特定的稀疏模式。作者通过在这种稀疏模式中嵌入一个特定的结构或模式，这个模式就是所谓的“签名”。这种签名是不可见的，但可以通过特定的验证过程来提取和识别。其主要功能是为网络的所有权提供证据，类似于给模型打上了一个“水印”。当有人试图非法复制或篡改模型时，这个嵌入的签名仍然可以被识别出来，从而验证模型的归属。签名的鲁棒性设计使其能够抵抗常见的攻击方式（如微调和进一步的剪枝），即使模型经历了这些操作，签名依然可以从其稀疏结构中被提取出来，证明模型的所有权。)(模型的所有权是指对一个机器学习模型（如神经网络模型）所拥有的法律和知识产权。所有权通常由开发者或公司拥有，表示他们对模型的设计、训练数据、训练方法以及最终生成的模型参数等有控制权和排他性使用权。这意味着只有模型的所有者有权利决定如何使用、修改、发布或授权使用该模型。（可能涉及到知识产权保护、商业机密的保密）)\\ \hline
%    \cite{attack06} & Portail HAL theses(theses.hal.science) & 2022 & 这篇文章有185页。本篇文章讨论了如何通过数字水印技术来保护机器学习模型的知识产权，防止模型被盗用。文章首先提供了当前水印技术的概述，并进一步扩展了这些技术在图像分类任务之外的应用，涵盖了回归、机器翻译和强化学习模型。作者还提出了针对模型托管平台的伪造攻击（即试图通过伪造水印来绕过验证）并介绍了一种基于公平性的水印技术，以增强模型在黑盒环境中的安全性。实验结果表明，这些水印技术不仅可以有效防止模型盗用，还能够在面对各种攻击时保持鲁棒性。
%    数字水印是一种嵌入信息的技术，用于在数字内容（如图像、音频、视频或机器学习模型）中隐藏特定的信息，以表明所有权或版权。对于机器学习模型来说，数字水印是一种通过特定算法将标识信息嵌入到模型的权重、结构或输出中的技术。这种标识信息通常是不可见或难以察觉的，但可以通过特定的提取过程来验证。（数字水印的目的有：知识产权保护：开发者可以通过在模型中嵌入水印来证明模型的所有权，防止未经授权的复制和使用；盗版检测：如果一个模型被盗用或未经许可发布，水印可以作为证据，证明模型的来源和合法所有者；内容跟踪：水印可以帮助追踪模型的使用情况，尤其是在多个平台或用户之间共享时，确保模型的使用符合许可协议。） \\ \hline
%     % \endfoot
%     % \hline
%     % \endlastfoot
%     % \hline
% \end{longtable}

% \begin{figure}[!t]
% \centering
% \includegraphics[width=\linewidth]{figures/fig4-history.jpg}
% \caption{Combining multiple rounds of historical identification to identify malicious nodes
% }
% \label{fig4:history}
% \end{figure}


% \begin{figure}[!t]
% \centering
% \includegraphics[width=\linewidth]{img/SecFFT-场景.pdf}
% \caption{Combining multiple rounds of historical identification to identify malicious nodes
% }
% \label{fig4:history}
% \end{figure}


% \begin{figure}[!t]
% \centering
% \includegraphics[width=\linewidth]{img/SecFFT-算法1.pdf}
% \caption{Combining multiple rounds of historical identification to identify malicious nodes
% }
% \label{fig4:history}
% \end{figure}

\end{document}
