{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the warning of torch not compiled with flash attention\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*Torch was not compiled with flash attention.*\"\n",
    ")\n",
    "\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the tensor from file\n",
    "# import torch\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# output_dir = \"cosine_similarity_backdoor_cifar100\"\n",
    "# files = [f\"{output_dir}/gradients_tensor_round_{i}.pt\" for i in range(15)]\n",
    "\n",
    "# for idx, file in enumerate(files):\n",
    "#     gradients  = torch.load(file)\n",
    "\n",
    "#     print(gradients.shape)\n",
    "\n",
    "#     gradients_last_layer = gradients[:, -20481:-1]\n",
    "\n",
    "#     print(gradients_last_layer.shape)\n",
    "\n",
    "\n",
    "#     # t-SNE\n",
    "#     tsne = TSNE(n_components=2, random_state=200, perplexity=40, learning_rate=\"auto\")\n",
    "#     gradients_tsne = tsne.fit_transform(gradients_last_layer.cpu().numpy())\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "\n",
    "#     colors = [\"red\" if i < 7 else \"blue\" for i in range(gradients_tsne.shape[0])]\n",
    "#     plt.scatter(gradients_tsne[:, 0], gradients_tsne[:, 1], c=colors)\n",
    "\n",
    "#     # for j in range(gradients_tsne.shape[0]):\n",
    "#     #     plt.text(gradients_tsne[j, 0], gradients_tsne[j, 1], f\"{j}\", fontsize=9)\n",
    "\n",
    "\n",
    "#     plt.title(f\"t-SNE - round {idx}\")\n",
    "#     plt.xlabel(\"t-SNE 1\")\n",
    "#     plt.ylabel(\"t-SNE 2\")\n",
    "#     plt.savefig(os.path.join(output_dir, f\"tsne_round_{idx}.png\"))\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from transformers import CLIPProcessor\n",
    "# from peft import LoraConfig\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# from peft import get_peft_model\n",
    "\n",
    "# target_modules = []\n",
    "# layers = [f\"vision_model.encoder.layers.{i}\" for i in range(12)]\n",
    "# modules = [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.out_proj\", \"mlp.fc1\", \"mlp.fc2\"]\n",
    "# target_modules.extend([f\"{layer}.{module}\" for layer in layers for module in modules])\n",
    "# target_modules.append(\"visual_projection\")\n",
    "# print(target_modules)\n",
    "\n",
    "\n",
    "# # Define the LoraConfig\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=target_modules,\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "# )\n",
    "# cache_dir = os.path.expanduser(\"~/Desktop/clip_lora/.cache/models\")\n",
    "\n",
    "# base_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=cache_dir)\n",
    "# print(base_model)\n",
    "\n",
    "# local_model = get_peft_model(base_model, config)\n",
    "# local_model.print_trainable_parameters()\n",
    "# print(local_model)\n",
    "\n",
    "# for name, param in local_model.named_parameters():\n",
    "#     print(name, param.size())\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\", \"fc1\", \"fc2\", \"visual_projection\", \"text_projection\"],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "# )\n",
    "# local_model = get_peft_model(base_model, config)\n",
    "# local_model.print_trainable_parameters()\n",
    "# print(local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backdoor on CIFAR-100 with 10 workers and 3 attackers\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from transformers import CLIPProcessor\n",
    "from peft import LoraConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from src import Worker\n",
    "from src import Aggregator\n",
    "from scipy.fftpack import dct\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import get_peft_model\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "torch.manual_seed(200)\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "cache_dir = os.path.expanduser(\"~/Desktop/clip_lora/.cache/models\")\n",
    "\n",
    "output_dir = \"test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# load the processor\n",
    "processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", cache_dir=cache_dir\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert device.type == \"cuda\", \"Please make sure CUDA is available\"\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "cifar100_train = CIFAR100(\n",
    "    os.path.expanduser(\"~\\Desktop\\clip_lora\\.cache\"),\n",
    "    train=True,\n",
    "    transform=lambda image: processor(images=image, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].squeeze(0),\n",
    "    download=False,\n",
    ")\n",
    "cifar100_test = CIFAR100(\n",
    "    os.path.expanduser(\"~\\Desktop\\clip_lora\\.cache\"),\n",
    "    train=False,\n",
    "    transform=lambda image: processor(images=image, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].squeeze(0),\n",
    "    download=False,\n",
    ")\n",
    "cifar100_backdoor_test = CIFAR100(\n",
    "    os.path.expanduser(\"~\\Desktop\\clip_lora\\.cache\"),\n",
    "    train=False,\n",
    "    transform=lambda image: processor(images=image, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].squeeze(0),\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "class_names = cifar100_train.classes\n",
    "train_size = len(cifar100_train)\n",
    "test_size = len(cifar100_test)\n",
    "test_loader = DataLoader(cifar100_test, batch_size=128, shuffle=False)\n",
    "\n",
    "# config for federated learning\n",
    "num_workers = 100\n",
    "rounds = 2\n",
    "round_to_start_attack = 1\n",
    "local_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "attckers = [i for i in range(20)]\n",
    "\n",
    "attack_type = \"backdoor\"\n",
    "attack_params = {\n",
    "    \"trigger_position\": (0, 0),\n",
    "    \"trigger_size\": (1, 1),\n",
    "    \"backdoor_rate\": 0.5,\n",
    "    \"target_label\": 88,\n",
    "    \"trigger_value\": 255,\n",
    "}\n",
    "\n",
    "# Split the train dataset into {num_workers} subsets\n",
    "subset_size = train_size // num_workers\n",
    "remaining = train_size % num_workers\n",
    "subset_lengths = [\n",
    "    subset_size + 1 if i < remaining else subset_size for i in range(num_workers)\n",
    "]\n",
    "train_subsets = random_split(cifar100_train, subset_lengths)\n",
    "\n",
    "logger.info(f\"Train dataset size: {len(cifar100_train)}\")\n",
    "logger.info(f\"Test dataset size: {len(cifar100_test)}\")\n",
    "\n",
    "# put trigger into the test dataset\n",
    "for i in range(len(cifar100_backdoor_test)):\n",
    "    image = cifar100_backdoor_test.data[i]\n",
    "    x, y = attack_params[\"trigger_position\"]\n",
    "    h, w = attack_params[\"trigger_size\"]\n",
    "    image[x : x + h, y : y + w, :] = attack_params[\"trigger_value\"]\n",
    "    cifar100_backdoor_test.data[i] = image\n",
    "    cifar100_backdoor_test.targets[i] = attack_params[\"target_label\"]\n",
    "data_backdoor_loader = DataLoader(cifar100_backdoor_test, batch_size=128, shuffle=False)\n",
    "\n",
    "# get the models\n",
    "target_modules = (\n",
    "    []\n",
    ")  # [\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\", \"fc1\", \"fc2\", \"visual_projection\", \"text_projection\"]\n",
    "layers = [f\"vision_model.encoder.layers.{i}\" for i in range(12)]\n",
    "modules = [\n",
    "    \"self_attn.q_proj\",\n",
    "    \"self_attn.k_proj\",\n",
    "    \"self_attn.v_proj\",\n",
    "    \"self_attn.out_proj\",\n",
    "    \"mlp.fc1\",\n",
    "    \"mlp.fc2\",\n",
    "]\n",
    "target_modules.extend([f\"{layer}.{module}\" for layer in layers for module in modules])\n",
    "target_modules.append(\"visual_projection\")\n",
    "# Define the LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# loccal model\n",
    "base_model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", cache_dir=cache_dir\n",
    ")\n",
    "local_model = get_peft_model(base_model, config)\n",
    "local_model.print_trainable_parameters()\n",
    "local_model.to(device)\n",
    "\n",
    "# target model\n",
    "base_model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", cache_dir=cache_dir\n",
    ")\n",
    "target_model = get_peft_model(base_model, config)\n",
    "target_model.print_trainable_parameters()\n",
    "target_model.to(device)\n",
    "\n",
    "# Create workers for each subset\n",
    "workers = []\n",
    "for idx, train_subset in enumerate(train_subsets):\n",
    "    if idx in attckers:\n",
    "        worker = Worker(\n",
    "            idx=idx,\n",
    "            processor=processor,\n",
    "            model=local_model,\n",
    "            data_set=train_subset,\n",
    "            device=device,\n",
    "            class_names=class_names,\n",
    "            rounds=rounds,\n",
    "            round_to_start_attack=round_to_start_attack,  # epoch_to_start_attack\n",
    "            epochs=local_epochs,\n",
    "            attack_type=attack_type,\n",
    "            attack_params=attack_params,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    else:\n",
    "        worker = Worker(\n",
    "            idx=idx,\n",
    "            processor=processor,\n",
    "            model=local_model,\n",
    "            data_set=train_subset,\n",
    "            device=device,\n",
    "            class_names=class_names,\n",
    "            rounds=rounds,\n",
    "            round_to_start_attack=rounds + 1,\n",
    "            epochs=local_epochs,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    workers.append(worker)\n",
    "\n",
    "# create the aggregator\n",
    "aggregator = Aggregator(\n",
    "    class_names=class_names,\n",
    "    processor=processor,\n",
    "    target_model=target_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "csv_file = os.path.join(output_dir, \"aggregator_summary.csv\")\n",
    "aggregator_summary = []\n",
    "target_model_params = aggregator.get_trainable_params()\n",
    "\n",
    "for round in range(rounds):\n",
    "    gradients = {}\n",
    "    for idx, worker in enumerate(workers):\n",
    "        worker.set_trainable_params(target_model_params)\n",
    "        gradient = worker.train()\n",
    "        gradients[idx] = gradient\n",
    "\n",
    "    flattend_gradients = []\n",
    "    for idx, gradient in gradients.items():\n",
    "        flattend_gradients.append(\n",
    "            torch.cat([params.flatten() for params in gradient.values()])\n",
    "        )\n",
    "\n",
    "    gradients_tensor = torch.stack(flattend_gradients)\n",
    "    # gradients_tensor = torch.stack(gradients)\n",
    "    normalized_gradients = gradients_tensor / gradients_tensor.norm(dim=1, keepdim=True)\n",
    "    cosine_similarity_matrix = (\n",
    "        torch.mm(normalized_gradients, normalized_gradients.T).to(\"cpu\").numpy()\n",
    "    )\n",
    "\n",
    "    # heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cosine_similarity_matrix, annot=False, cmap=\"Reds\", cbar=True)\n",
    "    plt.title(f\"Cosine Similarity Matrix - round {round}\")\n",
    "    plt.xlabel(\"Worker\")\n",
    "    plt.ylabel(\"Worker\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"cosine_similarity_round_{round}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # print the shape of gradients tensor\n",
    "    print(f\"Gradients tensor shape: {gradients_tensor.shape}\")\n",
    "    # save the gradients tensor\n",
    "    torch.save(\n",
    "        gradients_tensor, os.path.join(output_dir, f\"gradients_tensor_round_{round}.pt\")\n",
    "    )\n",
    "\n",
    "    # t-SNE\n",
    "    # tsne = TSNE(n_components=2, random_state=200, perplexity=5)\n",
    "    # grandients_tsne = tsne.fit_transform(gradients_tensor[:, -20481:-1].cpu().numpy()) # only the last layer\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # plt.scatter(grandients_tsne[:, 0], grandients_tsne[:, 1])\n",
    "    # plt.title(f\"t-SNE - round {round}\")\n",
    "    # plt.xlabel(\"t-SNE 1\")\n",
    "    # plt.ylabel(\"t-SNE 2\")\n",
    "    # plt.savefig(os.path.join(output_dir, f\"tsne_round_{round}.png\"))\n",
    "    # plt.close()\n",
    "\n",
    "    gradients_tensor_copy = gradients_tensor.clone()\n",
    "    # DCT transform\n",
    "    gradients_numpy = gradients_tensor_copy.cpu().numpy()\n",
    "    gradients_dct = dct(gradients_numpy, axis=1, norm=\"ortho\")\n",
    "    gradients_tensor_dct = torch.tensor(gradients_dct, dtype=torch.float32).to(device)\n",
    "    # cosine similarity matrix after DCT\n",
    "    normalized_gradients_dct = gradients_tensor_dct / gradients_tensor_dct.norm(\n",
    "        dim=1, keepdim=True\n",
    "    )\n",
    "    cosine_similarity_matrix_dct = (\n",
    "        torch.mm(normalized_gradients_dct, normalized_gradients_dct.T).to(\"cpu\").numpy()\n",
    "    )\n",
    "    # heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cosine_similarity_matrix_dct, annot=False, cmap=\"Reds\", cbar=True)\n",
    "    plt.title(f\"Cosine Similarity Matrix after DCT - round {round + 1}\")\n",
    "    plt.xlabel(\"Worker\")\n",
    "    plt.ylabel(\"Worker\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"cosine_similarity_dct_round_{round}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # t-SNE after DCT\n",
    "    # tsne = TSNE(n_components=2, random_state=200, perplexity=5)\n",
    "    # gradients_tsne_dct = tsne.fit_transform(gradients_tensor_dct.cpu().numpy())\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # plt.scatter(gradients_tsne_dct[:, 0], gradients_tsne_dct[:, 1])\n",
    "    # plt.title(f\"t-SNE after DCT - round {round}\")\n",
    "    # plt.xlabel(\"t-SNE 1\")\n",
    "    # plt.ylabel(\"t-SNE 2\")\n",
    "    # plt.savefig(os.path.join(output_dir, f\"tsne_dct_round_{round}.png\"))\n",
    "    # plt.close()\n",
    "\n",
    "    mean_gradients = gradients_tensor.mean(dim=0)\n",
    "\n",
    "    start = 0\n",
    "    for name, param in target_model_params.items():\n",
    "        end = start + param.numel()\n",
    "        target_model_params[name] = mean_gradients[start:end].reshape(param.shape) + param\n",
    "        start = end\n",
    "    assert start == mean_gradients.numel(), \"The number of parameters does not match\"\n",
    "\n",
    "    aggregator.set_trainable_params(target_model_params)\n",
    "    normal_accuracy, normal_loss = aggregator.eval(test_loader)\n",
    "    print(\n",
    "        f\"Round {round + 1} -Normal Accuracy: {normal_accuracy:.4f}, Loss: {normal_loss:.4f}\"\n",
    "    )\n",
    "    backdoor_accuracy, backdoor_loss = aggregator.eval(data_backdoor_loader)\n",
    "    print(\n",
    "        f\"Round {round + 1} -Backdoor Accuracy: {backdoor_accuracy:.4f}, Loss: {backdoor_loss:.4f}\"\n",
    "    )\n",
    "    aggregator_summary.append(\n",
    "        {\n",
    "            \"normal_accuracy\": normal_accuracy,\n",
    "            \"normal_loss\": normal_loss,\n",
    "            \"backdoor_accuracy\": backdoor_accuracy,\n",
    "            \"backdoor_loss\": backdoor_loss,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# save the summary of aggregator\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(aggregator_summary)\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "# save the summary of worker\n",
    "for idx, worker in workers:\n",
    "    csv_file = os.path.join(output_dir, f\"woker{idx}_summary.csv\")\n",
    "\n",
    "# save the model\n",
    "import datetime\n",
    "save_path = f\"{output_dir}/models_cifar100\"\n",
    "os.makedirs(save_path)\n",
    "\n",
    "aggregator.save(f\"{save_path}/aggregator.pt\")\n",
    "\n",
    "for idx, worker in enumerate(workers):\n",
    "    worker.save(f\"{save_path}/worker_{idx}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import plot_loss_curve, plot_accuracy_curve\n",
    "\n",
    "for idx, worker in enumerate(workers):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    train_loader_length = len(worker.data_loader)\n",
    "    worker.to(torch.device(\"cpu\"))\n",
    "    loss_values = [summary[\"loss\"] for summary in worker.train_summaries]\n",
    "    accuracy_values = [summary[\"accuracy\"] for summary in worker.train_summaries]\n",
    "\n",
    "    plot_loss_curve(axs[0], loss_values)\n",
    "    plot_accuracy_curve(axs[1], accuracy_values)\n",
    "\n",
    "    fig.suptitle(f\"Worker {idx} Training Summary\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "normal_accuracy = [summary[\"normal_accuracy\"] for summary in aggregator_summary]\n",
    "normal_loss = [summary[\"normal_loss\"] for summary in aggregator_summary]\n",
    "backdoor_accuracy = [summary[\"backdoor_accuracy\"] for summary in aggregator_summary]\n",
    "backdoor_loss = [summary[\"backdoor_loss\"] for summary in aggregator_summary]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Accuracy\", color=\"tab:blue\")\n",
    "ax1.plot(normal_accuracy, label=\"Normal Accuracy\", marker=\"o\", color=\"tab:blue\")\n",
    "ax1.plot(backdoor_accuracy, label=\"Backdoor Accuracy\", marker=\"o\", color=\"tab:cyan\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Loss\", color=\"tab:red\")\n",
    "ax2.plot(normal_loss, label=\"Normal Loss\", marker=\"o\", color=\"tab:red\")\n",
    "ax2.plot(backdoor_loss, label=\"Backdoor Loss\", marker=\"o\", color=\"tab:orange\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Training Metrics Over Time\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the model with current time\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.mkdir(current_time)\n",
    "aggregator.save(f\"{current_time}/aggregator.pt\")\n",
    "\n",
    "for idx, worker in enumerate(workers):\n",
    "    worker.save(f\"{current_time}/worker_{idx}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the backdoor attack success rate by one of the attacker's train dataset\n",
    "aggregator.to(device)\n",
    "for idx, worker in enumerate(workers):\n",
    "    print(f\"Accuracy on workers[{idx}].dataset: {aggregator.eval(worker.data_loader)}\")\n",
    "\n",
    "print(f\"Accuracy on test dataset: {aggregator.eval(test_loader)}\")\n",
    "aggregator.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the backdoor attack success rate by sampling from the target class in one of the attacker's train dataset\n",
    "data_set = workers[0].data_set\n",
    "original_data_set = data_set.dataset\n",
    "\n",
    "indices = data_set.indices\n",
    "print(f\"Original dataset size: {len(original_data_set)}\")\n",
    "\n",
    "target_label = attack_params[\"target_label\"]\n",
    "print(f\"Target label: {target_label}\")\n",
    "\n",
    "filtered_indices = [i for i in indices if original_data_set.targets[i] == target_label]\n",
    "print(f\"Filtered dataset size: {len(filtered_indices)}\")\n",
    "\n",
    "filtered_data_set = torch.utils.data.Subset(original_data_set, filtered_indices)\n",
    "data_loader = DataLoader(filtered_data_set, batch_size=128, shuffle=False)\n",
    "print(f\"data_loader size: {len(data_loader)}\")\n",
    "\n",
    "aggregator.to(device)\n",
    "print(f\"Backdoor attack success rate: {aggregator.eval(data_loader)}\")\n",
    "aggregator.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_test = CIFAR100(\n",
    "    os.path.expanduser(\"~\\Desktop\\clip_lora\\.cache\"),\n",
    "    train=False,\n",
    "    transform=lambda image: processor(images=image, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].squeeze(0),\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "\n",
    "# put the trigger into the test dataset\n",
    "for i in range(len(cifar100_test)):\n",
    "    image = cifar100_test.data[i]\n",
    "    x, y = attack_params[\"trigger_position\"]\n",
    "    h, w = attack_params[\"trigger_size\"]\n",
    "    image[x : x + h, y : y + w, :] = attack_params[\"trigger_value\"]\n",
    "    cifar100_test.data[i] = image\n",
    "    cifar100_test.targets[i] = attack_params[\"target_label\"]\n",
    "\n",
    "data_loader = DataLoader(cifar100_test, batch_size=128, shuffle=False)\n",
    "print(f\"Test dataset size: {len(cifar100_test)}\")\n",
    "aggregator.to(device)\n",
    "print(f\"Backdoor attack success rate on test dataset: {aggregator.eval(data_loader)}\")\n",
    "aggregator.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "cifar100_test = CIFAR100(\n",
    "    os.path.expanduser(\"~\\Desktop\\clip_lora\\.cache\"),\n",
    "    train=False,\n",
    "    transform=lambda image: processor(images=image, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].squeeze(0),\n",
    "    download=False,\n",
    ")\n",
    "idx = random.randint(0, len(cifar100_test))\n",
    "\n",
    "original_image, label = cifar100_test.data[idx], cifar100_test[idx][1]\n",
    "# draw the image\n",
    "plt.imshow(Image.fromarray(original_image))\n",
    "plt.title(f\"Original Label: {class_names[label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "image = processor(images=original_image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "predicetion = aggregator.predict(image.unsqueeze(0))\n",
    "print(f\"Predicted Label Before Backdoor: {class_names[predicetion]}\")\n",
    "\n",
    "x, y = attack_params[\"trigger_position\"]\n",
    "h, w = attack_params[\"trigger_size\"]\n",
    "original_image[x : x + 1, y : y + 1, :] = attack_params[\"trigger_value\"]\n",
    "# draw the image\n",
    "plt.imshow(Image.fromarray(original_image))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "image = processor(images=original_image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "predicetion = aggregator.predict(image.unsqueeze(0))\n",
    "print(f\"Predicted Label After Backdoor: {class_names[predicetion]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # backdoor on CIFAR-10 with 10 workers and 3 attackers\n",
    "# import os\n",
    "# import torch\n",
    "# from torch.utils.data import random_split, DataLoader\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from torchvision import transforms\n",
    "# from transformers import CLIPProcessor\n",
    "# from peft import LoraConfig\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from src import Worker\n",
    "# from src import Aggregator\n",
    "\n",
    "\n",
    "# # Load CIFAR-10 dataset\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "# cache_dir = os.path.expanduser(\"~/Desktop/clip_lora/.cache/models\")\n",
    "\n",
    "# output_dir = \"cosine_similarity_backdoor_cifar10\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # load the processor\n",
    "# processor = CLIPProcessor.from_pretrained(\n",
    "#     \"openai/clip-vit-base-patch32\", cache_dir=cache_dir\n",
    "# )\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# cifar10_train = CIFAR10(\n",
    "#     os.path.expanduser(\"~\\Desktop\\clip_lora\\.cache\"),\n",
    "#     train=True,\n",
    "#     transform=lambda image: processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0),\n",
    "#     download=True,\n",
    "# )\n",
    "# cifar10_test = CIFAR10(\n",
    "#     os.path.expanduser(\"~\\Desktop\\clip_lora\\.cache\"),\n",
    "#     train=False,\n",
    "#     transform=lambda image: processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0),\n",
    "#     download=True,\n",
    "# )\n",
    "\n",
    "# class_names = cifar10_train.classes\n",
    "# train_size = len(cifar10_train)\n",
    "# test_size = len(cifar10_test)\n",
    "# test_loader = DataLoader(cifar10_test, batch_size=256, shuffle=False)\n",
    "\n",
    "# # config for federated learning\n",
    "# num_workers = 10\n",
    "# rounds = 15\n",
    "# round_to_start_attack = 5\n",
    "# local_epochs = 10\n",
    "# # Create workers for each subset\n",
    "# workers = []\n",
    "# attckers = [0, 1, 2]\n",
    "\n",
    "# attack_type = \"backdoor\"\n",
    "# attack_params={\"trigger_position\": (0, 0), \"trigger_size\": (15, 15), \"backdoor_rate\": 0.7, \"target_label\": 6, \"trigger_value\": 0}\n",
    "\n",
    "# # Split the train dataset into {num_workers} subsets\n",
    "# subset_size = train_size // num_workers\n",
    "# remaining = train_size % num_workers\n",
    "# print(f\"subset_size: {subset_size}, remaining: {remaining}\")\n",
    "# subset_lengths = [\n",
    "#     subset_size + 1 if i < remaining else subset_size for i in range(num_workers)\n",
    "# ]\n",
    "# train_subsets = random_split(cifar10_train, subset_lengths)\n",
    "\n",
    "# print(f\"Train dataset size: {len(cifar10_train)}\")\n",
    "# print(f\"Test dataset size: {len(cifar10_test)}\")\n",
    "\n",
    "# # Define the LoraConfig\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\", \"fc1\", \"fc2\", \"visual_projection\", \"text_projection\"],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "# )\n",
    "\n",
    "# for idx, train_subset in enumerate(train_subsets):\n",
    "#     if idx in attckers:\n",
    "#         worker = Worker(\n",
    "#             idx = idx,\n",
    "#             data_set = train_subset,\n",
    "#             lora_config=config,\n",
    "#             class_names=class_names,\n",
    "#             cache_dir=cache_dir,\n",
    "#             rounds=rounds,\n",
    "#             round_to_start_attack=round_to_start_attack, # epoch_to_start_attack\n",
    "#             epochs=local_epochs,\n",
    "#             attack_type=attack_type,\n",
    "#             attack_params=attack_params,\n",
    "#         )\n",
    "#     else:\n",
    "#         worker = Worker(\n",
    "#             idx = idx,\n",
    "#             data_set = train_subset,\n",
    "#             lora_config=config,\n",
    "#             class_names=class_names,\n",
    "#             cache_dir=cache_dir,\n",
    "#             rounds=rounds,\n",
    "#             round_to_start_attack= rounds + 1,\n",
    "#             epochs=local_epochs,\n",
    "#         )\n",
    "#     workers.append(worker)\n",
    "\n",
    "# aggregator = Aggregator(config, class_names, cache_dir)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# aggregator_accuracy_summary = []\n",
    "# aggregator_loss_summary = []\n",
    "# for round in range(rounds):\n",
    "#     trainable_params_vector = aggregator.get_trainable_params_vector()\n",
    "#     for worker in workers:\n",
    "#         worker.set_trainable_params_vector(trainable_params_vector)\n",
    "#     gradients = []\n",
    "#     for idx, worker in enumerate(workers):\n",
    "#         worker.to(device)\n",
    "#         gradients.append(worker.train().to(torch.device(\"cpu\")))\n",
    "#         worker.to(torch.device(\"cpu\"))\n",
    "\n",
    "#     gradients_tensor = torch.stack(gradients)\n",
    "#     normalized_gradients = gradients_tensor / gradients_tensor.norm(dim=1, keepdim=True)\n",
    "#     cosine_similarity_matrix = torch.mm(normalized_gradients, normalized_gradients.T)\n",
    "\n",
    "#     # heatmap\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(cosine_similarity_matrix, annot=True, fmt=\".2f\", cmap=\"Reds\", cbar=True)\n",
    "#     plt.title(f\"Cosine Similarity Matrix - round {round + 1}\")\n",
    "#     plt.xlabel(\"Worker\")\n",
    "#     plt.ylabel(\"Worker\")\n",
    "#     plt.savefig(os.path.join(output_dir, f\"cosine_similarity_round_{round + 1}.png\"))\n",
    "#     plt.close()\n",
    "#     mean_gradients = torch.stack(gradients).mean(dim=0)\n",
    "#     trainable_params_vector = trainable_params_vector + mean_gradients\n",
    "#     aggregator.set_trainable_params_vector(trainable_params_vector)\n",
    "#     aggregator.to(device)\n",
    "#     accuracy, loss = aggregator.eval(test_loader)\n",
    "#     aggregator.to(torch.device(\"cpu\"))\n",
    "#     aggregator_accuracy_summary.append(accuracy)\n",
    "#     aggregator_loss_summary.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src import plot_loss_curve, plot_accuracy_curve\n",
    "\n",
    "# for idx, worker in enumerate(workers):\n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "#     train_loader_length = len(worker.data_loader)\n",
    "#     worker.to(torch.device(\"cpu\"))\n",
    "#     loss_values = [summary[\"loss\"] for summary in worker.train_summaries]\n",
    "#     accuracy_values = [summary[\"accuracy\"] for summary in worker.train_summaries]\n",
    "\n",
    "#     plot_loss_curve(axs[0], loss_values)\n",
    "#     plot_accuracy_curve(axs[1], accuracy_values)\n",
    "\n",
    "#     fig.suptitle(f\"Worker {idx} Training Summary\")\n",
    "#     plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
    "# axs[0].plot(aggregator_accuracy_summary, label='Accuracy', color='blue', marker='o')\n",
    "# axs[0].set_title('Aggregator Accuracy Curve')\n",
    "# axs[0].set_xlabel('Round')\n",
    "# axs[0].set_ylabel('Accuracy')\n",
    "# axs[0].grid(True)\n",
    "# axs[0].legend()\n",
    "\n",
    "# axs[1].plot(aggregator_loss_summary, label='Loss', color='red', marker='o')\n",
    "# axs[1].set_title('Aggregator Loss Curve')\n",
    "# axs[1].set_xlabel('Round')\n",
    "# axs[1].set_ylabel('Loss')\n",
    "# axs[1].grid(True)\n",
    "# axs[1].legend()\n",
    "\n",
    "# fig.suptitle(\"Aggregator Performance Over Rounds\")\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "# plt.show()\n",
    "\n",
    "# # Save the model with current time\n",
    "# import datetime\n",
    "\n",
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# os.mkdir(current_time)\n",
    "# aggregator.save(f\"{current_time}/aggregator.pt\")\n",
    "\n",
    "# for idx, worker in enumerate(workers):\n",
    "#     worker.save(f\"{current_time}/worker_{idx}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the test dataset\n",
    "# print(f\"Normal accuracy: {aggregator.eval(test_loader)}\")\n",
    "\n",
    "# # Evaluate the backdoor attack success rate by one of the attacker's train dataset\n",
    "# print(f\"Backdoor attack success rate: {aggregator.eval(workers[0].data_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
