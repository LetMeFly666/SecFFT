\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{longtable}
\usepackage[UTF8]{ctex}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}


\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{ragged2e}
\usepackage{latexsym, amssymb, verbatim, amsmath}
\usepackage{amsmath,bm}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{subeqnarray}
\usepackage{cases}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{color}
\usepackage{cite}
\usepackage[]{chapterbib}
\usepackage{enumerate}

\begin{document}
\subsection{Related work}
\subsubsection{Backdoor attack}
后门攻击对深度学习模型构成了严重的安全威胁，它旨在通过对模型输入添加特殊扰动来诱导目标模型产生不当行为，例如图像分类器中的错误分类等。我们将现有针对模型的后门攻击分为两种，即空间隐蔽性后门攻击和时间隐蔽性后门攻击。

空间隐蔽性后门攻击是指攻击者通过微调模型参数、修改输入特征或操纵模型训练过程等方式，使得现有防御方法无法在特征空间中有效区分恶意攻击者与良性参与者，从而有效逃避检测。Shadowcast\cite{xu2024shadowcast}通过在特征空间中引入微小、不可察觉的扰动，操纵视觉语言模型（VLMs）在特定视觉输入下生成误导性的文本输出。BadClip\cite{liang2024badclip}提出了一种针对多模态对比学习模型（如CLIP）的双嵌入引导后门攻击方法，通过优化视觉触发模式，使其在嵌入空间中接近目标文本语义，从而在不显著改变模型参数的情况下，植入难以被检测到的后门。Flip\cite{jha2023label}通过修改训练数据的标签，使得模型训练轨迹接近于带有后门的专家模型，从而在特征空间中达到隐蔽攻击的效果，即使在图像干净的情况下也能逃避现有检测方法。

时间隐蔽性后门攻击是指攻击者通过调整模型训练过程或优化后门植入等方式，使后门在恶意攻击者结束攻击后仍能长期存在，不被后续良性更新所覆盖，从而持续影响模型行为。 Chameleon\cite{dai2023chameleon}关注如何通过对比学习来调整中毒样本、干扰样本以及促进样本嵌入表示之间的关系，从而提高后门攻击的空间隐蔽性。ImgTrojan\cite{tao2024imgtrojan}通过在视觉语言模型的训练数据中注入少量恶意样本，使得这些后门即使在模型后续的良性训练中仍能保持有效，从而持续影响模型的行为。\cite{gu2023gradient}提出了一种新的梯度控制方法，通过跨层梯度幅度归一化和层内梯度方向投影，解决了参数高效调优（PET）过程中后门攻击的遗忘问题，从而维持攻击的持久性和隐蔽性。A3FL\cite{zhang2024a3fl}通过对抗性适应损失函数来优化触发器的方式，使得后门能够在全局训练动态中持久存在，从而在联邦学习模型中实现高效且持久的攻击效果。
% 总之，现有的后门攻击策略在空间隐蔽性和时间隐蔽性方面都有了显著的进展。空间隐蔽性后门攻击通过优化特征空间中的触发器或梯度，使得恶意行为难以被现有的检测方法发现，从而实现了更高效、更难以检测的攻击。而时间隐蔽性后门攻击则专注于提高后门的持久性，使得即使在恶意攻击结束后，这些后门依然能够在模型中长期存在并继续影响其行为。通过这些攻击方式的结合，攻击者能够在各种应用场景中有效地躲避检测，并确保攻击效果的持续性，这对现有的防御机制提出了更高的挑战，强调了未来在开发更加鲁棒的防御方法方面的迫切需求。

总的来看，现有的后门攻击方法已展现出显著的隐蔽性和持久性，能够有效规避当前的检测机制。这些攻击不仅在特征空间中巧妙隐藏恶意意图，还能在长期训练过程中保持攻击效果，对模型行为产生持续而深远的影响，也对模型安全构成了严峻挑战。这对现有的防御机制提出了更高的挑战，强调了未来在开发更加鲁棒的防御方法方面的迫切需求。

\subsubsection{Defenses}
在当前深度学习模型的安全性研究中，针对后门攻击的防御措施主要可以分为两大类：后门检测和后门消除。% Backdoor Mitigation or Elimination


后门检测主要集中在识别模型中已经存在的后门攻击。在这一类中，SEER\cite{zhu2024seer}提出了一种用于视觉-语言模型的后门检测算法，通过在图像和文本模态的特征空间中联合搜索目标文本和图像触发器，成功实现了在多种场景下的后门检测。ASSET\cite{pan2023asset}提出了一种通过主动诱导后门样本和干净样本在模型行为上的差异，从而实现跨多种深度学习范式的鲁棒后门数据检测方法，并在端到端监督学习、自监督学习和迁移学习中表现出色。DECREE\cite{feng2023detecting}提出了一种用于检测自监督学习中预训练图像编码器后门的创新方法，该方法无需依赖标签数据或下游分类器，并在多种数据集和攻击类型下展现了极高的检测准确率。


后门消除则侧重于通过各种技术手段来消除或减轻后门攻击的影响。Semantic Shield\cite{ishmam2024semantic}通过强制模型在训练时对齐图像区域与外部知识，以防御视觉-语言模型中的后门和数据中毒攻击，从而显著提高了模型的鲁棒性。MCLDef\cite{yue2023model}提出了一种基于模型对比学习的两阶段后门防御方法，通过收缩或破坏中毒数据在特征空间中的聚类，并将中毒数据的特征拉向其干净对应物，从而有效消除深度神经网络中的后门，并在不显著降低模型准确性的情况下提高防御效果。DPoE\cite{liu2023shortcuts}提出了一种基于端到端集成的后门防御框架，通过结合浅层模型和主模型来捕捉和抑制后门触发器，从而有效应对各种显性和隐性的后门攻击，同时减轻噪声标签对模型性能的影响。PSIM\cite{zhao2024defending}提出了一种基于参数高效微调的防御模块，通过利用样本的置信度来识别被后门攻击污染的样本，显著增强了模型抵御权重中毒后门攻击的能力，并在不影响模型准确性的情况下有效过滤出被污染的样本。CleanCLIP\cite{bansal2023cleanclip}通过在视觉和文本编码器上进行无监督微调，独立调整每种模态的表示，以削弱视觉-语言对比学习模型中由后门攻击引入的错误关联，从而有效减少后门攻击的影响，同时保持模型在正常数据上的性能。

尽管这些方法在应对后门攻击方面展现了显著的效果，但它们在面对更隐蔽或复杂的攻击时可能表现不足，某些方法还依赖于外部知识或置信度的设定，导致误判或性能瓶颈，同时在实际应用中也增加了计算开销和训练时间，因此仍需进一步优化以提升整体性能和效率。

% \begin{itemize}
%     % \item catastrophically forget 灾难性遗忘
%     \item detection-based methods, erasing-based methods
%     \item 
% \end{itemize}


\onecolumn
\begin{longtable}{|p{3.5cm}|p{2cm}|p{10.5cm}|} 
    \caption{Attack}
    \label{tabl:attack} \\ \hline
    Title & Conference & Main Content \\ \hline
    \endfirsthead \hline
    Title & Conference & Main Content \\ \hline 
    \endhead \hline
   Flip\cite{jha2023label} & Neurips(2023) &  本文提出了一种新颖的标签投毒攻击方法，称为FLIP。与传统的后门攻击不同，FLIP仅通过修改训练数据的标签即可实现对模型的控制，而无需更改图像本身。这种方法特别适用于当训练标签可能来自潜在恶意的第三方（如众包标注或知识蒸馏）的场景。本文通过实验展示了FLIP在多个数据集和模型架构上的高效性，证明了在仅污染少量标签的情况下，FLIP能够显著影响模型的预测结果。 \\ \hline
   % \cite{yang2023data} & ICML(2023) & 本文首次研究了针对多模态模型的投毒攻击，包括视觉和语言两种模态。研究的主要问题是：（1）语言模态是否也容易受到投毒攻击？（2）哪种模态更易受攻击？本文提出了三种针对多模态模型的投毒攻击，并通过在不同数据集和模型架构上的广泛评估，表明这些攻击可以在保持模型实用性的同时实现显著的攻击效果。为缓解这些攻击，本文还提出了预训练和后训练的防御措施，并证明这些防御措施能够显著降低攻击效果，同时保持模型的效用。\\ \hline
   Chameleon\cite{dai2023chameleon} & ICML(2023) & 这篇文章的主要贡献在于提出了Chameleon攻击方法，这是一种通过利用正常图像与被污染图像之间的关系，来增强后门在联邦学习（FL）系统中持久性的策略。通过对比学习调整图像嵌入距离，Chameleon成功延长了后门的存续时间，使其在多种数据集、后门类型和模型架构下的耐久性提高了1.2至4倍，显著优于现有方法。 \\ \hline
   Gradient Control\cite{gu2023gradient} & ACL(2023) & 这篇文章提出了一种新的梯度控制方法，旨在解决参数高效调优（PET）过程中后门攻击的遗忘问题。通过将后门注入过程视为多任务学习，文章引入了跨层梯度幅度归一化和层内梯度方向投影两种策略，以减少不同任务之间的梯度冲突和梯度大小的不平衡，从而增强后门攻击在用户微调模型后的效果。实验结果表明，该方法在情感分类和垃圾邮件检测任务中显著提高了后门攻击的持久性和有效性。 \\ \hline
   A3FL\cite{zhang2024a3fl} & Neurips(2024) & 这篇文章的主要贡献是提出了一种新的后门攻击方法A3FL，它通过对抗性自适应策略优化后门触发器，使其在联邦学习的全局训练动态中更持久、更难被检测到。与现有方法相比，A3FL显著提高了攻击的成功率和隐蔽性，并在多种防御机制下展现了出色的效果，揭示了现有防御方法的不足，强调了开发新防御策略的必要性。\\ \hline
   Shadowcast\cite{xu2024shadowcast} & Arxiv(2024) & 这篇文章主要介绍了一种针对视觉语言模型（VLMs）的隐蔽数据投毒攻击方法，称为Shadowcast。该方法通过向模型的训练数据中注入视觉上与正常图像几乎无法区分的投毒样本，从而误导模型在推理时生成错误或误导性的信息。文章探讨了两种攻击类型：标签攻击（Label Attack）和说服攻击（Persuasion Attack），前者旨在让模型错误识别图像类别，而后者通过生成具有说服力但错误的文本，改变用户对图像的认知。实验结果表明，Shadowcast攻击在多种VLM架构下都非常有效，且在不同的提示词和数据增强条件下依然保持攻击效果。\\ \hline
   BadCLIP\cite{liang2024badclip} & CVPR(2024) & 这篇文章提出了一种针对多模态对比学习模型（如CLIP）的双嵌入引导后门攻击方法，称为BadCLIP。BadCLIP通过优化视觉触发模式，使其在嵌入空间中接近目标文本语义，从而在不显著改变模型参数的情况下，植入难以被检测到的后门。此外，该方法通过对抗性训练，增强了中毒样本的视觉特征，使得后门在模型经过清洁数据微调后仍能保持有效。实验结果显示，BadCLIP在多种防御机制下都表现出显著的攻击成功率，展示了其对现有防御方法的强大威胁。\\ \hline
   ImgTrojan\cite{tao2024imgtrojan} & arxiv(2024) & 文章主要讨论了一种名为“ImgTrojan”的攻击方法，该方法旨在通过在视觉语言模型（VLM）的训练数据中注入恶意的图像-文本对，从而突破模型的安全屏障。这种方法通过数据投毒的方式，将原本无害的图像与恶意的文本提示关联起来，使得经过投毒训练的模型在遇到这些图像时，可能生成有害的内容。这种攻击方式在实验中展示了极高的成功率，即使在训练数据中仅包含极少量的投毒样本。 \\ \hline
   % \cite{attack01} & arXiv preprint & 2024 & 本篇文章次揭示了在联邦学习环境下对大语言模型（LLM）进行指令调优时存在的安全漏洞。文章提出了一种简洁但有效的安全攻击方法，恶意客户端通过使用未对齐的数据来训练本地模型，从而大幅度削弱了全球模型的安全对齐性（“未对齐的数据”，是指那些与预期的安全或伦理规范不一致的数据。例如，未对齐的数据可能包含有害的、误导性的或是不道德的信息，而这些信息在普通情况下不会被用于训练模型）。实验表明，该攻击方法能够将模型的安全性降低高达70\%，而现有的防御方法在应对此类攻击时几乎无效，仅能提高4\%的安全性。为了解决这一问题，作者进一步提出了一种新的事后防御方法，即通过服务器端生成对齐数据并进一步对全局模型进行微调，从而增强模型的安全性（中央服务器在接收到各个客户端的更新后，会主动生成一组对齐的数据。这些对齐的数据是预先定义好的，确保与预期的安全和伦理规范一致。这些数据可能包含严格筛选过的内容，如道德上中立或积极的文本片段）。实验结果显示，这种防御方法能够将模型的安全性提高最多69\%，且不显著降低模型的有效性。（这篇看完了发现不是视觉大模型） \\ \hline
   % \cite{attack02} & arXiv preprint & 2024 & 本篇文章探讨了一种针对大语言模型（LLMs）的新型训练方法，称为目标潜在对抗性训练（Targeted Latent Adversarial Training, LAT），文章提出了通过在模型的潜在表示（latent representations）中引入针对性的扰动，来更有效地消除模型中顽固的不良行为（如后门攻击和模型“越狱”）。(潜在表示是指在神经网络的中间层中，数据通过多层非线性变换后所形成的特征表示。这些表示通常处于更高的抽象层次，与原始输入相比，能够捕捉到数据的深层次特征。在视觉大模型中，这些潜在表示可能包括图像的边缘、形状、纹理等更抽象的特征，而不再是具体的像素值。潜在表示在模型中扮演着至关重要的角色，因为它们是模型用来进行预测和决策的核心特征。)(针对性的扰动是指在训练或评估过程中，特意对模型的输入或潜在表示进行细微的修改或扰动，以诱导模型产生特定的（通常是不希望的）行为。通过这种方法，可以测试和增强模型在面对各种攻击时的鲁棒性。在本文中，作者使用潜在空间中的针对性扰动来模拟攻击，目的是强化模型的防御能力，使其能够抵抗类似的实际攻击，如后门攻击或越狱行为。)研究表明，与传统的对抗性训练相比，目标潜在对抗性训练可以显著提高模型抵抗这些攻击的能力，同时对模型的整体性能影响较小。文章通过实验验证了该方法在增强模型鲁棒性方面的有效性，尤其是在面对未知触发条件的后门攻击时，表现出色。  \\ \hline
   % \cite{attack03} & arXiv preprint & 2024 & 本文探讨了开放权重大语言模型（LLMs）在面对篡改攻击时的脆弱性，并提出了一种名为TAR（Tampering Attack Resistance）的方法，旨在增强这些模型的抗篡改能力。文章指出，现有的安全防护措施，如拒绝机制和偏好训练，容易在少量微调步骤后被攻击者绕过，导致模型被恶意修改。为此，TAR方法通过对抗性训练和元学习，设计了一种新的防护机制，使得即使在经历数千步的微调攻击后，模型仍能保持其原有的安全防护功能。实验结果显示，与现有方法相比，TAR显著提高了模型的抗篡改能力，同时保留了模型的正常功能。研究还通过大量红队评估验证了TAR方法的有效性，展示了其在应对各种复杂攻击时的鲁棒性。(红队评估是一种在网络安全和机器学习领域常用的测试方法，它通过模拟攻击者的行为来评估系统或模型的安全性和防御能力。红队通常扮演“敌方”角色，主动寻找和利用系统的漏洞，以测试系统在真实攻击场景下的表现。这种方法帮助识别和修复安全漏洞，使系统在面对潜在的实际攻击时更加稳健。这篇文章中研究人员通过设计多个测试对手，这些对手模拟了各种可能的攻击策略，试图篡改或破坏大语言模型的功能。文章中提到进行了28个不同的红队评估测试，每个测试都旨在突破TAR的防护机制。)文章中的攻击方式涉及通过微调大语言模型的权重来篡改其行为。攻击者可以在模型的开放权重上进行少量微调，使其在特定情况下产生不希望的输出。例如，攻击者可能会在输入特定触发词时，让模型生成有害内容或偏离其正常功能。\\ \hline
   % \cite{attack04} & arXiv preprint & 2024 & 这是一篇综述。本文主要介绍了以下攻击方式：\begin{itemize}
   %  \item 对抗性攻击：通过对输入数据进行微小的扰动，这些扰动虽然对人类几乎不可见，但会导致模型产生显著错误的输出，例如在图像分类中，可能会使模型将一个正常的图像误分类为完全不同的类别；
   %  \item 后门攻击：和之前咱做的一样；
   %  \item 数据中毒攻击：攻击者向模型的训练数据中注入恶意样本，这些样本会导致模型在遇到类似数据时输出错误结果，例如在物体识别任务中，中毒数据可能会导致模型误将无害物体识别为威胁；
   %  \item 模型逃逸：攻击者通过调整输入或模型参数，试图找到绕过模型防御机制的方法，使模型输出不受控制的内容，这种攻击常用于测试模型的防御效果；
   %  \item 多模态攻击：针对处理多种类型输入（如文本和图像）的模型，攻击者通过操纵一种模态的输入来影响另一种模态的输出，例如在多模态对话系统中，通过改变图像输入可能会影响系统的文本回应；
   %  \item 跨语言攻击：在多语言任务中，攻击者通过在一种语言中引入扰动来影响模型在另一种语言中的表现，这类攻击特别针对多语言翻译或生成模型，可能导致不同语言间的翻译不准确或失真。
   %  \end{itemize}\\ \hline
   % \cite{attack05} &  Advances in Neural Information Processing Systems 34 (NeurIPS 2021) & 2021 & 本篇文章讨了如何保护通过“彩票假设”（Lottery Ticket Hypothesis, LTH）找到的稀疏子网络（即“中奖票”）的所有权。文章提出了一种新的基于稀疏结构信息的验证方法，通过在网络的稀疏结构中嵌入签名来进行所有权验证。这种方法能够在白盒和黑盒场景下保护模型的知识产权，并且对细微调整（如微调和剪枝）具有很强的鲁棒性。研究还通过大量实验验证了该方法在多种模型（如ResNet-20、ResNet-18、ResNet-50）和数据集（如CIFAR-10和CIFAR-100）上的有效性，展示了其在应对移除攻击和模糊攻击时的坚韧性。具体攻击方式有：细微调整（Fine-tuning）攻击：对模型进行微调来改变模型的权重值，同时希望不改变网络的稀疏结构。这种攻击旨在通过调整权重，试图使嵌入的签名信息变得不可辨认或无效。然而，由于嵌入的信息是基于网络的稀疏结构（即被剪枝后的模型结构），细微调整难以改变这一基础结构，从而无法有效移除签名。剪枝（Pruning）攻击：攻击者尝试通过进一步剪枝来移除嵌入的签名信息。这种攻击的目的是通过减少模型的非零参数，使得嵌入的结构信息丢失。然而，文章中提出的嵌入方法确保了签名信息在极端稀疏的情况下仍能保留，即使剪枝比例达到一定程度，签名依然可以从稀疏结构中提取出来。模糊攻击（Ambiguity Attacks）：攻击者试图通过制造伪签名或模糊原有签名的信息来混淆所有权验证。这种攻击可能包括添加噪声、篡改稀疏结构等手段，旨在使得验证机制无法区分真实的所有权签名和伪造的信息。然而，文章中的验证方法通过设计稳健的结构嵌入机制，使得这种模糊攻击难以成功。(“签名”指的是嵌入到神经网络稀疏结构中的一种独特的标识信息。这种签名通过在模型的剪枝过程中，利用网络的稀疏性来实现。具体而言，当模型被剪枝后，一部分神经元和连接被移除，剩余的结构会呈现出一种特定的稀疏模式。作者通过在这种稀疏模式中嵌入一个特定的结构或模式，这个模式就是所谓的“签名”。这种签名是不可见的，但可以通过特定的验证过程来提取和识别。其主要功能是为网络的所有权提供证据，类似于给模型打上了一个“水印”。当有人试图非法复制或篡改模型时，这个嵌入的签名仍然可以被识别出来，从而验证模型的归属。签名的鲁棒性设计使其能够抵抗常见的攻击方式（如微调和进一步的剪枝），即使模型经历了这些操作，签名依然可以从其稀疏结构中被提取出来，证明模型的所有权。)(模型的所有权是指对一个机器学习模型（如神经网络模型）所拥有的法律和知识产权。所有权通常由开发者或公司拥有，表示他们对模型的设计、训练数据、训练方法以及最终生成的模型参数等有控制权和排他性使用权。这意味着只有模型的所有者有权利决定如何使用、修改、发布或授权使用该模型。（可能涉及到知识产权保护、商业机密的保密）)\\ \hline
   % \cite{attack06} & Portail HAL theses(theses.hal.science) & 2022 & 这篇文章有185页。本篇文章讨论了如何通过数字水印技术来保护机器学习模型的知识产权，防止模型被盗用。文章首先提供了当前水印技术的概述，并进一步扩展了这些技术在图像分类任务之外的应用，涵盖了回归、机器翻译和强化学习模型。作者还提出了针对模型托管平台的伪造攻击（即试图通过伪造水印来绕过验证）并介绍了一种基于公平性的水印技术，以增强模型在黑盒环境中的安全性。实验结果表明，这些水印技术不仅可以有效防止模型盗用，还能够在面对各种攻击时保持鲁棒性。
   % 数字水印是一种嵌入信息的技术，用于在数字内容（如图像、音频、视频或机器学习模型）中隐藏特定的信息，以表明所有权或版权。对于机器学习模型来说，数字水印是一种通过特定算法将标识信息嵌入到模型的权重、结构或输出中的技术。这种标识信息通常是不可见或难以察觉的，但可以通过特定的提取过程来验证。（数字水印的目的有：知识产权保护：开发者可以通过在模型中嵌入水印来证明模型的所有权，防止未经授权的复制和使用；盗版检测：如果一个模型被盗用或未经许可发布，水印可以作为证据，证明模型的来源和合法所有者；内容跟踪：水印可以帮助追踪模型的使用情况，尤其是在多个平台或用户之间共享时，确保模型的使用符合许可协议。） \\ \hline
    % \endfoot
    % \hline
    % \endlastfoot
    % \hline
\end{longtable}

\newpage
\onecolumn
\begin{longtable}{|p{3.5cm}|p{2cm}|p{10.5cm}|} 
    \caption{defense}
    \label{tabl:defense} \\ \hline
    Title & Conference & Main Content \\ \hline
    \endfirsthead \hline
    Title & Conference & Main Content \\ \hline 
    \endhead \hline
    SEER\cite{zhu2024seer} & AAAI 2024 & 这篇文章介绍了一种名为SEER的新型后门检测算法，专门用于检测视觉-语言模型中的后门攻击。研究的动机源于现有单模态模型（如图像分类模型）的后门检测方法无法直接应用于多模态模型（如视觉-语言模型）的复杂性和搜索空间的增加。为此，作者提出了一种新方法，通过在视觉和语言模态共享的特征空间中联合搜索图像触发器和恶意目标文本来检测后门。该方法的主要步骤包括：初始化目标文本和图像触发器的特征表示，设计优化算法在图像和文本空间中联合搜索可能的后门触发器和目标文本，并通过分析搜索结果来判断模型是否存在后门。实验结果表明，SEER在各种设置下能够以超过92\%的检测率成功识别视觉-语言模型中的后门，并且无需访问训练数据或了解下游任务。文章还比较了SEER与现有方法的效率，表明SEER在计算效率和检测效果方面均具有显著优势。\\ \hline

    Semantic Shield\cite{ishmam2024semantic} & CVPR 2024 & 这篇文章提出了一种名为“Semantic Shield”的方法，旨在通过精细化知识对齐来防御视觉-语言模型中的后门和投毒攻击。研究的动机在于，随着大规模视觉-语言模型在实际应用中的广泛使用，这些模型变得容易受到攻击，如通过向训练数据中注入恶意数据来操纵模型行为。现有的防御方法通常无法有效应对这些攻击，尤其是在对比学习的多模态模型中。该方法通过引入外部知识来指导模型关注与外部知识对齐的视觉区域，从而避免模型学习到与攻击信号相关的错误关联。具体来说，研究人员使用大型语言模型（如Vicuna）从图像字幕中提取潜在的知识元素（KE），并通过对比学习的方式将这些KE与图像区域对齐。此外，模型的注意力机制也被调整，使其更加关注与KE高度对齐的图像区域，从而降低对受攻击区域的依赖。\\ \hline

    DECREE\cite{feng2023detecting} & CVPR 2023 & 主要关注自监督学习中预训练图像编码器的后门检测问题，鉴于现有方法在处理不同类型的后门攻击时表现不一致，且通常需要标签数据或下游分类器，本文提出了一种无需标签数据或下游分类器的新方法。DECREE 通过利用输入图像特征的扰动性和鲁棒性之间的差异来识别潜在的后门攻击，采用了一种基于自适应优化的策略，有效地将干净样本与后门样本分离。 \\ \hline

    ASSET\cite{pan2023asset} & USENIX 2023 & 这篇文章介绍了一种名为“ASSET”的方法，旨在跨多种深度学习范式中实现稳健的后门数据检测。研究动机是当前的后门检测方法在不同的深度学习设置（如自监督学习和迁移学习）中的表现差异很大，尤其在面对最新的干净标签后门攻击时，现有方法表现不佳。为了解决这一问题，作者提出了ASSET方法，它通过主动诱导后门样本和干净样本之间的模型行为差异来促进它们的分离。具体来说，ASSET采用了一个两步优化过程：首先在一个干净的基础数据集上最小化损失，然后在整个包含后门样本的训练集上最大化同样的损失，从而使后门样本和干净样本在损失值上出现显著差异，进而实现样本的分离和检测。\\ \hline

    MCLDef\cite{yue2023model} & ACM MM 2023 & 这篇文章介绍了一种名为“基于模型对比学习的后门消除”（Model-Contrastive Learning for Backdoor Elimination，简称MCLDef）的新方法。研究的动机是当前的后门攻击对深度神经网络（DNNs）构成了显著威胁，而现有的后门防御方法在降低攻击成功率（ASR）的同时，往往会显著降低模型对干净数据的分类准确性（BA）。为了解决这一问题，作者提出了MCLDef方法，该方法通过模型对比学习来消除DNN中的后门，并最大限度地减少对BA的影响。具体来说，MCLDef通过定义正对和负对的特征对来进行对比学习：正对指的是干净数据样本与其中毒对应数据的特征表示，负对则是中毒数据样本在后门模型和纯化模型中的特征表示。通过对比学习，MCLDef能够缩小或消除中毒数据在特征空间中的聚类，并将中毒数据的特征拉向其干净对应样本的特征，从而有效地消除后门攻击带来的影响。 \\ \hline

    DPoE\cite{liu2023shortcuts} & Arxiv 2023 & 这篇文章提出了一种名为DPoE（Denoised Product-of-Experts）的后门防御框架，旨在应对自然语言处理（NLP）模型中由多种不同触发器引发的后门攻击。研究的动机在于，现有的后门防御方法主要针对显式触发器，而难以有效应对隐式或多种触发器混合的复杂攻击。DPoE方法的核心思想是通过设计一个由两个模型组成的集成框架：一个浅层模型专门捕捉后门攻击中存在的“捷径”，即错误关联；另一个主模型则避免学习这些“捷径”。此外，DPoE引入了去噪设计来减轻由后门攻击者造成的标签翻转问题，从而进一步增强防御效果。 \\ \hline

    PSIM\cite{zhao2024defending} & arxiv 2024 & 这篇文章的主要内容是探讨参数高效微调（PEFT）方法在语言模型中的安全问题，特别是在模型的预训练阶段被植入后门的情况下。这种攻击被称为“权重中毒后门攻击”，发生在模型的预训练阶段，攻击者通过在训练数据中加入特定的触发器，并使用这些数据训练模型，从而在模型的权重中嵌入后门。当微调后的模型遇到这些触发器时，模型会按照攻击者的意图输出错误的结果。本文的主要目的是揭示PEFT方法在面对这种预训练阶段的权重中毒后门攻击时的脆弱性。研究表明，与全参数微调方法相比，PEFT更容易受到这种攻击，攻击成功率接近100\%。为了解决这一问题，作者提出了一种名为“中毒样本识别模块”（PSIM）的防御方法，通过预测置信度来识别和过滤可能被植入后门的样本，从而有效防御这些攻击，同时保持模型的分类准确性。 \\ \hline
    
    CleanCLIP\cite{bansal2023cleanclip} & ICCV 2023 & 这篇文章提出了一种名为CleanCLIP的方法，旨在减轻多模态对比学习模型（如CLIP）中数据投毒攻击的影响。研究动机是因为现有的多模态对比学习模型在训练过程中容易受到后门攻击的威胁，甚至少量的中毒样本就可以显著操控模型行为。CleanCLIP通过在模型预训练后进行微调来减弱这种攻击的影响。具体来说，该方法在微调过程中，通过同时应用多模态对比损失和自监督学习目标，使模型独立地学习每种模态（图像和文本）的表示，减少后门触发器与目标标签之间的错误关联。实验结果表明，CleanCLIP在不影响模型对干净样本性能的前提下，有效降低了多种后门攻击的成功率。此外，文章还讨论了不同的微调方法对清除后门触发器的效果，并发现CleanCLIP能够显著削弱这些攻击的影响。 \\ \hline
\end{longtable}

\bibliographystyle{IEEEtran}
\bibliography{attack_defense}

\end{document}