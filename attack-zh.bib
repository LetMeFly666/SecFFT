% attack
@article{jha2023label,
    title   = {Label poisoning is all you need},
    author  = {Jha, Rishi and Hayase, Jonathan and Oh, Sewoong},
    journal = {Advances in Neural Information Processing Systems},
    volume  = {36},
    pages   = {71029--71052},
    year    = {2023}
}

@inproceedings{yang2023data,
    title        = {Data poisoning attacks against multimodal encoders},
    author       = {Yang, Ziqing and He, Xinlei and Li, Zheng and Backes, Michael and Humbert, Mathias and Berrang, Pascal and Zhang, Yang},
    booktitle    = {International Conference on Machine Learning},
    pages        = {39299--39313},
    year         = {2023},
    organization = {PMLR}
}

@inproceedings{dai2023chameleon,
    title        = {Chameleon: Adapting to peer images for planting durable backdoors in federated learning},
    author       = {Dai, Yanbo and Li, Songze},
    booktitle    = {International Conference on Machine Learning},
    pages        = {6712--6725},
    year         = {2023},
    organization = {PMLR}
}

@article{zhang2024a3fl,
    title   = {A3fl: Adversarially adaptive backdoor attacks to federated learning},
    author  = {Zhang, Hangfan and Jia, Jinyuan and Chen, Jinghui and Lin, Lu and Wu, Dinghao},
    journal = {Advances in Neural Information Processing Systems},
    volume  = {36},
    year    = {2024}
}

@article{xu2024shadowcast,
    title   = {Shadowcast: Stealthy data poisoning attacks against vision-language models},
    author  = {Xu, Yuancheng and Yao, Jiarui and Shu, Manli and Sun, Yanchao and Wu, Zichu and Yu, Ning and Goldstein, Tom and Huang, Furong},
    journal = {arXiv preprint arXiv:2402.06659},
    year    = {2024}
}

@inproceedings{liang2024badclip,
    title     = {Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning},
    author    = {Liang, Siyuan and Zhu, Mingli and Liu, Aishan and Wu, Baoyuan and Cao, Xiaochun and Chang, Ee-Chien},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages     = {24645--24654},
    year      = {2024}
}

@inproceedings{gu2023gradient,
    title     = {A gradient control method for backdoor attacks on parameter-efficient tuning},
    author    = {Gu, Naibin and Fu, Peng and Liu, Xiyu and Liu, Zhengxiao and Lin, Zheng and Wang, Weiping},
    booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages     = {3508--3520},
    year      = {2023}
}

@article{tao2024imgtrojan,
    title   = {ImgTrojan: Jailbreaking Vision-Language Models with ONE Image},
    author  = {Tao, Xijia and Zhong, Shuai and Li, Lei and Liu, Qi and Kong, Lingpeng},
    journal = {arXiv preprint arXiv:2403.02910},
    year    = {2024}
}


@article{attack01,
    title   = {Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models},
    author  = {Ye, Rui and Chai, Jingyi and Liu, Xiangrui and Yang, Yaodong and Wang, Yanfeng and Chen, Siheng},
    journal = {arXiv preprint arXiv:2406.10630},
    year    = {2024}
}

@article{attack02,
    title   = {Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs},
    author  = {Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and others},
    journal = {arXiv preprint arXiv:2407.15549},
    year    = {2024}
}

@article{attack03,
    title   = {Tamper-Resistant Safeguards for Open-Weight LLMs},
    author  = {Tamirisa, Rishub and Bharathi, Bhrugu and Phan, Long and Zhou, Andy and Gatti, Alice and Suresh, Tarun and Lin, Maxwell and Wang, Justin and Wang, Rowan and Arel, Ron and others},
    journal = {arXiv preprint arXiv:2408.00761},
    year    = {2024}
}

% 这是一篇综述
@article{attack04,
    title   = {Against The Achilles' Heel: A Survey on Red Teaming for Generative Models},
    author  = {Lin, Lizhi and Mu, Honglin and Zhai, Zenan and Wang, Minghan and Wang, Yuxia and Wang, Renxi and Gao, Junjie and Zhang, Yixuan and Che, Wanxiang and Baldwin, Timothy and others},
    journal = {arXiv preprint arXiv:2404.00629},
    year    = {2024}
}

@article{attack05,
    title   = {You are caught stealing my winning lottery ticket! making a lottery ticket claim its ownership},
    author  = {Chen, Xuxi and Chen, Tianlong and Zhang, Zhenyu and Wang, Zhangyang},
    journal = {Advances in neural information processing systems},
    volume  = {34},
    pages   = {1780--1791},
    year    = {2021}
}

@phdthesis{attack06,
    title  = {Watermarking machine learning models},
    author = {Lounici, Sofiane},
    year   = {2022},
    school = {Sorbonne Universit{\'e}}
}

%defense
@inproceedings{zhu2024seer,
    title     = {SEER: Backdoor Detection for Vision-Language Models through Searching Target Text and Image Trigger Jointly},
    author    = {Zhu, Liuwan and Ning, Rui and Li, Jiang and Xin, Chunsheng and Wu, Hongyi},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume    = {38},
    number    = {7},
    pages     = {7766--7774},
    year      = {2024}
}

@inproceedings{ishmam2024semantic,
    title     = {Semantic Shield: Defending Vision-Language Models Against Backdooring and Poisoning via Fine-grained Knowledge Alignment},
    author    = {Ishmam, Alvi Md and Thomas, Christopher},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages     = {24820--24830},
    year      = {2024}
}

@inproceedings{feng2023detecting,
    title     = {Detecting backdoors in pre-trained encoders},
    author    = {Feng, Shiwei and Tao, Guanhong and Cheng, Siyuan and Shen, Guangyu and Xu, Xiangzhe and Liu, Yingqi and Zhang, Kaiyuan and Ma, Shiqing and Zhang, Xiangyu},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages     = {16352--16362},
    year      = {2023}
}

@inproceedings{pan2023asset,
    title     = {$\{$ASSET$\}$: Robust backdoor data detection across a multiplicity of deep learning paradigms},
    author    = {Pan, Minzhou and Zeng, Yi and Lyu, Lingjuan and Lin, Xue and Jia, Ruoxi},
    booktitle = {32nd USENIX Security Symposium (USENIX Security 23)},
    pages     = {2725--2742},
    year      = {2023}
}

@inproceedings{yue2023model,
    title     = {Model-Contrastive Learning for Backdoor Elimination},
    author    = {Yue, Zhihao and Xia, Jun and Ling, Zhiwei and Hu, Ming and Wang, Ting and Wei, Xian and Chen, Mingsong},
    booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
    pages     = {8869--8880},
    year      = {2023}
}

@article{liu2023shortcuts,
    title   = {From shortcuts to triggers: Backdoor defense with denoised poe},
    author  = {Liu, Qin and Wang, Fei and Xiao, Chaowei and Chen, Muhao},
    journal = {arXiv preprint arXiv:2305.14910},
    year    = {2023}
}

@article{zhao2024defending,
    title   = {Defending against weight-poisoning backdoor attacks for parameter-efficient fine-tuning},
    author  = {Zhao, Shuai and Gan, Leilei and Tuan, Luu Anh and Fu, Jie and Lyu, Lingjuan and Jia, Meihuizi and Wen, Jinming},
    journal = {arXiv preprint arXiv:2402.12168},
    year    = {2024}
}

@inproceedings{bansal2023cleanclip,
    title     = {Cleanclip: Mitigating data poisoning attacks in multimodal contrastive learning},
    author    = {Bansal, Hritik and Singhi, Nishad and Yang, Yu and Yin, Fan and Grover, Aditya and Chang, Kai-Wei},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages     = {112--123},
    year      = {2023}
}

@inproceedings{defense01,
    title     = {SEER: Backdoor Detection for Vision-Language Models through Searching Target Text and Image Trigger Jointly},
    author    = {Zhu, Liuwan and Ning, Rui and Li, Jiang and Xin, Chunsheng and Wu, Hongyi},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume    = {38},
    number    = {7},
    pages     = {7766--7774},
    year      = {2024}
}

@article{defense02,
    title   = {Revisiting backdoor attacks against large vision-language models},
    author  = {Liang, Siyuan and Liang, Jiawei and Pang, Tianyu and Du, Chao and Liu, Aishan and Chang, Ee-Chien and Cao, Xiaochun},
    journal = {arXiv preprint arXiv:2406.18844},
    year    = {2024}
}

@article{defense03,
    title   = {Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models},
    author  = {Ni, Zhenyang and Ye, Rui and Wei, Yuxi and Xiang, Zhen and Wang, Yanfeng and Chen, Siheng},
    journal = {arXiv preprint arXiv:2404.12916},
    year    = {2024}
}

@article{defense04,
    title   = {A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends},
    author  = {Liu, Daizong and Yang, Mingyu and Qu, Xiaoye and Zhou, Pan and Hu, Wei and Cheng, Yu},
    journal = {arXiv preprint arXiv:2407.07403},
    year    = {2024}
}

@inproceedings{defense05,
    title     = {Semantic Shield: Defending Vision-Language Models Against Backdooring and Poisoning via Fine-grained Knowledge Alignment},
    author    = {Ishmam, Alvi Md and Thomas, Christopher},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages     = {24820--24830},
    year      = {2024}
}

@article{defense06,
    title   = {Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models},
    author  = {Liang, Jiawei and Liang, Siyuan and Luo, Man and Liu, Aishan and Han, Dongchen and Chang, Ee-Chien and Cao, Xiaochun},
    journal = {arXiv preprint arXiv:2402.13851},
    year    = {2024}
}

@article{defense07,
    title   = {Robust contrastive language-image pretraining against data poisoning and backdoor attacks},
    author  = {Yang, Wenhan and Gao, Jingdong and Mirzasoleiman, Baharan},
    journal = {Advances in Neural Information Processing Systems},
    volume  = {36},
    year    = {2024}
}

@inproceedings{defense08,
    title     = {Causality Based Front-door Defense Against Backdoor Attack on Language Models},
    author    = {Liu, Yiran and Xu, Xiaoang and Hou, Zhiyi and Yu, Yang},
    booktitle = {Forty-first International Conference on Machine Learning}
}

@article{defense09,
    title   = {Shadowcast: Stealthy data poisoning attacks against vision-language models},
    author  = {Xu, Yuancheng and Yao, Jiarui and Shu, Manli and Sun, Yanchao and Wu, Zichu and Yu, Ning and Goldstein, Tom and Huang, Furong},
    journal = {arXiv preprint arXiv:2402.06659},
    year    = {2024}
}

@article{defense10,
    title   = {Compromising Embodied Agents with Contextual Backdoor Attacks},
    author  = {Liu, Aishan and Zhou, Yuguang and Liu, Xianglong and Zhang, Tianyuan and Liang, Siyuan and Wang, Jiakai and Pu, Yanjun and Li, Tianlin and Zhang, Junqi and Zhou, Wenbo and others},
    journal = {arXiv preprint arXiv:2408.02882},
    year    = {2024}
}

% 时间上不连续的攻击
@article{abad2024time,
    title={Time-Distributed Backdoor Attacks on Federated Spiking Learning},
    author={Abad, Gorka and Picek, Stjepan and Urbieta, Aitor},
    journal={arXiv preprint arXiv:2402.02886},
    year={2024}
}

% 时间上不连续的攻击2
@article{lyu2024coba,
    title={CoBA: Collusive Backdoor Attacks with Optimized Trigger to Federated Learning},
    author={Lyu, Xiaoting and Han, Yufei and Wang, Wei and Liu, Jingkai and Wang, Bin and Chen, Kai and Li, Yidong and Liu, Jiqiang and Zhang, Xiangliang},
    journal={IEEE Transactions on Dependable and Secure Computing},
    year={2024},
    publisher={IEEE}
}

% 非直奔目标的攻击  % 382次引用
@article{nguyen2020input,
    title={Input-aware dynamic backdoor attack},
    author={Nguyen, Tuan Anh and Tran, Anh},
    journal={Advances in Neural Information Processing Systems},
    volume={33},
    pages={3454--3464},
    year={2020}
}

% 非直奔目标的攻击
@article{yang2023efficient,
    title={Efficient and persistent backdoor attack by boundary trigger set constructing against federated learning},
    author={Yang, Deshan and Luo, Senlin and Zhou, Jinjie and Pan, Limin and Yang, Xiaonan and Xing, Jiyuan},
    journal={Information Sciences},
    volume={651},
    pages={119743},
    year={2023},
    publisher={Elsevier}
}

% 非直奔目标的攻击  % 将攻击转到频域上，从而增强隐蔽性。诶，这不是正好能被咱的检测到？
@article{qiao2024stealthy,
    title={Stealthy Backdoor Attack against Federated Learning through Frequency Domain by Backdoor Neuron Constraint and Model Camouflage},
    author={Qiao, Yanqi and Liu, Dazhuang and Wang, Rui and Liang, Kaitai},
    journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
    year={2024},
    publisher={IEEE}
}