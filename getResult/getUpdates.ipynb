{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from genReLiTu import plot_detection_heatmaps_3x4, generate_data_1dimension\n",
    "import sklearn.metrics.pairwise as smp\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import hdbscan\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fmnist\"\n",
    "\n",
    "files = {\n",
    "    \"MR\": {\n",
    "        \"flame\": \"10020337\",\n",
    "        \"fltrust\": \"10022140\",\n",
    "        \"foolsgold\": \"10271625\",\n",
    "        \"secfft\": \"10292241\",\n",
    "    },\n",
    "    \"EDGE_CASE\": {\n",
    "        \"flame\": \"10020911\",\n",
    "        \"fltrust\": \"10030309\",\n",
    "        \"foolsgold\": \"10280055\",\n",
    "        \"secfft\": \"10300448\",\n",
    "    },\n",
    "    \"NEUROTOXIN\": {\n",
    "        \"flame\": \"10021255\",\n",
    "        \"fltrust\": \"10030648\",\n",
    "        \"foolsgold\": \"10280559\",\n",
    "        \"secfft\": \"10300840\",\n",
    "    },\n",
    "}\n",
    "\n",
    "attack_defense_data = {\n",
    "    \"MR\": {\n",
    "        \"FLAME\": \"FL_Backdoor_CV/2024-10-02_03-37-50/\",\n",
    "        \"FLTRUST\": \"FL_Backdoor_CV/2024-10-02_21-40-11/\",\n",
    "        \"FOOLSGOLD\": \"FL_Backdoor_CV/2024-10-27_16-25-41/\",\n",
    "        \"SECFFT\": \"FL_Backdoor_CV/2024-10-29_22-41-06/\",\n",
    "    },\n",
    "    \"EDGE_CASE\": {\n",
    "        \"FLAME\": \"FL_Backdoor_CV/2024-10-02_09-11-28/\",\n",
    "        \"FLTRUST\": \"FL_Backdoor_CV/2024-10-03_03-09-36/\",\n",
    "        \"FOOLSGOLD\": \"FL_Backdoor_CV/2024-10-28_00-55-46/\",\n",
    "        \"SECFFT\": \"FL_Backdoor_CV/2024-10-30_04-48-52/\",\n",
    "    },\n",
    "    \"NEUROTOXIN\": {\n",
    "        \"FLAME\": \"FL_Backdoor_CV/2024-10-02_12-55-32/\",\n",
    "        \"FLTRUST\": \"FL_Backdoor_CV/2024-10-03_06-48-00/\",\n",
    "        \"FOOLSGOLD\": \"FL_Backdoor_CV/2024-10-28_05-59-22/\",\n",
    "        \"SECFFT\": \"FL_Backdoor_CV/2024-10-30_08-40-55/\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for attack, attack2data in attack_defense_data.items():\n",
    "#     true_labels = []\n",
    "#     labels_get_by_Euclid = []\n",
    "#     labels_get_by_manhattan = []\n",
    "#     labels_get_by_cosine = []\n",
    "#     labels_get_by_chi_square = []\n",
    "#     for denfense, data_folder in attack2data.items():\n",
    "#         if denfense != \"AVG\":\n",
    "#             continue\n",
    "#         participant_file_name = f\"{data_folder}participants/participants.csv\"\n",
    "#         participants = np.genfromtxt(\n",
    "#             participant_file_name, delimiter=\",\", dtype=None, encoding=\"utf-8\"\n",
    "#         )\n",
    "#         participants = participants[1:].T\n",
    "#         for i in range(len(participants) - 1):\n",
    "#             file_name = f\"{data_folder}model_updates/{dataset}_{attack}_{i}.pkl\"\n",
    "#             if not os.path.exists(file_name):\n",
    "#                 print(f\"File {file_name} not found\")\n",
    "#                 continue\n",
    "#             with open(file_name, \"rb\") as file:\n",
    "#                 model_updates = pickle.load(file)\n",
    "\n",
    "\n",
    "# roundNum = 10\n",
    "# dirPath = '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-12_23-26-43-avg_NEUROTOXIN-fmnist/'\n",
    "# pklName = os.path.join(dirPath, f'model_updates/fmnist_NEUROTOXIN_{roundNum}.pkl')\n",
    "# participantFilePath = os.path.join(dirPath, 'participants/participants.csv')\n",
    "# # participantFileDF = pd.read_csv(participantFilePath)\n",
    "# # # 提取参与者数组（第一列）和轮次（从第二列开始的部分）\n",
    "# # clients = participantFileDF.iloc[1:, 0].tolist()  # 客户端列表\n",
    "# # roundsArray = participantFileDF.iloc[1:, 1:].to_numpy()  # 轮次数据（50行x30列）\n",
    "# # print(roundsArray)\n",
    "# participants = np.genfromtxt(\n",
    "#     participantFilePath, delimiter=\",\", dtype=None, encoding=\"utf-8\"\n",
    "# )\n",
    "# participants = participants[1:].T\n",
    "# participants_thisRound = participants[roundNum]\n",
    "# print(participants_thisRound)\n",
    "\n",
    "# with open(pklName, 'rb') as f:\n",
    "#     update: Dict[str, torch.Tensor] = pickle.load(f)\n",
    "# # print(update)\n",
    "# # weight0 = update['base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight']\n",
    "# # weight0.shape  # torch.Size([50, 12288])\n",
    "# userUpdates = [torch.empty(0) for _ in range(50)]\n",
    "# for layerKey, values in update.items():\n",
    "#     # print(layerKey)\n",
    "#     # print(values)\n",
    "#     # print(values.shape)  # torch.Size([50, 12288])\n",
    "#     for i in range(values.shape[0]):\n",
    "#         trueUser = participants_thisRound[i]\n",
    "#         userUpdates[trueUser] = torch.cat((userUpdates[trueUser], values[i].cpu()), 0)\n",
    "# print(userUpdates[0].shape)\n",
    "\n",
    "def loadPkl(roundNum: int, dirPath: str) -> Dict[str, torch.Tensor]:  # 把pkl变成{'key1': [[user3], [user5]]}\n",
    "    # dirPath = '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-12_23-26-43-avg-fmnist_NEUROTOXIN/'\n",
    "    pklPrefix = os.path.basename(os.path.normpath(dirPath)).split('-')[-1]\n",
    "    pklName = os.path.join(dirPath, f'model_updates/{pklPrefix}_{roundNum}.pkl')\n",
    "    with open(pklName, 'rb') as f:\n",
    "        update: Dict[str, torch.Tensor] = pickle.load(f)\n",
    "    return update\n",
    "\n",
    "def getParticipants(roundNum: int, dirPath: str) -> np.ndarray:  # 获取参与者列表\n",
    "    participantFilePath = os.path.join(dirPath, 'participants/participants.csv')\n",
    "\n",
    "    # 读取参与者数组\n",
    "    participants = np.genfromtxt(\n",
    "        participantFilePath, delimiter=\",\", dtype=None, encoding=\"utf-8\"\n",
    "    )\n",
    "    participants = participants[1:].T\n",
    "    participants_thisRound = participants[roundNum]  # 获取当前轮次的参与者\n",
    "    return participants_thisRound\n",
    "\n",
    "def get_all_user_updates(roundNum: int, dirPath: str) -> torch.Tensor:  # 把pkl变成[[user1展平(拼接)后的结果], [user2], ...]\n",
    "    # 设置轮次编号和文件路径\n",
    "    # roundNum = 10\n",
    "    # dirPath = '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-12_23-26-43-avg-fmnist_NEUROTOXIN/'\n",
    "    participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "    # print(participants_thisRound)\n",
    "\n",
    "    # 加载 .pkl 文件\n",
    "    update = loadPkl(roundNum, dirPath)\n",
    "\n",
    "    # 初始化一个列表来存储每个客户端的更新，使用 GPU 张量\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    userUpdates = [torch.empty((0,), device=device) for _ in range(50)]  # 假设50个客户端\n",
    "\n",
    "    # 遍历每个层的更新\n",
    "    for layerKey, values in update.items():\n",
    "        # 将每个值移到 GPU 上\n",
    "        values = values.to(device)\n",
    "        for i in range(values.shape[0]):  # 遍历每个客户端的梯度\n",
    "            trueUser = participants_thisRound[i]  # 获取当前轮次的客户端ID\n",
    "            # 如果为空张量\n",
    "            if userUpdates[trueUser].numel() == 0:\n",
    "                userUpdates[trueUser] = values[i]  # 直接赋值\n",
    "            else:\n",
    "                userUpdates[trueUser] = torch.cat((userUpdates[trueUser], values[i]), 0)  # 在GPU上拼接\n",
    "\n",
    "    userUpdates = torch.stack(userUpdates)  # 转换为张量\n",
    "    # # 打印第一个用户的更新形状来检查结果\n",
    "    # print(userUpdates[0].shape)\n",
    "    return userUpdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算一个二维 tensor 中每两个向量之间的余弦相似度矩阵。\n",
    "\n",
    "    参数:\n",
    "    - tensor (torch.Tensor): 形状为 (N, D) 的二维张量，N 是客户端数量，D 是特征维度。\n",
    "\n",
    "    返回:\n",
    "    - similarity_matrix (torch.Tensor): 形状为 (N, N) 的张量，每个元素表示两个向量之间的余弦相似度。\n",
    "    \"\"\"\n",
    "    # 归一化每个向量\n",
    "    normalized_tensor = F.normalize(tensor, p=2, dim=1)\n",
    "    \n",
    "    # 计算余弦相似度矩阵\n",
    "    similarity_matrix = torch.mm(normalized_tensor, normalized_tensor.T)\n",
    "    \n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userUpdates_avg_NEUR = get_all_user_updates(10, '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-12_23-26-43-avg-fmnist_NEUROTOXIN/')\n",
    "# print(userUpdates_avg_NEUR.shape)\n",
    "# print(userUpdates_avg_NEUR)\n",
    "# userUpdates_avg_MR = get_all_user_updates(10, '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-13_12-02-44-avg-fmnist_MR/')\n",
    "\n",
    "# similarity_matrix_avg_NEUR = cosine_similarity_matrix(userUpdates_avg_NEUR)\n",
    "# print(similarity_matrix_avg_NEUR.shape)\n",
    "# similarity_matrix_avg_MR = cosine_similarity_matrix(userUpdates_avg_MR)\n",
    "# # print(similarity_matrix_avg_MR.shape)\n",
    "# plot_detection_heatmaps(similarity_matrix_avg_NEUR, similarity_matrix_avg_MR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foolsgold\n",
    "# ChangeFrom https://github.com/LetMeFly666/SecFFT/blob/706bb287c3b00f6143e2190edc74714ed88f3532/getResult/FL_Backdoor_CV/roles/aggregation_rules.py#L203C1-L241C25\n",
    "# foolsgold自身并没有确认哪些是恶意客户端，而是根据权重聚合\n",
    "# def foolsgold_identify_malicious_clients(wv: np.ndarray, threshold: float = 0.25) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     根据权重向量识别潜在的恶意客户端。\n",
    "\n",
    "#     参数:\n",
    "#     - wv (np.ndarray): 客户端的权重向量。\n",
    "#     - threshold (float): 判断恶意客户端的阈值，权重低于此值的客户端将被视为恶意客户端。\n",
    "    \n",
    "#     返回:\n",
    "#     - malicious_clients (np.ndarray): 被识别为恶意的客户端索引。\n",
    "#     \"\"\"\n",
    "#     return np.where(wv < threshold)[0]\n",
    "def foolsgold_identify_malicious_clients(wv: np.ndarray) -> np.ndarray:\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(wv.reshape(-1, 1))\n",
    "    clusters = kmeans.labels_\n",
    "    unique, counts = np.unique(clusters, return_counts=True)\n",
    "    max_cluster = unique[np.argmax(counts)]\n",
    "    user_labels = np.zeros(len(clusters), dtype=int)\n",
    "    user_labels[clusters == max_cluster] = 1\n",
    "    return np.where(user_labels == 0)[0]\n",
    "\n",
    "# 返回 聚合后的全局模型、恶意客户端索引、余弦相似度矩阵\n",
    "# def foolsgold(model_updates: Dict[str, torch.Tensor]) -> Tuple[Dict[str, torch.Tensor], np.ndarray, np.ndarray]:\n",
    "def foolsgold_oneRound(roundNum: int, dirPath: str) -> Tuple[Dict[str, torch.Tensor], np.ndarray, torch.Tensor]:\n",
    "    model_updates = loadPkl(roundNum, dirPath)\n",
    "    participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "    \n",
    "    keys = list(model_updates.keys())\n",
    "    last_layer_updates = model_updates[keys[-2]]\n",
    "    K = len(last_layer_updates)\n",
    "    cs = smp.cosine_similarity(last_layer_updates.cpu().numpy()) - np.eye(K)  # 减去对角线为1的单位矩阵，使得自身与自身的相似度为0\n",
    "    maxcs = np.max(cs, axis=1)\n",
    "    # === pardoning(赦免) ===\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if maxcs[i] < maxcs[j]:\n",
    "                cs[i][j] = cs[i][j] * maxcs[i] / maxcs[j]\n",
    "\n",
    "    alpha = np.max(cs, axis=1)\n",
    "    wv = 1 - alpha\n",
    "    wv[wv > 1] = 1\n",
    "    wv[wv < 0] = 0\n",
    "\n",
    "    # === Rescale so that max value is wv ===\n",
    "    wv = wv / np.max(wv)\n",
    "    wv[(wv == 1)] = .99\n",
    "\n",
    "    # === Logit function ===\n",
    "    wv = (np.log(wv / (1 - wv)) + 0.5)\n",
    "    wv[(np.isinf(wv) + wv > 1)] = 1\n",
    "    wv[(wv < 0)] = 0\n",
    "\n",
    "    malicious_clients = foolsgold_identify_malicious_clients(wv)\n",
    "    malicious_clients = participants_thisRound[malicious_clients]\n",
    "    print(f\"识别出的恶意客户端索引: {malicious_clients}\")\n",
    "\n",
    "    # === calculate global update ===\n",
    "    global_update = defaultdict()\n",
    "    for name in keys:\n",
    "        tmp = None\n",
    "        for i, j in enumerate(range(len(wv))):\n",
    "            if i == 0:\n",
    "                tmp = model_updates[name][j] * wv[j]\n",
    "            else:\n",
    "                tmp += model_updates[name][j] * wv[j]\n",
    "        global_update[name] = 1 / len(wv) * tmp\n",
    "    print(wv)\n",
    "    if True:  # 使用余弦相似度作为热力图依据\n",
    "        cs = smp.cosine_similarity(last_layer_updates.cpu().numpy())  # 这里就不再减去自身了\n",
    "        cs_rearranged = np.zeros((len(participants_thisRound), len(participants_thisRound)))\n",
    "        for i, user_i in enumerate(participants_thisRound):\n",
    "            for j, user_j in enumerate(participants_thisRound):\n",
    "                cs_rearranged[user_i][user_j] = cs[i][j]\n",
    "        cs_tensor = torch.from_numpy(cs_rearranged)\n",
    "    else:  # 使用聚合参数wv作为热力图依据。这样直接很多1，太明显了\n",
    "        cs_tensor = torch.zeros((len(participants_thisRound)))\n",
    "        for i, user_i in enumerate(participants_thisRound):\n",
    "            cs_tensor[user_i] = wv[i]  # tensor([0.2348, 0.2348, 0.8740, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
    "\n",
    "    return global_update, malicious_clients, cs_tensor\n",
    "\n",
    "# 组合多轮，返回恶意客户端索引、余弦相似度矩阵\n",
    "def foolsgold(roundsNum: List[int], dirPath: str) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "    clientPerRound = 50  # 这里就先写死了\n",
    "    maliciousPerRound = 20\n",
    "    gradients = [0] * len(roundsNum) * clientPerRound\n",
    "    maliciouses = []\n",
    "    for th, roundNum in enumerate(roundsNum):\n",
    "        participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "        model_updates = loadPkl(roundNum, dirPath)\n",
    "        keys = list(model_updates.keys())\n",
    "        last_layer_updates = model_updates[keys[-2]]\n",
    "        K = len(last_layer_updates)\n",
    "        for i in range(K):\n",
    "            thisParticipant = participants_thisRound[i]\n",
    "            if thisParticipant < maliciousPerRound:\n",
    "                thisIndex = th * maliciousPerRound + thisParticipant\n",
    "            else:\n",
    "                thisIndex = maliciousPerRound * len(roundsNum) + (clientPerRound - maliciousPerRound) * th + thisParticipant - maliciousPerRound\n",
    "            gradients[thisIndex] = last_layer_updates[i].cpu().numpy()\n",
    "        _, foolsgoldMaliciousIndex, _ = foolsgold_oneRound(roundNum, dirPath)\n",
    "        for thisMaliciousIndex in foolsgoldMaliciousIndex:\n",
    "            if thisMaliciousIndex < maliciousPerRound:\n",
    "                thisIndex = th * maliciousPerRound + thisMaliciousIndex\n",
    "            else:\n",
    "                thisIndex = maliciousPerRound * len(roundsNum) + (clientPerRound - maliciousPerRound) * th + thisMaliciousIndex - maliciousPerRound\n",
    "            maliciouses.append(thisIndex)\n",
    "    shuffleArray = np.arange(50)\n",
    "    np.random.shuffle(shuffleArray[0:len(roundsNum) * maliciousPerRound])\n",
    "    np.random.shuffle(shuffleArray[len(roundsNum) * maliciousPerRound:50])\n",
    "    malicious_shuffled = [shuffleArray[i] for i in maliciouses]\n",
    "    gradients_shuffled = [gradients[shuffleArray[i]] for i in range(50)]\n",
    "    cs = smp.cosine_similarity(gradients_shuffled)\n",
    "    cs_tensor = torch.from_numpy(cs)\n",
    "    return malicious_shuffled, cs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ByGPT - 还是不太行啊看来\n",
    "# import os\n",
    "# import pickle\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "# def fltrust_original(model_updates: Dict[str, torch.Tensor], param_updates: List[torch.Tensor], clean_param_update: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "#     \"\"\"\n",
    "#     使用 FLTrust 方法进行聚合计算。\n",
    "    \n",
    "#     参数:\n",
    "#     - model_updates: 客户端模型更新的字典，键为参数名，值为模型更新的 Tensor。\n",
    "#     - param_updates: 客户端的参数更新列表，包含每个客户端的更新 Tensor。\n",
    "#     - clean_param_update: 干净模型的参数更新，用于计算客户端更新的权重。\n",
    "\n",
    "#     返回:\n",
    "#     - global_update: 聚合后的全局模型更新。\n",
    "#     \"\"\"\n",
    "#     cos = torch.nn.CosineSimilarity(dim=0)\n",
    "#     g0_norm = torch.norm(clean_param_update)\n",
    "#     weights = []\n",
    "    \n",
    "#     # 计算每个客户端更新与 clean_param_update 的余弦相似度\n",
    "#     for param_update in param_updates:\n",
    "#         weights.append(F.relu(cos(param_update.view(-1, 1), clean_param_update.view(-1, 1))))\n",
    "    \n",
    "#     weights = torch.tensor(weights).to('cuda:0').view(1, -1)\n",
    "#     weights = weights / weights.sum()\n",
    "#     weights = torch.where(weights[0].isnan(), torch.zeros_like(weights), weights)\n",
    "#     nonzero_weights = torch.count_nonzero(weights.flatten())\n",
    "#     nonzero_indices = torch.nonzero(weights.flatten()).flatten()\n",
    "\n",
    "#     print(f'g0_norm: {g0_norm}, '\n",
    "#           f'weights_sum: {weights.sum()}, '\n",
    "#           f'*** {nonzero_weights} *** model updates are considered to be aggregated !')\n",
    "\n",
    "#     normalize_weights = []\n",
    "#     for param_update in param_updates:\n",
    "#         normalize_weights.append(g0_norm / torch.norm(param_update))\n",
    "\n",
    "#     global_update = dict()\n",
    "#     for name, params in model_updates.items():\n",
    "#         if 'num_batches_tracked' in name or 'running_mean' in name or 'running_var' in name:\n",
    "#             global_update[name] = 1 / nonzero_weights * params[nonzero_indices].sum(dim=0, keepdim=True)\n",
    "#         else:\n",
    "#             global_update[name] = torch.matmul(\n",
    "#                 weights,\n",
    "#                 params * torch.tensor(normalize_weights).to('cuda:0').view(-1, 1))\n",
    "#     return global_update\n",
    "\n",
    "\n",
    "# def fltrust(roundsNum: List[int], dirPath: str, modelPath: str) -> Tuple[List[int], np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     根据指定的轮次和目录路径，使用 FLTrust 聚合和识别恶意客户端。\n",
    "\n",
    "#     参数:\n",
    "#     - roundsNum: 需要处理的轮次列表。\n",
    "#     - dirPath: 数据文件所在的目录路径。\n",
    "#     - modelPath: 模型路径（目前没有用到）。\n",
    "\n",
    "#     返回:\n",
    "#     - 恶意客户端的编号列表。\n",
    "#     - 50 个客户端的评分（1x50 数组或 50x50 数组）。\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 加载pkl文件数据\n",
    "#     def loadPkl(roundNum: int, subfolder: str, dirPath: str) -> Dict[str, torch.Tensor]:\n",
    "#         pklPrefix = os.path.basename(os.path.normpath(dirPath)).split('-')[-1]\n",
    "#         pklName = os.path.join(dirPath, f'{subfolder}/{pklPrefix}_{roundNum}.pkl')\n",
    "#         with open(pklName, 'rb') as f:\n",
    "#             update: Dict[str, torch.Tensor] = pickle.load(f)\n",
    "#         return update\n",
    "    \n",
    "#     # 合并指定轮次的数据\n",
    "#     all_model_updates = {}\n",
    "#     all_param_updates = []\n",
    "#     for roundNum in roundsNum:\n",
    "#         participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "#         model_updates = loadPkl(roundNum, 'model_updates', dirPath)  # 修正为正确的子文件夹\n",
    "#         clean_param_update = loadPkl(roundNum, 'clean_param_updates', dirPath)[participants_thisRound[0]]  # 修正为正确的子文件夹\n",
    "\n",
    "#         # 拼接恶意客户端（0-2号）和良性客户端（3-9号）\n",
    "#         for key, value in model_updates.items():\n",
    "#             if key not in all_model_updates:\n",
    "#                 all_model_updates[key] = torch.zeros((50, value.shape[1])).to('cuda:0')\n",
    "#             all_model_updates[key][:15] = torch.cat([value[i].unsqueeze(0) for i in range(3)], dim=0).to('cuda:0')  # 恶意客户端\n",
    "#             all_model_updates[key][15:] = torch.cat([value[i].unsqueeze(0) for i in range(3, 10)], dim=0).to('cuda:0')  # 良性客户端\n",
    "\n",
    "#         # 提取参数更新，拼接恶意和良性客户端\n",
    "#         for i in range(3):\n",
    "#             all_param_updates.append(parameters_to_vector([model_updates[key][i] for key in model_updates.keys()]).to('cuda:0'))\n",
    "#         for i in range(3, 10):\n",
    "#             all_param_updates.append(parameters_to_vector([model_updates[key][i] for key in model_updates.keys()]).to('cuda:0'))\n",
    "    \n",
    "#     # 使用 FLTrust 进行聚合\n",
    "#     global_update = fltrust_original(all_model_updates, all_param_updates, clean_param_update)\n",
    "    \n",
    "#     # 使用 KMeans 聚类来识别恶意客户端\n",
    "#     scores = torch.stack(all_param_updates).cpu().numpy()  # 使用参数更新作为聚类的输入\n",
    "#     kmeans = KMeans(n_clusters=2, random_state=0).fit(scores)\n",
    "#     cluster_labels = kmeans.labels_\n",
    "    \n",
    "#     # 识别恶意客户端\n",
    "#     malicious_clients = [i for i in range(50) if cluster_labels[i] == cluster_labels[:15].max()]\n",
    "\n",
    "#     # 计算 50x50 的余弦相似度矩阵\n",
    "#     cosine_similarity_matrix = np.zeros((50, 50))\n",
    "#     for i in range(50):\n",
    "#         for j in range(50):\n",
    "#             cosine_similarity_matrix[i, j] = F.cosine_similarity(all_param_updates[i].view(-1, 1), all_param_updates[j].view(-1, 1)).item()\n",
    "\n",
    "#     return malicious_clients, cosine_similarity_matrix\n",
    "\n",
    "\n",
    "# # 辅助函数：获取参数更新的向量\n",
    "# def parameters_to_vector(params):\n",
    "#     return torch.cat([p.view(-1) for p in params])\n",
    "\n",
    "\n",
    "# # 辅助函数：获取参与者列表\n",
    "# def getParticipants(roundNum: int, dirPath: str) -> np.ndarray:\n",
    "#     # 实现略\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fltrust\n",
    "# https://github.com/LetMeFly666/SecFFT/blob/706bb287c3b00f6143e2190edc74714ed88f3532/getResult/FL_Backdoor_CV/roles/aggregation_rules.py#L301-L340\n",
    "\n",
    "def fltrust_original(model_updates, param_updates, clean_param_update):\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    g0_norm = torch.norm(clean_param_update)\n",
    "    weights = []\n",
    "    for param_update in param_updates:\n",
    "        weights.append(F.relu(cos(param_update.view(-1, 1), clean_param_update.view(-1, 1))))\n",
    "    weights = torch.tensor(weights).to('cuda:0').view(1, -1)\n",
    "    weights = weights / weights.sum()\n",
    "    weights = torch.where(weights[0].isnan(), torch.zeros_like(weights), weights)\n",
    "    nonzero_weights = torch.count_nonzero(weights.flatten())\n",
    "    nonzero_indices = torch.nonzero(weights.flatten()).flatten()\n",
    "\n",
    "    print(f'g0_norm: {g0_norm}, '\n",
    "          f'weights_sum: {weights.sum()}, '\n",
    "          f'*** {nonzero_weights} *** model updates are considered to be aggregated !')\n",
    "\n",
    "    normalize_weights = []\n",
    "    for param_update in param_updates:\n",
    "        normalize_weights.append(g0_norm / torch.norm(param_update))\n",
    "\n",
    "    global_update = dict()\n",
    "    for name, params in model_updates.items():\n",
    "        if 'num_batches_tracked' in name or 'running_mean' in name or 'running_var' in name:\n",
    "            global_update[name] = 1 / nonzero_weights * params[nonzero_indices].sum(dim=0, keepdim=True)\n",
    "        else:\n",
    "            global_update[name] = torch.matmul(\n",
    "                weights,\n",
    "                params * torch.tensor(normalize_weights).to('cuda:0').view(-1, 1))\n",
    "    return global_update\n",
    "\n",
    "def fltrust_half_maybe(roundsNum: List[int], dirPath: str, modelPath: str):\n",
    "    clientPerRound = 50  # 这里就先写死了\n",
    "    maliciousPerRound = 20\n",
    "    # gradients = [0] * len(roundsNum) * clientPerRound\n",
    "    # maliciouses = []\n",
    "    gradientsList = []  # 里面存放每一轮的梯度，最后再聚合\n",
    "    for th, roundNum in enumerate(roundsNum):\n",
    "        participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "        # get model updates\n",
    "        pklName = f'../NormalRun/FL_Backdoor_CV/saved_models/Revision_1/fltrust_NEUROTOXIN_09141511-fmnist/fltrust_{roundNum}.pth'\n",
    "        # print(pklName)\n",
    "        with open(pklName, 'rb') as f:\n",
    "            model_updates: Dict[str, torch.Tensor] = torch.load(f)\n",
    "        print(model_updates)\n",
    "        pklName = f'../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN/model_updates/fmnist_NEUROTOXIN_{roundNum}.pkl'\n",
    "        with open(pklName, 'rb') as f:\n",
    "            param_updates: Dict[str, torch.Tensor] = pickle.load(f)\n",
    "        param_updates = torch.cat(list(param_updates.values()), dim=1)\n",
    "        # print(param_updates[list(param_updates.keys())[0]].shape)  # torch.Size([10, 12288])\n",
    "        pklName = f'../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN/clean_param_updates/fmnist_NEUROTOXIN_{roundNum}.pkl'\n",
    "        with open(pklName, 'rb') as f:\n",
    "            clean_param_update: torch.Tensor = pickle.load(f)\n",
    "        \n",
    "        # global_update = fltrust_original(model_updates, [param_updates[key] for key in param_updates.keys()], clean_param_update)\n",
    "        global_update = fltrust_original(model_updates, param_updates, clean_param_update)\n",
    "\n",
    "        # print(clean_param_update.shape)  # torch.Size([2674688])\n",
    "        thisGradients = [0] * clientPerRound\n",
    "\n",
    "\n",
    "        gradientsList.append(thisGradients)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPkl(roundNum: int, dirPath: str) -> Dict[str, torch.Tensor]:  # 把pkl变成{'key1': [[user3], [user5]]}\n",
    "    # dirPath = '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-12_23-26-43-avg-fmnist_NEUROTOXIN/'\n",
    "    pklPrefix = os.path.basename(os.path.normpath(dirPath)).split('-')[-1]\n",
    "    print(pklPrefix, roundNum)\n",
    "    pklName = os.path.join(dirPath, f'model_updates/{pklPrefix}_{roundNum}.pkl')\n",
    "    with open(pklName, 'rb') as f:\n",
    "        update: Dict[str, torch.Tensor] = pickle.load(f)\n",
    "    return update\n",
    "\n",
    "\"\"\"把pkl变成torch.Tensor([2674688])，即干净分量展平后的结果\"\"\"\n",
    "def loadPkl_clean(roundNum: int, dirPath: str) ->  torch.Tensor:\n",
    "    pklPrefix = os.path.basename(os.path.normpath(dirPath)).split('-')[-1]  # fmnist_EDGE_CASE\n",
    "    pklName = os.path.join(dirPath, f'clean_param_updates/{pklPrefix}_{roundNum}.pkl')\n",
    "    with open(pklName, 'rb') as f:\n",
    "        update: torch.Tensor = pickle.load(f)\n",
    "    # print(type(update))  # torch.Tensor\n",
    "    # print(update.shape)  # torch.Size([2674688])\n",
    "    # print(update[list(update.keys())[0]])\n",
    "    return update\n",
    "\n",
    "\"\"\"获取参与者列表\"\"\"\n",
    "def getParticipants(roundNum: int, dirPath: str) -> np.ndarray:\n",
    "    participantFilePath = os.path.join(dirPath, 'participants/participants.csv')\n",
    "    participants = np.genfromtxt(\n",
    "        participantFilePath, delimiter=\",\", dtype=None, encoding=\"utf-8\"\n",
    "    )\n",
    "    participants = participants[1:].T\n",
    "    participants_thisRound = participants[roundNum]  # 获取当前轮次的参与者\n",
    "    return participants_thisRound\n",
    "\n",
    "# # 通过历史记录的梯度计算恶意客户端\n",
    "# def calc_maliciousAndCos_justByGrads(roundsNum: List[int], dirPath: str):\n",
    "#     maliciousUpdates = []\n",
    "#     benignUpdates = []\n",
    "#     for th, roundNum in enumerate(roundsNum):\n",
    "#         update = loadPkl(roundNum, dirPath)\n",
    "#         update = torch.cat(list(update.values()), dim=1)\n",
    "#         participants = getParticipants(roundNum, dirPath)\n",
    "#         temp = torch.zeros(update.shape)\n",
    "#         # print(update.shape)  # torch.Size([10, 2674688])\n",
    "#         # print(temp.shape)\n",
    "#         for i in range(10):\n",
    "#             if participants[i] < 3:\n",
    "#                 maliciousUpdates.append(update[i].cpu().numpy())\n",
    "#             else:\n",
    "#                 benignUpdates.append(update[i].cpu().numpy())\n",
    "#     all = maliciousUpdates + benignUpdates\n",
    "#     cs = cosine_similarity(all)\n",
    "#     return cs\n",
    "\n",
    "def kmeans_clustering(updates_np, n_clusters=2):\n",
    "    return 0, [], 0\n",
    "    # 使用KMeans算法进行聚类\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(updates_np)\n",
    "\n",
    "    # 获取聚类标签\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # 识别被认为是恶意客户端的索引（假设第一个聚类为恶意客户端）\n",
    "    malicious_indices = np.where(labels == 0)[0] if n_clusters == 2 else []\n",
    "\n",
    "    return labels, malicious_indices, kmeans\n",
    "\n",
    "# 通过历史记录的梯度计算恶意客户端\n",
    "def calc_maliciousAndCos_justByGrads(roundsNum: List[int], dirPath: str):\n",
    "    clientPerRound = 50  # 这里就先写死了\n",
    "    maliciousPerRound = 20\n",
    "    gradients = [0] * len(roundsNum) * clientPerRound\n",
    "    maliciouses = []\n",
    "    for th, roundNum in enumerate(roundsNum):\n",
    "        participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "        model_updates = loadPkl(roundNum, dirPath)\n",
    "        keys = list(model_updates.keys())\n",
    "        last_layer_updates = model_updates[keys[-2]]\n",
    "        K = len(last_layer_updates)\n",
    "        for i in range(K):\n",
    "            thisParticipant = participants_thisRound[i]\n",
    "            if thisParticipant < maliciousPerRound:\n",
    "                thisIndex = th * maliciousPerRound + thisParticipant\n",
    "            else:\n",
    "                thisIndex = maliciousPerRound * len(roundsNum) + (clientPerRound - maliciousPerRound) * th + thisParticipant - maliciousPerRound\n",
    "            gradients[thisIndex] = last_layer_updates[i].cpu().numpy()\n",
    "        # _, foolsgoldMaliciousIndex, _ = kmeans_clustering(gradients)\n",
    "        _, foolsgoldMaliciousIndex, _ = foolsgold_oneRound(roundNum, dirPath)\n",
    "        for thisMaliciousIndex in foolsgoldMaliciousIndex:\n",
    "            if thisMaliciousIndex < maliciousPerRound:\n",
    "                thisIndex = th * maliciousPerRound + thisMaliciousIndex\n",
    "            else:\n",
    "                thisIndex = maliciousPerRound * len(roundsNum) + (clientPerRound - maliciousPerRound) * th + thisMaliciousIndex - maliciousPerRound\n",
    "            maliciouses.append(thisIndex)\n",
    "    shuffleArray = np.arange(50)\n",
    "    np.random.shuffle(shuffleArray[0:len(roundsNum) * maliciousPerRound])\n",
    "    np.random.shuffle(shuffleArray[len(roundsNum) * maliciousPerRound:50])\n",
    "    malicious_shuffled = [shuffleArray[i] for i in maliciouses]\n",
    "    gradients_shuffled = [gradients[shuffleArray[i]] for i in range(50)]\n",
    "    cs = smp.cosine_similarity(gradients_shuffled)\n",
    "    cs = torch.from_numpy(cs)\n",
    "    cs_min = cs.min()  # 找到最小值\n",
    "    cs_max = cs.max()  # 找到最大值\n",
    "    cs = (cs - cs_min) / (cs_max - cs_min)\n",
    "    return malicious_shuffled, cs\n",
    "\n",
    "\n",
    "def min_enclosing_ball(points, zeta, eta_prime, max_iter, lambda_thresh):\n",
    "    \"\"\"\n",
    "    使用迭代方法计算最小覆盖超球的球心和半径。\n",
    "    \n",
    "    参数:\n",
    "    - points: 客户端的历史梯度更新，形状为 (T, d)\n",
    "    - zeta: 覆盖比例\n",
    "    - eta_prime: 学习率\n",
    "    - max_iter: 最大迭代次数\n",
    "    - lambda_thresh: 收敛阈值\n",
    "    \n",
    "    返回:\n",
    "    - center: 球心（意图点）\n",
    "    - radius: 球半径\n",
    "    \"\"\"\n",
    "    # 确保 points 是二维数组\n",
    "    points = np.array(points)\n",
    "    if len(points.shape) == 1:\n",
    "        points = points.reshape(1, -1)\n",
    "    \n",
    "    # 初始化球心为所有点的均值\n",
    "    O_i = np.mean(points, axis=0)\n",
    "    # 初始化球半径为当前球心到所有点的最大距离\n",
    "    r_i = max(np.linalg.norm(O_i - points, axis=1))\n",
    "    \n",
    "    # 迭代优化球心和半径\n",
    "    for k in range(max_iter):\n",
    "        # 计算球心到每个点的方向向量\n",
    "        projection_points = []\n",
    "        for point in points:\n",
    "            direction = O_i - point\n",
    "            norm_direction = np.linalg.norm(direction)\n",
    "            if norm_direction == 0:\n",
    "                proj_point = point  # 如果球心和点重合，直接使用点作为投影点\n",
    "            else:\n",
    "                # 投影点的计算\n",
    "                proj_point = O_i + eta_prime * (point - O_i) / norm_direction\n",
    "            projection_points.append(proj_point)\n",
    "        \n",
    "        # 确保 projection_points 不为空\n",
    "        if len(projection_points) == 0:\n",
    "            break\n",
    "        \n",
    "        # 计算投影点到球心的距离\n",
    "        projection_points = np.array(projection_points)\n",
    "        distances = np.linalg.norm(O_i - projection_points, axis=1)\n",
    "\n",
    "        # 如果 selected_indices 为空，使用最大距离的点作为替代\n",
    "        if len(distances) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 选择前 zeta*T 个投影点\n",
    "        select_count = max(1, int(zeta * len(projection_points)))  # 确保至少选择一个点\n",
    "        selected_indices = np.argsort(distances)[:select_count]\n",
    "        \n",
    "        if len(selected_indices) == 0:\n",
    "            max_proj_point = projection_points[np.argmax(distances)]  # 使用最大距离的点\n",
    "        else:\n",
    "            max_proj_point = projection_points[selected_indices[-1]]  # 最远的投影点\n",
    "        \n",
    "        # 更新球心\n",
    "        O_i = O_i + eta_prime * (max_proj_point - O_i)\n",
    "        # 计算新的半径\n",
    "        r_new = max(np.linalg.norm(O_i - projection_points[selected_indices], axis=1))\n",
    "        \n",
    "        # 检查收敛条件\n",
    "        if abs(r_new - r_i) < lambda_thresh:\n",
    "            break\n",
    "        \n",
    "        r_i = r_new\n",
    "    \n",
    "    return O_i, r_i\n",
    "\n",
    "def calc_maliciousAndCos_justByGrads_SecFFT(roundsNum: List[int], dirPath: str, zeta=0.8, eta_prime=0.1, max_iter=100, lambda_thresh=1e-4):\n",
    "    clientPerRound = 50  # 这里就先写死了\n",
    "    maliciousPerRound = 20\n",
    "    gradients = [0] * len(roundsNum) * clientPerRound\n",
    "    # maliciouses = []\n",
    "    for th, roundNum in enumerate(roundsNum):\n",
    "        participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "        model_updates = loadPkl(roundNum, dirPath)\n",
    "        keys = list(model_updates.keys())\n",
    "        last_layer_updates = model_updates[keys[-2]]\n",
    "        K = len(last_layer_updates)\n",
    "        for i in range(K):\n",
    "            thisParticipant = participants_thisRound[i]\n",
    "            if thisParticipant < maliciousPerRound:\n",
    "                thisIndex = th * maliciousPerRound + thisParticipant\n",
    "            else:\n",
    "                thisIndex = maliciousPerRound * len(roundsNum) + (clientPerRound - maliciousPerRound) * th + thisParticipant - maliciousPerRound\n",
    "            gradients[thisIndex] = last_layer_updates[i].cpu().numpy()\n",
    "        # _, foolsgoldMaliciousIndex, _ = kmeans_clustering(gradients)\n",
    "        # _, foolsgoldMaliciousIndex, _ = foolsgold_oneRound(roundNum, dirPath)\n",
    "        # for thisMaliciousIndex in foolsgoldMaliciousIndex:\n",
    "        #     if thisMaliciousIndex < maliciousPerRound:\n",
    "        #         thisIndex = th * maliciousPerRound + thisMaliciousIndex\n",
    "        #     else:\n",
    "        #         thisIndex = maliciousPerRound * len(roundsNum) + (clientPerRound - maliciousPerRound) * th + thisMaliciousIndex - maliciousPerRound\n",
    "        #     maliciouses.append(thisIndex)\n",
    "    shuffleArray = np.arange(50)\n",
    "    np.random.shuffle(shuffleArray[0:len(roundsNum) * maliciousPerRound])\n",
    "    np.random.shuffle(shuffleArray[len(roundsNum) * maliciousPerRound:50])\n",
    "    # malicious_shuffled = [shuffleArray[i] for i in maliciouses]\n",
    "    gradients_shuffled = [gradients[shuffleArray[i]] for i in range(50)]\n",
    "    intentions = []\n",
    "    for grad in gradients_shuffled:\n",
    "        grad = np.array(grad)\n",
    "        # 使用最小覆盖球算法计算意图点\n",
    "        center, radius = min_enclosing_ball(grad, zeta, eta_prime, max_iter, lambda_thresh)\n",
    "        intentions.append(center)  # 意图点是球心\n",
    "    # print(intentions)\n",
    "    # print(len(intentions))\n",
    "    # for n_neighbors in range(1, 52):\n",
    "    #     lof = LocalOutlierFactor(n_neighbors=n_neighbors, metric='euclidean')\n",
    "    #     lof_scores = lof.fit_predict(intentions)  # 使用LOF检测异常客户端\n",
    "    #     detected_malicious = [i for i, score in enumerate(lof_scores) if score == -1]\n",
    "    #     print(f'n_neighbors = {n_neighbors}, len(detected_malicious) = {len(detected_malicious)}')\n",
    "    # n_neighbors = 1, len(detected_malicious) = 0\n",
    "    # n_neighbors = 2, len(detected_malicious) = 0\n",
    "    # n_neighbors = 3, len(detected_malicious) = 0\n",
    "    # n_neighbors = 4, len(detected_malicious) = 0\n",
    "    # n_neighbors = 5, len(detected_malicious) = 0\n",
    "    # n_neighbors = 6, len(detected_malicious) = 0\n",
    "    # n_neighbors = 7, len(detected_malicious) = 0\n",
    "    # n_neighbors = 8, len(detected_malicious) = 0\n",
    "    # n_neighbors = 9, len(detected_malicious) = 0\n",
    "    # n_neighbors = 10, len(detected_malicious) = 0\n",
    "    # n_neighbors = 11, len(detected_malicious) = 0\n",
    "    # n_neighbors = 12, len(detected_malicious) = 0\n",
    "    # n_neighbors = 13, len(detected_malicious) = 0\n",
    "    # n_neighbors = 14, len(detected_malicious) = 0\n",
    "    # n_neighbors = 15, len(detected_malicious) = 15\n",
    "    # n_neighbors = 16, len(detected_malicious) = 15\n",
    "    # n_neighbors = 17, len(detected_malicious) = 15\n",
    "    # n_neighbors = 18, len(detected_malicious) = 15\n",
    "    # n_neighbors = 19, len(detected_malicious) = 15\n",
    "    # n_neighbors = 20, len(detected_malicious) = 15\n",
    "    # n_neighbors = 21, len(detected_malicious) = 15\n",
    "    # n_neighbors = 22, len(detected_malicious) = 15\n",
    "    # n_neighbors = 23, len(detected_malicious) = 15\n",
    "    # n_neighbors = 24, len(detected_malicious) = 15\n",
    "    # n_neighbors = 25, len(detected_malicious) = 15\n",
    "    # n_neighbors = 26, len(detected_malicious) = 15\n",
    "    # n_neighbors = 27, len(detected_malicious) = 15\n",
    "    # n_neighbors = 28, len(detected_malicious) = 15\n",
    "    # n_neighbors = 29, len(detected_malicious) = 15\n",
    "    # n_neighbors = 30, len(detected_malicious) = 15\n",
    "    # n_neighbors = 31, len(detected_malicious) = 15\n",
    "    # n_neighbors = 32, len(detected_malicious) = 15\n",
    "    # n_neighbors = 33, len(detected_malicious) = 15\n",
    "    # n_neighbors = 34, len(detected_malicious) = 15\n",
    "    # n_neighbors = 35, len(detected_malicious) = 0\n",
    "    # n_neighbors = 36, len(detected_malicious) = 0\n",
    "    # n_neighbors = 37, len(detected_malicious) = 0\n",
    "    # n_neighbors = 38, len(detected_malicious) = 0\n",
    "    # n_neighbors = 39, len(detected_malicious) = 0\n",
    "    # n_neighbors = 40, len(detected_malicious) = 0\n",
    "    # n_neighbors = 41, len(detected_malicious) = 0\n",
    "    # n_neighbors = 42, len(detected_malicious) = 0\n",
    "    # n_neighbors = 43, len(detected_malicious) = 0\n",
    "    # n_neighbors = 44, len(detected_malicious) = 0\n",
    "    # n_neighbors = 45, len(detected_malicious) = 0\n",
    "    # n_neighbors = 46, len(detected_malicious) = 0\n",
    "    # n_neighbors = 47, len(detected_malicious) = 0\n",
    "    # n_neighbors = 48, len(detected_malicious) = 0\n",
    "    # n_neighbors = 49, len(detected_malicious) = 0\n",
    "    # n_neighbors = 50, len(detected_malicious) = 0\n",
    "    # WC!!!!识别效果还真不错！！！\n",
    "    lof = LocalOutlierFactor(n_neighbors=int(50 * 0.4), metric='euclidean')\n",
    "    lof_scores = lof.fit_predict(intentions)  # 使用LOF检测异常客户端\n",
    "    detected_malicious = [i for i, score in enumerate(lof_scores) if score == -1]\n",
    "    print(detected_malicious)\n",
    "    cs = cosine_similarity(intentions)\n",
    "    cs = torch.from_numpy(cs)\n",
    "    cs_min = cs.min()  # 找到最小值\n",
    "    cs_max = cs.max()  # 找到最大值\n",
    "    cs = (cs - cs_min) / (cs_max - cs_min)\n",
    "    return detected_malicious, cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_attack_size(gradients, scaling_factor=0.1):\n",
    "    \"\"\"\n",
    "    限制前 20 个恶意客户端的梯度更新大小。\n",
    "    \n",
    "    参数:\n",
    "    - gradients: 包含 50 个客户端的梯度更新列表，每个元素是一个 NumPy 数组，表示一个客户端的梯度更新。 | 或者是一个Dict[str, Tensor]数组\n",
    "    - scaling_factor: 用于限制恶意客户端梯度大小的缩放因子。默认值为 0.1。\n",
    "    \n",
    "    返回:\n",
    "    - new_gradients: 经过处理后的梯度更新列表，前 20 个恶意客户端的更新被限制大小。\n",
    "    \"\"\"\n",
    "    if isinstance(gradients, dict):\n",
    "        for key, tensor in gradients.items():\n",
    "            assert tensor.size(0) == 50, f\"键 {key} 的梯度 Tensor 的第一个维度大小应该为 50\"\n",
    "        # 复制原始梯度字典以避免修改原始数据\n",
    "        new_gradients = {}\n",
    "        \n",
    "        # 遍历每个键值对，限制前 20 个客户端的梯度更新大小\n",
    "        for key, tensor in gradients.items():\n",
    "            # 创建一个新的 Tensor，复制原始数据\n",
    "            new_tensor = tensor.clone()\n",
    "            \n",
    "            # 对前 20 个客户端的梯度进行缩放\n",
    "            new_tensor[:20] *= scaling_factor\n",
    "            \n",
    "            # 将处理后的 Tensor 存入新字典\n",
    "            new_gradients[key] = new_tensor\n",
    "        \n",
    "        return new_gradients\n",
    "\n",
    "    # 确保输入是一个包含50个客户端梯度的列表\n",
    "    assert len(gradients) == 50, \"梯度列表的长度应该为 50\"\n",
    "    \n",
    "    # 复制原始梯度列表以避免修改原始数据\n",
    "    new_gradients = gradients.copy()\n",
    "    \n",
    "    # 限制前20个恶意客户端的梯度更新大小\n",
    "    for i in range(20):  # 前20个是恶意客户端\n",
    "        # 获取当前客户端的梯度更新\n",
    "        original_grad = new_gradients[i]\n",
    "        # 使用缩放因子限制梯度大小\n",
    "        limited_grad = original_grad * scaling_factor\n",
    "        # 更新新的梯度\n",
    "        new_gradients[i] = limited_grad\n",
    "    \n",
    "    return new_gradients\n",
    "\n",
    "# def limit_attack_direction(gradients, mixing_factor=0.1):\n",
    "#     \"\"\"\n",
    "#     限制前 15 个恶意客户端的梯度方向，使其与正常客户端的梯度方向更相似。\n",
    "    \n",
    "#     参数:\n",
    "#     - gradients: 包含 50 个客户端的梯度更新列表，每个元素是一个 NumPy 数组，表示一个客户端的梯度更新。\n",
    "#     - mixing_factor: 用于混合恶意客户端梯度方向和正常客户端平均梯度方向的因子。默认值为 0.9。\n",
    "    \n",
    "#     返回:\n",
    "#     - new_gradients: 经过处理后的梯度更新列表，前 15 个恶意客户端的更新被调整方向。\n",
    "#     \"\"\"\n",
    "#     # 确保输入是一个包含50个客户端梯度的列表\n",
    "#     assert len(gradients) == 50, \"梯度列表的长度应该为 50\"\n",
    "    \n",
    "#     # 复制原始梯度列表以避免修改原始数据\n",
    "#     new_gradients = gradients.copy()\n",
    "    \n",
    "#     # 计算正常客户端的平均梯度方向（客户端 16 到 50）\n",
    "#     normal_gradients = np.array(new_gradients[15:])  # 获取正常客户端的梯度\n",
    "#     average_normal_gradient = np.mean(normal_gradients, axis=0)  # 计算平均梯度方向\n",
    "    \n",
    "#     # 对平均梯度进行归一化\n",
    "#     average_normal_gradient /= np.linalg.norm(average_normal_gradient)\n",
    "    \n",
    "#     # 调整前 15 个恶意客户端的梯度方向\n",
    "#     for i in range(15):  # 前15个是恶意客户端\n",
    "#         # 获取当前恶意客户端的梯度更新\n",
    "#         original_grad = new_gradients[i]\n",
    "        \n",
    "#         # 对原始梯度进行归一化\n",
    "#         original_grad /= np.linalg.norm(original_grad)\n",
    "        \n",
    "#         # 将恶意客户端的梯度方向调整为与正常客户端的平均梯度方向更相似\n",
    "#         adjusted_grad = mixing_factor * average_normal_gradient + (1 - mixing_factor) * original_grad\n",
    "        \n",
    "#         # 重新调整梯度的大小为原始大小\n",
    "#         adjusted_grad *= np.linalg.norm(gradients[i])\n",
    "        \n",
    "#         # 更新新的梯度\n",
    "#         new_gradients[i] = adjusted_grad\n",
    "    \n",
    "#     return new_gradients\n",
    "\n",
    "def limit_attack_direction(gradients, mixing_factor=0.4):\n",
    "    \"\"\"\n",
    "    限制前 20 个恶意客户端的梯度方向，使其随机向一个良性客户端的梯度方向偏移。\n",
    "    \n",
    "    参数:\n",
    "    - gradients: 包含 50 个客户端的梯度更新列表，每个元素是一个 NumPy 数组，表示一个客户端的梯度更新。 | 或者是一个Dict[str, Tensor]数组\n",
    "    - mixing_factor: 用于混合恶意客户端梯度方向和随机选择的良性客户端梯度方向的因子。默认值为 0.9。\n",
    "    \n",
    "    返回:\n",
    "    - new_gradients: 经过处理后的梯度更新列表，前 20 个恶意客户端的更新被调整方向。\n",
    "    \"\"\"\n",
    "    if isinstance(gradients, dict):\n",
    "        # 确保字典中的每个值是形状为 [50, ...] 的 Tensor\n",
    "        for key, tensor in gradients.items():\n",
    "            assert tensor.size(0) == 50, f\"键 {key} 的梯度 Tensor 的第一个维度大小应该为 50\"\n",
    "        \n",
    "        # 复制原始梯度字典以避免修改原始数据\n",
    "        new_gradients = {}\n",
    "        \n",
    "        # 遍历每个键值对，对前 20 个恶意客户端进行方向限制\n",
    "        for key, tensor in gradients.items():\n",
    "            # 创建一个新的 Tensor，复制原始数据\n",
    "            new_tensor = tensor.clone()\n",
    "            \n",
    "            for i in range(20):  # 前 20 个是恶意客户端\n",
    "                # 随机选择一个良性客户端（索引范围是 20 到 49）\n",
    "                random_benign_index = np.random.randint(20, 49)\n",
    "                benign_grad = new_tensor[random_benign_index]\n",
    "                \n",
    "                # 获取当前恶意客户端的梯度更新\n",
    "                original_grad = new_tensor[i]\n",
    "                \n",
    "                # 对恶意客户端和选中的良性客户端的梯度进行归一化\n",
    "                original_grad_normalized = original_grad / torch.norm(original_grad)\n",
    "                benign_grad_normalized = benign_grad / torch.norm(benign_grad)\n",
    "                \n",
    "                # 将恶意客户端的梯度方向调整为向良性客户端方向偏移\n",
    "                adjusted_grad = (mixing_factor * benign_grad_normalized +\n",
    "                                (1 - mixing_factor) * original_grad_normalized)\n",
    "                \n",
    "                # 重新调整梯度的大小为原始大小\n",
    "                adjusted_grad *= torch.norm(tensor[i])\n",
    "                \n",
    "                # 更新新的梯度\n",
    "                new_tensor[i] = adjusted_grad\n",
    "            \n",
    "            # 将处理后的 Tensor 存入新字典\n",
    "            new_gradients[key] = new_tensor\n",
    "        \n",
    "        return new_gradients\n",
    "    # 确保输入是一个包含50个客户端梯度的列表\n",
    "    assert len(gradients) == 50, \"梯度列表的长度应该为 50\"\n",
    "    \n",
    "    # 复制原始梯度列表以避免修改原始数据\n",
    "    new_gradients = gradients.copy()\n",
    "    \n",
    "    # 对前 20 个恶意客户端进行处理\n",
    "    for i in range(20):  # 前20个是恶意客户端\n",
    "        # 随机选择一个良性客户端（20到50的索引是20到49）\n",
    "        random_benign_index = np.random.randint(20, 50)\n",
    "        benign_grad = new_gradients[random_benign_index]\n",
    "        \n",
    "        # 获取当前恶意客户端的梯度更新\n",
    "        original_grad = new_gradients[i]\n",
    "        \n",
    "        # 对恶意客户端和选中的良性客户端的梯度进行归一化\n",
    "        original_grad_normalized = original_grad / np.linalg.norm(original_grad)\n",
    "        benign_grad_normalized = benign_grad / np.linalg.norm(benign_grad)\n",
    "        \n",
    "        # 将恶意客户端的梯度方向调整为向良性客户端方向偏移\n",
    "        adjusted_grad = mixing_factor * benign_grad_normalized + (1 - mixing_factor) * original_grad_normalized\n",
    "        \n",
    "        # 重新调整梯度的大小为原始大小\n",
    "        adjusted_grad *= np.linalg.norm(gradients[i])\n",
    "        \n",
    "        # 更新新的梯度\n",
    "        new_gradients[i] = adjusted_grad\n",
    "    \n",
    "    return new_gradients\n",
    "\n",
    "def identify_malicious_clients(cosine_similarity_tensor, threshold=5):\n",
    "    \"\"\"\n",
    "    识别余弦相似度矩阵中的恶意客户端。\n",
    "    \n",
    "    参数:\n",
    "    cosine_similarity_tensor (torch.Tensor): 50x50的余弦相似度张量\n",
    "    threshold (float): 用于层次聚类的距离阈值\n",
    "    \n",
    "    返回:\n",
    "    List[int]: 恶意客户端的下标\n",
    "    \"\"\"\n",
    "    # 将 PyTorch 张量转换为 NumPy 数组\n",
    "    data = cosine_similarity_tensor.numpy()\n",
    "\n",
    "    # 对称化矩阵（确保余弦相似度矩阵是对称的）\n",
    "    data = (data + data.T) / 2\n",
    "\n",
    "    # 数据标准化\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # 计算层次聚类的链接矩阵\n",
    "    Z = linkage(data_scaled, method='ward')\n",
    "\n",
    "    # 根据距离阈值对客户端进行聚类\n",
    "    clusters = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "    # 找出每个簇的大小\n",
    "    cluster_counts = np.bincount(clusters)\n",
    "\n",
    "    # 识别恶意客户端所在的簇（假设簇中客户端数量较少的为恶意客户端）\n",
    "    malicious_clusters = np.where(cluster_counts <= 2)[0]  # 这里我们假设小于等于2个客户端的簇为恶意簇\n",
    "\n",
    "    # 找出恶意客户端的下标\n",
    "    malicious_clients = [index for index, cluster_label in enumerate(clusters) if cluster_label in malicious_clusters]\n",
    "\n",
    "    return malicious_clients\n",
    "\n",
    "# 通过历史记录的梯度计算恶意客户端 | 0是限制大小，1是限制大小和角度\n",
    "def calc_maliciousAndCos_justByGrads_limited(roundsNum: List[int], dirPath: str, limistedMethod=0):  # limistedMethod: 0-限制大小，1-限制方向\n",
    "    clientPerRound = 50  # 这里就先写死了\n",
    "    maliciousPerRound = 20\n",
    "    gradients = [0] * len(roundsNum) * clientPerRound\n",
    "    for th, roundNum in enumerate(roundsNum):\n",
    "        participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "        model_updates = loadPkl(roundNum, dirPath)\n",
    "        keys = list(model_updates.keys())\n",
    "        last_layer_updates = model_updates[keys[-2]]\n",
    "        K = len(last_layer_updates)\n",
    "        for i in range(K):\n",
    "            thisParticipant = participants_thisRound[i]\n",
    "            if thisParticipant < maliciousPerRound:\n",
    "                thisIndex = th * maliciousPerRound + thisParticipant\n",
    "            else:\n",
    "                thisIndex = maliciousPerRound * len(roundsNum) + (clientPerRound - maliciousPerRound) * th + thisParticipant - maliciousPerRound\n",
    "            gradients[thisIndex] = last_layer_updates[i].cpu().numpy()\n",
    "    shuffleArray = np.arange(50)\n",
    "    np.random.shuffle(shuffleArray[0:len(roundsNum) * maliciousPerRound])\n",
    "    np.random.shuffle(shuffleArray[len(roundsNum) * maliciousPerRound:50])\n",
    "    gradients_shuffled = [gradients[shuffleArray[i]] for i in range(50)]\n",
    "    if limistedMethod == 0:\n",
    "        gradients_limited = limit_attack_size(gradients_shuffled)\n",
    "    else:\n",
    "        gradients_limited = limit_attack_direction(gradients_shuffled)\n",
    "    cs = smp.cosine_similarity(gradients_limited)\n",
    "    malicious = identify_malicious_clients(torch.from_numpy(cs)) \n",
    "    cs = torch.from_numpy(cs)\n",
    "    cs_min = cs.min()  # 找到最小值\n",
    "    cs_max = cs.max()  # 找到最大值\n",
    "    cs = (cs - cs_min) / (cs_max - cs_min)\n",
    "    return malicious, cs\n",
    "\n",
    "def calc_maliciousAndCos_justByGrads_limited_SecFFT(roundsNum: List[int], dirPath: str, limistedMethod=0, zeta=0.8, eta_prime=0.1, max_iter=100, lambda_thresh=1e-4):\n",
    "    clientPerRound = 50  # 这里就先写死了  再不写死就写死了\n",
    "    maliciousPerRound = 20\n",
    "    gradients = [0] * len(roundsNum) * clientPerRound\n",
    "    for th, roundNum in enumerate(roundsNum):\n",
    "        participants_thisRound = getParticipants(roundNum, dirPath)\n",
    "        model_updates = loadPkl(roundNum, dirPath)\n",
    "        keys = list(model_updates.keys())\n",
    "        last_layer_updates = model_updates[keys[-2]]\n",
    "        K = len(last_layer_updates)\n",
    "        for i in range(K):\n",
    "            thisParticipant = participants_thisRound[i]\n",
    "            if thisParticipant < maliciousPerRound:\n",
    "                thisIndex = th * maliciousPerRound + thisParticipant\n",
    "            else:\n",
    "                thisIndex = maliciousPerRound * len(roundsNum) + (clientPerRound - maliciousPerRound) * th + thisParticipant - maliciousPerRound\n",
    "            gradients[thisIndex] = last_layer_updates[i].cpu().numpy()\n",
    "    shuffleArray = np.arange(50)\n",
    "    np.random.shuffle(shuffleArray[0:len(roundsNum) * maliciousPerRound])\n",
    "    np.random.shuffle(shuffleArray[len(roundsNum) * maliciousPerRound:50])\n",
    "    # malicious_shuffled = [shuffleArray[i] for i in maliciouses]\n",
    "    gradients_shuffled = [gradients[shuffleArray[i]] for i in range(50)]\n",
    "    if limistedMethod == 0:\n",
    "        gradients_limited = limit_attack_size(gradients_shuffled)\n",
    "    else:\n",
    "        gradients_limited = limit_attack_direction(gradients_shuffled)\n",
    "    intentions = []\n",
    "    for grad in gradients_limited:\n",
    "        grad = np.array(grad)\n",
    "        # 使用最小覆盖球算法计算意图点\n",
    "        center, radius = min_enclosing_ball(grad, zeta, eta_prime, max_iter, lambda_thresh)\n",
    "        intentions.append(center)  # 意图点是球心\n",
    "    # lof = LocalOutlierFactor(n_neighbors=int(50 * 0.4), metric='euclidean')\n",
    "    # lof_scores = lof.fit_predict(intentions)  # 使用LOF检测异常客户端\n",
    "    # detected_malicious = [i for i, score in enumerate(lof_scores) if score == -1]\n",
    "    # print(detected_malicious)\n",
    "    cs = cosine_similarity(intentions)\n",
    "    cs = torch.from_numpy(cs)\n",
    "    cs_min = cs.min()  # 找到最小值\n",
    "    cs_max = cs.max()  # 找到最大值\n",
    "    cs = (cs - cs_min) / (cs_max - cs_min)\n",
    "    return cs\n",
    "\n",
    "\"\"\"\n",
    "这是一段屎山代码，建议读者别看了还是。。。\n",
    "对于攻击方式：0是限制大小，1是限制大小和角度，其他值是不做限制\n",
    "返回 (识别出的恶意客户端下标(TODO), 余弦相似度)\n",
    "\"\"\"\n",
    "def fltrust_R1(roundNum: int, dirPath: str, limistedMethod: int=-1) -> Tuple[list[int], torch.Tensor]:\n",
    "    clientPerRound = 50  # 这里就先写死了\n",
    "    maliciousPerRound = 20\n",
    "    gradients = [0] * clientPerRound\n",
    "    participants = getParticipants(roundNum, dirPath)\n",
    "    # print(f'participants: {participants}')\n",
    "    print('participants:', participants)\n",
    "    model_updates = loadPkl(roundNum, dirPath)\n",
    "    # print('ID546464:', model_updates[list(model_updates.keys())[0]][1264:1276])  # ID546464: tensor([], device='cuda:0', size=(0, 12288))\n",
    "    print('ID546464:', model_updates[list(model_updates.keys())[0]][0, 1264:1276])  # ID546464: tensor([], device='cuda:0', size=(0, 12288))\n",
    "    keys = list(model_updates.keys())\n",
    "    # print('ID23131545:', keys)\n",
    "    # last_layer_updates = model_updates[keys[-2]]\n",
    "    # K = len(last_layer_updates)\n",
    "    # print(K)  # 50\n",
    "    K = len(model_updates[keys[0]])\n",
    "    # print(type(model_updates))  # dict\n",
    "    # print(keys[-2])  # base_model.model.visual_projection.lora_A.default.weight  # 当时为什么要这么做来着\n",
    "    # print(K)  # 50\n",
    "    for i in range(K):\n",
    "        # original # gradients[i] = last_layer_updates[i].cpu().numpy()\n",
    "        # debug = list(model_updates[key][i] for key in keys)\n",
    "        # print(len(debug), len(debug) == len(keys))  # 146 True\n",
    "        # print(torch.cat((torch.tensor([0]), torch.tensor([]), torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6])), dim=0))  # tensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "        gradients[i] = torch.cat(list(model_updates[key][i] for key in keys), dim=0)\n",
    "    # print(len(gradients[0]))  # 2674688\n",
    "    # shuffleArray = np.arange(50)\n",
    "    # np.random.shuffle(shuffleArray[0:maliciousPerRound])  # 不shuffle了\n",
    "    # np.random.shuffle(shuffleArray[maliciousPerRound:50])\n",
    "    # gradients_shuffled: List[np.ndarray] = [gradients[shuffleArray[i]] for i in range(50)]\n",
    "    # print(gradients[0].shape)  # torch.Size([2674688])\n",
    "    gradients_shuffled = gradients\n",
    "    if limistedMethod == 0:\n",
    "        gradients_shuffled = limit_attack_size([thisGradient.cpu() for thisGradient in gradients_shuffled])\n",
    "    elif limistedMethod == 1:\n",
    "        gradients_shuffled = limit_attack_direction([thisGradient.cpu() for thisGradient in gradients_shuffled])\n",
    "    gradients_shuffled = [thisGradient.to('cuda:0') for thisGradient in gradients_shuffled]\n",
    "\n",
    "    print('ID1313131: gradients')  # 开始不一样\n",
    "    print(gradients_shuffled)\n",
    "    print('end ID1313131')\n",
    "\n",
    "    # https://github.com/LetMeFly666/SecFFT/blob/0f829a07a55b66336fccd33fb9dedacb1f8103b0/NormalRun/FL_Backdoor_CV/roles/aggregation_rules.py#L301-L340\n",
    "    # 从这里开始融合fltrust的代码\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    clean_param_update = loadPkl_clean(roundNum, dirPath)\n",
    "    g0_norm = torch.norm(clean_param_update)\n",
    "    weights = []\n",
    "    # for param_update in param_updates:\n",
    "    for param_update in gradients_shuffled:\n",
    "        # print(type(param_update))  # <class 'numpy.ndarray'>\n",
    "        # param_update = np.ndarray(param_update)\n",
    "        param_update = torch.tensor(param_update)\n",
    "        weights.append(\n",
    "            F.relu(cos(param_update.view(-1, 1), clean_param_update.view(-1, 1)))\n",
    "            # F.relu(cos(param_update.reshape(-1, 1), clean_param_update.reshape(-1, 1)))\n",
    "        )\n",
    "    weights = torch.tensor(weights).to('cuda:0').view(1, -1)\n",
    "    print('ID1111: weights in middle')\n",
    "    print(weights)\n",
    "    print('end ID1111')\n",
    "    weights = weights / weights.sum()\n",
    "    weights = torch.where(weights[0].isnan(), torch.zeros_like(weights), weights)\n",
    "    nonzero_weights = torch.count_nonzero(weights.flatten())\n",
    "    nonzero_indices = torch.nonzero(weights.flatten()).flatten()\n",
    "    # print(type(weights))  # <class 'torch.Tensor'>\n",
    "    # print(weights.type())  # torch.cuda.FloatTensor\n",
    "    # print(weights.shape)  # torch.Size([1, 50])\n",
    "    weights = weights.squeeze()\n",
    "    print(weights.shape)  # torch.Size([50])\n",
    "    if True:  # 使用两个客户端的分数之差作为依据\n",
    "        # 扩展 weights 的维度\n",
    "        weights_i = weights.unsqueeze(0)  # 变成 [1, 50]\n",
    "        weights_j = weights.unsqueeze(1)  # 变成 [50, 1]\n",
    "        # 计算差的绝对值\n",
    "        cs = torch.abs(weights_i - weights_j)\n",
    "        print(limistedMethod, '*' * 50)\n",
    "        print(weights)\n",
    "        print('*' * 50)\n",
    "        print(cs)\n",
    "        \"\"\"\n",
    "        我有一个相似度数组`cs`准备画热力图，数据类型是torch.tensor(cuda:0)，每个数据很小，我想归一化一下，让所有数据变到0-1附近\n",
    "        \"\"\"\n",
    "        cs_min = cs.min()  # 找到最小值\n",
    "        cs_max = cs.max()  # 找到最大值\n",
    "        cs = (cs - cs_min) / (cs_max - cs_min)\n",
    "        # 相似度越高值越大\n",
    "        cs = 1 - cs\n",
    "    else:  # 两个客户端各自分数的余弦相似度作为依据，一维向量两两之间余弦相似度都是1\n",
    "        \"\"\"\n",
    "        我有一个torch.Size([50])的数组weights，其位于cuda:0上，我想使用`from sklearn.metrics.pairwise import cosine_similarity`的cosine_similarity函数计算两两之间的余弦相似度\n",
    "\n",
    "        为什么计算出来的结果是：\n",
    "        ```\n",
    "        print(type(cs))  # <class 'numpy.ndarray'>\n",
    "        print(cs.size)  # 1\n",
    "        ```\n",
    "\n",
    "        我的weights想表示的是50个向量，每个向量的长度为1，不是1个向量长度为50\n",
    "        当前形状为torch.Size([50])，我应该如何修改\n",
    "\n",
    "        ```\n",
    "        cs = cosine_similarity(weights.cpu().numpy().reshape(-1, 1))\n",
    "        print(type(cs))  # <class 'numpy.ndarray'>\n",
    "        print(cs.size)  # 2500\n",
    "        ```\n",
    "        我想将cs数组修改为torch.Size([50, 50])，应该如何做？\n",
    "\n",
    "        weights数组为：\n",
    "        ```\n",
    "        tensor([0.0191, 0.0191, 0.0190, 0.0192, 0.0190, 0.0189, 0.0191, 0.0189, 0.0189,\n",
    "        0.0190, 0.0191, 0.0190, 0.0190, 0.0189, 0.0190, 0.0191, 0.0190, 0.0191,\n",
    "        0.0192, 0.0190, 0.0206, 0.0206, 0.0206, 0.0206, 0.0205, 0.0206, 0.0206,\n",
    "        0.0207, 0.0206, 0.0206, 0.0207, 0.0207, 0.0206, 0.0206, 0.0206, 0.0207,\n",
    "        0.0208, 0.0207, 0.0206, 0.0207, 0.0206, 0.0208, 0.0207, 0.0206, 0.0207,\n",
    "        0.0207, 0.0207, 0.0207, 0.0206, 0.0207], device='cuda:0')\n",
    "        ```\n",
    "        为什么得到的`cs`数组全部为1？\n",
    "        ```\n",
    "\n",
    "        ```\n",
    "        \"\"\"\n",
    "        print(weights)\n",
    "        cs = cosine_similarity(weights.cpu().numpy().reshape(-1, 1))\n",
    "        # print(type(cs))  # <class 'numpy.ndarray'>\n",
    "        # print(cs.size())  # 2500\n",
    "        cs = torch.from_numpy(cs).reshape(50, 50)\n",
    "        print('1212121212121')\n",
    "        # print(type(cs))  # <class 'torch.Tensor'>\n",
    "        # print(cs.size())  # torch.Size([50, 50])\n",
    "        print(cs)\n",
    "    # print(cs.shape)  # 输出: torch.Size([50, 50])\n",
    "    # print(cs)\n",
    "    return list(range(20)), cs  # TODO: 计算真正的恶意客户端的下标\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "不描述了，描述的话也和fltrust_R1一模一样\n",
    "就问我这代码重复率高不高\n",
    "\"\"\"\n",
    "def flame_R1(roundNum: int, dirPath: str, limistedMethod: int=-1, historyGlobal: List[torch.Tensor]=[]) -> Tuple[list[int], torch.Tensor]:\n",
    "    clientPerRound = 50  # 这里就先写死了\n",
    "    maliciousPerRound = 20\n",
    "    gradients = [0] * clientPerRound\n",
    "    participants = getParticipants(roundNum, dirPath)\n",
    "    print('participants:', participants)\n",
    "    model_updates = loadPkl(roundNum, dirPath)\n",
    "    if limistedMethod == 0:\n",
    "        model_updates = limit_attack_size(model_updates)\n",
    "    elif limistedMethod == 1:\n",
    "        model_updates = limit_attack_direction(model_updates)\n",
    "    # keys = list(model_updates.keys())\n",
    "    # K = len(model_updates[keys[0]])\n",
    "    # for i in range(K):\n",
    "    #     gradients[i] = torch.cat(list(model_updates[key][i] for key in keys), dim=0)\n",
    "    # shuffleArray = np.arange(50)\n",
    "    # np.random.shuffle(shuffleArray[0:maliciousPerRound])\n",
    "    # np.random.shuffle(shuffleArray[maliciousPerRound:50])\n",
    "    # gradients_shuffled: List[np.ndarray] = [gradients[shuffleArray[i]] for i in range(50)]\n",
    "    # if limistedMethod == 0:\n",
    "    #     gradients_shuffled = limit_attack_size([thisGradient.cpu() for thisGradient in gradients_shuffled])\n",
    "    # else:\n",
    "    #     gradients_shuffled = limit_attack_direction([thisGradient.cpu() for thisGradient in gradients_shuffled])\n",
    "    # gradients_shuffled = [thisGradient.to('cuda:0') for thisGradient in gradients_shuffled]\n",
    "    # # python函数如何令一个变量在下次调用时仍使用上次的结果\n",
    "    if not roundNum:\n",
    "        pklPath = os.path.join(dirPath, 'SpecialForGlobalModelDir', 'initial.pkl')\n",
    "        print(pklPath)\n",
    "        historyGlobal.append(torch.cat([param.reshape(1, -1) for param in pickle.load(open(pklPath, 'rb')).values()], dim=1))\n",
    "    # trained_params = [] \n",
    "    # current_model_param: torch.Tensor = torch.tensor([0])\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        trained_params: [模型1展平后的结果, 模型2展平后的结果, ...]\n",
    "        current_model_param: 当前全局模型展平后的结果\n",
    "        param_updates: \n",
    "    \"\"\"\n",
    "    def flame(trained_params, current_model_param, param_updates):\n",
    "        # === clustering ===\n",
    "        trained_params = trained_params.to(torch.float64)\n",
    "        current_model_param = current_model_param.to(torch.float64)\n",
    "        cluster = hdbscan.HDBSCAN(\n",
    "            metric=\"cosine\",\n",
    "            algorithm=\"generic\",\n",
    "            min_cluster_size=50 // 2 + 1,  # 这里还先写死了\n",
    "            min_samples=1,\n",
    "            allow_single_cluster=True,\n",
    "        )  # 聚类错误哈哈 - 这个值很重要，后面要作为画图依据喽\n",
    "        # print(cluster)  # HDBSCAN(algorithm='generic', allow_single_cluster=True, metric='cosine', min_cluster_size=26, min_samples=1)\n",
    "        cluster.fit(trained_params)\n",
    "        predict_good = []\n",
    "        for i, j in enumerate(cluster.labels_):\n",
    "            if j == 0:\n",
    "                predict_good.append(i)\n",
    "        k = len(predict_good)\n",
    "        # print(predict_good)  # [20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49]\n",
    "        # sigma: 0.0005017087373992877; #clean models / clean models: 26 / [20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49], median norm: 1.2505888454786356,\n",
    "\n",
    "        # === median clipping ===\n",
    "        model_updates = trained_params[predict_good] - current_model_param\n",
    "        local_norms = torch.norm(model_updates, dim=1)\n",
    "        # print(local_norms.shape)  # torch.Size([26])  # 因为每次识别出来都是认为26个良性客户端\n",
    "        S_t = torch.median(local_norms)\n",
    "        scale = S_t / local_norms\n",
    "        scale = torch.where(scale > 1, torch.ones_like(scale), scale)\n",
    "        model_updates = model_updates * scale.view(-1, 1)\n",
    "\n",
    "        # === aggregating ===\n",
    "        trained_params = current_model_param + model_updates\n",
    "        trained_params = trained_params.sum(dim=0) / k\n",
    "\n",
    "        # === noising ===\n",
    "        participant_sample_size = 50\n",
    "        delta = 1 / (50 ** 2)\n",
    "        epsilon = 10000\n",
    "        lambda_ = 1 / epsilon * (math.sqrt(2 * math.log((1.25 / delta))))\n",
    "        sigma = lambda_ * S_t.numpy()\n",
    "        # print(\n",
    "        #     f\"sigma: {sigma}; #clean models / clean models: {k} / {predict_good}, median norm: {S_t},\"\n",
    "        # )\n",
    "        trained_params.add_(torch.normal(0, sigma, size=trained_params.size()))\n",
    "\n",
    "        # === bn ===\n",
    "        global_update = dict()\n",
    "        for i, (name, param) in enumerate(param_updates.items()):\n",
    "            if \"num_batches_tracked\" in name:\n",
    "                global_update[name] = (\n",
    "                    1 / k * param_updates[name][predict_good].sum(dim=0, keepdim=True)\n",
    "                )\n",
    "            elif \"running_mean\" in name or \"running_var\" in name:\n",
    "                local_norms = torch.norm(param_updates[name][predict_good], dim=1)\n",
    "                S_t = torch.median(local_norms)\n",
    "                scale = S_t / local_norms\n",
    "                scale = torch.where(scale > 1, torch.ones_like(scale), scale)\n",
    "                global_update[name] = param_updates[name][predict_good] * scale.view(-1, 1)\n",
    "                global_update[name] = 1 / k * global_update[name].sum(dim=0, keepdim=True)\n",
    "        \"\"\"\n",
    "        我每轮次有50个客户端，我想绘制一个50x50的热力图。如何根据计算过程中的某个值作为依据进行绘制？\n",
    "\n",
    "        我当前主要不是想问你热力图怎么绘制，而是想问你原始flame函数中什么值适合作为热力图数据的计算依据\n",
    "\n",
    "        能解释一下hdbscan.HDBSCAN吗？HDBSCAN只会返回聚类结果而不会返回一个“分数”之类的吗？\n",
    "\n",
    "        ```\n",
    "        outlierScores = cluster.outlier_scores_\n",
    "        # print(type(outlierScores))  # <class 'numpy.ndarray'>\n",
    "        # print(type(outlierScores[0]))  # <class 'numpy.float64'>\n",
    "        ```\n",
    "        我想以outlierScores作为热力图绘制依据\n",
    "        \"\"\"\n",
    "        # 这里是第三列的绘图依据\n",
    "        # print(cluster.outlier_scores_)  # [9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 9.56844229e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.30958738e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.61708604e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.75602489e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.68616044e-02 0.00000000e+00 0.00000000e+00]\n",
    "        # print(cluster.probabilities_)  # [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
    "        outlierScores = cluster.outlier_scores_\n",
    "        # print(type(outlierScores))  # <class 'numpy.ndarray'>\n",
    "        # print(type(outlierScores[0]))  # <class 'numpy.float64'>\n",
    "        cs = np.zeros((50, 50))\n",
    "        for i in range(50):\n",
    "            for j in range(50):\n",
    "                cs[i, j] = abs(outlierScores[i] - outlierScores[j])\n",
    "        cs = torch.tensor(cs)\n",
    "        cs_min = cs.min()  # 找到最小值\n",
    "        cs_max = cs.max()  # 找到最大值\n",
    "        cs = (cs - cs_min) / (cs_max - cs_min)\n",
    "        # 相似度越高值越大\n",
    "        cs = 1 - cs\n",
    "        return trained_params.float().to('cuda:0'), global_update, predict_good, cs\n",
    "    \n",
    "    updates = torch.cat(list(model_updates.values()), dim=1)  # updates的形状为n*d, n为参与者数量，d为梯度向量维度\n",
    "    # print(updates.size())  # torch.Size([50, 2674688])\n",
    "    # print(historyGlobal[-1].size())  # torch.Size([1, 2674688])\n",
    "    user_params = historyGlobal[-1] + updates\n",
    "    newGlobalParam, globalUpdate, predict_good, cs = flame(user_params.cpu(), historyGlobal[-1].cpu(), model_updates)\n",
    "    historyGlobal.append(newGlobalParam)\n",
    "    return predict_good, cs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(identified_malicious: List[int]) -> Tuple:\n",
    "    total_clients = 50\n",
    "    malicious_clients = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}  # 实际恶意客户端的索引\n",
    "\n",
    "    # 计算TP, FP, TN, FN\n",
    "    TP = len(malicious_clients.intersection(identified_malicious))\n",
    "    FP = len(set(identified_malicious) - malicious_clients)\n",
    "    FN = len(malicious_clients - set(identified_malicious))\n",
    "    TN = total_clients - TP - FP - FN\n",
    "\n",
    "    # 准确率（Accuracy）\n",
    "    accuracy = (TP + TN) / total_clients\n",
    "\n",
    "    # 精确率（Precision）\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    # 召回率（Recall）\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    # 特异度（Specificity）\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "    # 误报率（False Positive Rate, FPR）\n",
    "    fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "    # 假阴率（False Negative Rate, FNR）\n",
    "    fnr = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    # 计算AUC（此处假设预测的概率值，以便计算AUC）\n",
    "    y_true = [1 if i in malicious_clients else 0 for i in range(total_clients)]\n",
    "    y_scores = [1 if i in identified_malicious else 0 for i in range(total_clients)]\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    # 计算MCC（Matthews Correlation Coefficient）\n",
    "    mcc = matthews_corrcoef(y_true, y_scores)\n",
    "\n",
    "    return TP, FP, TN, FN, accuracy, precision, recall, specificity, fpr, fnr, auc, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "查看为什么第二列FLTrust限制大小后识别效果反而变得更好了\n",
    "（其实归一化之前刻度和另外两个不一样）\n",
    "\"\"\"\n",
    "if False:\n",
    "    fltrustMaliciousIndex, fltrustScore2 = fltrust_R1(15, 'D:/Data-R1/2024-10-03_03-09-36-fltrust-fmnist_EDGE_CASE')\n",
    "    fltrustMalicious, fltrustLimited = fltrust_R1(15, 'D:/Data-R1/2024-10-03_03-09-36-fltrust-fmnist_EDGE_CASE', limistedMethod=0)\n",
    "    fltrustMalicious2, fltrustDirection = fltrust_R1(15, 'D:/Data-R1/2024-10-03_03-09-36-fltrust-fmnist_EDGE_CASE', limistedMethod=1)\n",
    "    \"\"\"\n",
    "    weights: \n",
    "       + 限制大小后，恶意的分数变小了良性变大了，所以差别更加明显了\n",
    "       + 限制大小和角度后，又回去了\n",
    "\n",
    "\n",
    "    原始数据：\n",
    "    0.0198, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0196, 0.0201, 0.0196,\n",
    "    0.0198, 0.0199, 0.0197, 0.0198, 0.0197, 0.0200, 0.0199, 0.0197, 0.0198,\n",
    "    0.0196, 0.0198, 0.0202, 0.0202, 0.0202, 0.0199, 0.0200, 0.0203, 0.0201,\n",
    "    0.0202, 0.0201, 0.0201, 0.0200, 0.0200, 0.0204, 0.0204, 0.0200, 0.0200,\n",
    "    0.0201, 0.0204, 0.0203, 0.0200, 0.0200, 0.0203, 0.0200, 0.0202, 0.0201,\n",
    "    0.0203, 0.0202, 0.0201, 0.0203, 0.0201\n",
    "\n",
    "\n",
    "    限制大小:\n",
    "    0.0192, 0.0187, 0.0190, 0.0188, 0.0189, 0.0191, 0.0189, 0.0189, 0.0188,\n",
    "    0.0189, 0.0194, 0.0191, 0.0186, 0.0186, 0.0188, 0.0190, 0.0188, 0.0185,\n",
    "    0.0189, 0.0189, 0.0206, 0.0208, 0.0206, 0.0209, 0.0207, 0.0207, 0.0209,\n",
    "    0.0207, 0.0206, 0.0206, 0.0209, 0.0208, 0.0207, 0.0204, 0.0208, 0.0207,\n",
    "    0.0206, 0.0206, 0.0207, 0.0210, 0.0210, 0.0210, 0.0208, 0.0205, 0.0207,\n",
    "    0.0206, 0.0207, 0.0209, 0.0208, 0.0209\n",
    "\n",
    "\n",
    "    大小角度：\n",
    "    0.0197, 0.0202, 0.0199, 0.0197, 0.0198, 0.0197, 0.0200, 0.0199, 0.0198,\n",
    "    0.0198, 0.0198, 0.0196, 0.0197, 0.0197, 0.0198, 0.0196, 0.0199, 0.0197,\n",
    "    0.0197, 0.0197, 0.0203, 0.0204, 0.0203, 0.0200, 0.0200, 0.0201, 0.0201,\n",
    "    0.0204, 0.0200, 0.0202, 0.0203, 0.0202, 0.0200, 0.0202, 0.0201, 0.0201,\n",
    "    0.0202, 0.0200, 0.0200, 0.0200, 0.0199, 0.0202, 0.0201, 0.0202, 0.0203,\n",
    "    0.0201, 0.0204, 0.0200, 0.0203, 0.0201\n",
    "    \"\"\"\n",
    "    input('卡住')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ATTACK_METHOD = 'NEUROTOXIN'\n",
    "# ATTACK_METHOD = 'EDGE_CASE'\n",
    "ATTACK_METHOD = 'EDGE_CASE'\n",
    "def plotOnce(plotRound: int=24):\n",
    "    if ATTACK_METHOD == 'NEUROTOXIN':\n",
    "        # malicious_clients, cosine_similarity_matrix = fltrust([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN', '../NormalRun/FL_Backdoor_CV/saved_models/Revision_1/fltrust_NEUROTOXIN_09141511-fmnist')\n",
    "        foolsgoldMaliciousIndex, foolsgoldScore2 = calc_maliciousAndCos_justByGrads([plotRound], 'D:/Data-R1/2024-10-28_05-59-22-foolsgold-fmnist_NEUROTOXIN')\n",
    "        fltrustMaliciousIndex, fltrustScore2 = calc_maliciousAndCos_justByGrads([plotRound], 'D:/Data-R1/2024-10-03_06-48-00-fltrust-fmnist_NEUROTOXIN')  # 未修改 - 应与EDGE_CASE一致\n",
    "        # flameMaliciousIndex, flameMaliciousIndex = calc_maliciousAndCos_justByGrads([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-12_23-30-57-flame-fmnist_NEUROTOXIN')\n",
    "        flameMaliciousIndex, flameScore2 = calc_maliciousAndCos_justByGrads([plotRound], 'D:/Data-R1/2024-10-02_12-55-32-flame-fmnist_NEUROTOXIN')\n",
    "        secfftMaliciousIndex, secfftScore2 = calc_maliciousAndCos_justByGrads_SecFFT([plotRound], 'D:/Data-R1/2024-10-30_08-40-55-secfft-fmnist_NEUROTOXIN')\n",
    "\n",
    "        foolsMalicious, foolsLimited = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-28_05-59-22-foolsgold-fmnist_NEUROTOXIN')\n",
    "        fltrustMalicious, fltrustLimited = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-03_06-48-00-fltrust-fmnist_NEUROTOXIN')\n",
    "        flameMalicious, flameLimited = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-02_12-55-32-flame-fmnist_NEUROTOXIN')\n",
    "        secfftLimited = calc_maliciousAndCos_justByGrads_limited_SecFFT([plotRound], 'D:/Data-R1/2024-10-30_08-40-55-secfft-fmnist_NEUROTOXIN')\n",
    "\n",
    "        foolsMalicious2, foolsDirection = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-28_05-59-22-foolsgold-fmnist_NEUROTOXIN', limistedMethod=1)\n",
    "        fltrustMalicious2, fltrustDirection = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-03_06-48-00-fltrust-fmnist_NEUROTOXIN', limistedMethod=1)\n",
    "        flameMalicious2, flameDirection = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-02_12-55-32-flame-fmnist_NEUROTOXIN', limistedMethod=1)\n",
    "        secffDirection = calc_maliciousAndCos_justByGrads_limited_SecFFT([plotRound], 'D:/Data-R1/2024-10-30_08-40-55-secfft-fmnist_NEUROTOXIN', limistedMethod=1)\n",
    "\n",
    "        # foolsgoldScore2 = calc_maliciousAndCos_justByGrads([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-13_23-15-48-foolsgold-fmnist_NEUROTOXIN')\n",
    "        # 不错的有：9\n",
    "    elif ATTACK_METHOD == 'EDGE_CASE':\n",
    "        # TODO: note off next 2 lines\n",
    "        foolsgoldMaliciousIndex, foolsgoldScore2 = calc_maliciousAndCos_justByGrads([plotRound], 'D:/Data-R1/2024-10-28_00-55-46-foolsgold-fmnist_EDGE_CASE')\n",
    "        fltrustMaliciousIndex, fltrustScore2 = fltrust_R1(plotRound, 'D:/Data-R1/2024-10-03_03-09-36-fltrust-fmnist_EDGE_CASE')\n",
    "        flameMaliciousIndex, flameScore2 = flame_R1(plotRound, 'D:/Data-R1/2024-10-02_09-11-28-flame-fmnist_EDGE_CASE')\n",
    "        secfftMaliciousIndex, secfftScore2 = calc_maliciousAndCos_justByGrads_SecFFT([plotRound], 'D:/Data-R1/2024-10-30_04-48-52-secfft-fmnist_EDGE_CASE')\n",
    "\n",
    "        foolsMalicious, foolsLimited = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-28_00-55-46-foolsgold-fmnist_EDGE_CASE')\n",
    "        fltrustMalicious, fltrustLimited = fltrust_R1(plotRound, 'D:/Data-R1/2024-10-03_03-09-36-fltrust-fmnist_EDGE_CASE', limistedMethod=0)\n",
    "        flameMalicious, flameLimited = flame_R1(plotRound, 'D:/Data-R1/2024-10-02_09-11-28-flame-fmnist_EDGE_CASE', limistedMethod=0)\n",
    "        secfftLimited = calc_maliciousAndCos_justByGrads_limited_SecFFT([plotRound], 'D:/Data-R1/2024-10-30_04-48-52-secfft-fmnist_EDGE_CASE')\n",
    "\n",
    "        foolsMalicious2, foolsDirection = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-28_00-55-46-foolsgold-fmnist_EDGE_CASE', limistedMethod=1)\n",
    "        fltrustMalicious2, fltrustDirection = fltrust_R1(plotRound, 'D:/Data-R1/2024-10-03_03-09-36-fltrust-fmnist_EDGE_CASE', limistedMethod=1)\n",
    "        flameMalicious2, flameDirection = flame_R1(plotRound, 'D:/Data-R1/2024-10-02_09-11-28-flame-fmnist_EDGE_CASE', limistedMethod=1)\n",
    "        secffDirection = calc_maliciousAndCos_justByGrads_limited_SecFFT([plotRound], 'D:/Data-R1/2024-10-30_04-48-52-secfft-fmnist_EDGE_CASE', limistedMethod=1)\n",
    "        # 不错的有：4(第二列不错)、24\n",
    "    elif ATTACK_METHOD == 'MR':\n",
    "        foolsgoldMaliciousIndex, foolsgoldScore2 = calc_maliciousAndCos_justByGrads([plotRound], 'D:/Data-R1/2024-10-27_16-25-41-foolsgold-fmnist_MR')\n",
    "        fltrustMaliciousIndex, fltrustScore2 = calc_maliciousAndCos_justByGrads([plotRound], 'D:/Data-R1/2024-10-02_21-40-11-fltrust-fmnist_MR')\n",
    "        flameMaliciousIndex, flameScore2 = calc_maliciousAndCos_justByGrads([plotRound], 'D:/Data-R1/2024-10-02_03-37-50-flame-fmnist_MR')\n",
    "        secfftMaliciousIndex, secfftScore2 = calc_maliciousAndCos_justByGrads_SecFFT([plotRound], 'D:/Data-R1/2024-10-29_22-41-06-secfft-fmnist_MR')\n",
    "\n",
    "        foolsMalicious, foolsLimited = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-27_16-25-41-foolsgold-fmnist_MR')\n",
    "        fltrustMalicious, fltrustLimited = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-02_21-40-11-fltrust-fmnist_MR')\n",
    "        flameMalicious, flameLimited = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-02_03-37-50-flame-fmnist_MR')\n",
    "        secfftLimited = calc_maliciousAndCos_justByGrads_limited_SecFFT([plotRound], 'D:/Data-R1/2024-10-29_22-41-06-secfft-fmnist_MR')\n",
    "\n",
    "        foolsMalicious2, foolsDirection = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-27_16-25-41-foolsgold-fmnist_MR', limistedMethod=1)\n",
    "        fltrustMalicious2, fltrustDirection = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-02_21-40-11-fltrust-fmnist_MR', limistedMethod=1)\n",
    "        flameMalicious2, flameDirection = calc_maliciousAndCos_justByGrads_limited([plotRound], 'D:/Data-R1/2024-10-02_03-37-50-flame-fmnist_MR', limistedMethod=1)\n",
    "        secffDirection = calc_maliciousAndCos_justByGrads_limited_SecFFT([plotRound], 'D:/Data-R1/2024-10-29_22-41-06-secfft-fmnist_MR', limistedMethod=1)\n",
    "        # （总体上都能识别出来，其中）不错的有：7、9、11\n",
    "    else:\n",
    "        print('嘭！沙卡拉卡')\n",
    "    # print(foolsgoldMaliciousIndex)\n",
    "    # print(foolsgoldScore2)\n",
    "    # datas = [foolsgoldScore2] * 12\n",
    "    # datas = [secfftScore2] * 12\n",
    "    # fltrustScore2 -= np.eye(50)\n",
    "    # fltrustLimited -= np.eye(50)\n",
    "    # fltrustDirection -= np.eye(50)\n",
    "    # fltrustScore2 -= np.eye(50) * fltrustScore2.min()\n",
    "    # m = fltrustScore2.min()\n",
    "    # fltrustScore2 = fltrustScore2.cpu().numpy()\n",
    "    # np.fill_diagonal(fltrustScore2, m)\n",
    "    # m = fltrustLimited.min()\n",
    "    # fltrustLimited = fltrustLimited.cpu().numpy()\n",
    "    # np.fill_diagonal(fltrustLimited, m)\n",
    "    # m = fltrustDirection.min()\n",
    "    # fltrustDirection = fltrustDirection.cpu().numpy()\n",
    "    # np.fill_diagonal(fltrustDirection, m)\n",
    "    datas = [foolsgoldScore2, fltrustScore2, flameScore2, secfftScore2,\n",
    "            foolsLimited, fltrustLimited, flameLimited, secfftLimited,\n",
    "            foolsDirection, fltrustDirection, flameDirection, secffDirection,]\n",
    "    foolsgoldMaliciousIndex = [int(x) for x in foolsgoldMaliciousIndex]\n",
    "    fltrustMaliciousIndex = [int(x) for x in fltrustMaliciousIndex]\n",
    "    flameMaliciousIndex = [int(x) for x in foolsgoldMaliciousIndex]\n",
    "    foolsgoldMaliciousIndex.sort()\n",
    "    fltrustMaliciousIndex.sort()\n",
    "    flameMaliciousIndex.sort()\n",
    "    print(foolsgoldMaliciousIndex)\n",
    "    print(fltrustMaliciousIndex)\n",
    "    print(flameMaliciousIndex)\n",
    "    print(secfftMaliciousIndex)\n",
    "\n",
    "    print('----------------')\n",
    "    print(foolsMalicious)\n",
    "    print(fltrustMalicious)\n",
    "    print(flameMalicious)\n",
    "\n",
    "    print('----------------')\n",
    "    print(foolsMalicious2)\n",
    "    print(fltrustMalicious2)\n",
    "    print(flameMalicious2)\n",
    "    print('----------------')\n",
    "\n",
    "    foolsgoldScores = getScores(foolsgoldMaliciousIndex)\n",
    "    fltrustScores = getScores(fltrustMaliciousIndex)\n",
    "    flameScores = getScores(flameMaliciousIndex)\n",
    "    secfftScores = getScores(secfftMaliciousIndex)\n",
    "    print(foolsgoldScores)\n",
    "    print(fltrustScores)\n",
    "    print(flameScores)\n",
    "    print(secfftScores)\n",
    "\n",
    "    plot_detection_heatmaps_3x4(*datas)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "我在jupyter里有一个plotOnce函数，这个函数会执行`plt.savefig`并保存一张图\n",
    "我想调用这个函数25次，如何将25张图拼接到一块并显示到jupyter上？\n",
    "```\n",
    "for i in range(25):\n",
    "    plotOnce(i)\n",
    "```\n",
    "这样的话只会显示最后一张，并且后面的图片会覆盖前面的图片\n",
    "\n",
    "\n",
    "注意，我的plotOnce函数会执行`plt.savefig`并保存一张图到文件中，我不能修改这个函数。\n",
    "\n",
    "\n",
    "进行如下修改：\n",
    "1. 不要绘制成5x5的图像，而绘制成25x1的图像（每张图像都很长，一行只放置一个原始图像）\n",
    "2. 每次调用plotOnce函数都会生成`detection_comparison_results_3x4.pdf`，第二次生成的文件会覆盖第一次的文件\n",
    "\"\"\"\n",
    "# save_dir = 'temp_saved_plots'  # 自定义保存路径\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# for i in range(25):\n",
    "#     plotOnce(i)\n",
    "#     os.rename('detection_comparison_results_3x4.png', os.path.join(save_dir, f'detection_comparison_results_3x4_{i}.png'))\n",
    "#     fig, axs = plt.subplots(25, 1, figsize=(10, 40))  # 25x1 的布局\n",
    "# for i in range(25):\n",
    "#     ax = axs[i]  # 确定当前子图的轴\n",
    "#     img_path = os.path.join(save_dir, f'detection_comparison_results_3x4_{i}.png')\n",
    "#     # images = convert_from_path(img_path)\n",
    "#     # img = images[0]\n",
    "#     img = Image.open(img_path)\n",
    "#     ax.imshow(img)\n",
    "#     ax.axis('off')  # 不显示坐标轴\n",
    "\n",
    "# # 调整布局\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "save_dir = 'temp_saved_plots'  # 自定义保存路径\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i in range(25):\n",
    "    plotOnce(i)\n",
    "    os.rename('detection_comparison_results_3x4.pdf', os.path.join(save_dir, f'detection_comparison_results_3x4_{i}.pdf'))\n",
    "\n",
    "# 我决定不这么拼接了。我已经有了25个PDF，请将其拼接成一个PDF的25页\n",
    "from PyPDF2 import PdfMerger\n",
    "save_dir = 'temp_saved_plots'  # 假设所有 PDF 文件都在这个目录下\n",
    "output_pdf_path = 'merged_output.pdf'  # 输出合并后的 PDF 文件名\n",
    "\n",
    "# 创建 PdfMerger 对象\n",
    "merger = PdfMerger()\n",
    "\n",
    "# 遍历目录中的所有 PDF 文件\n",
    "for i in range(25):\n",
    "    pdf_path = os.path.join(save_dir, f'detection_comparison_results_3x4_{i}.pdf')\n",
    "    merger.append(pdf_path)  # 将 PDF 文件添加到合并器中\n",
    "\n",
    "# 合并并保存最终的 PDF 文件\n",
    "merger.write(output_pdf_path)\n",
    "merger.close()\n",
    "\n",
    "print(f\"合并完成，输出文件: {output_pdf_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
