Ëß£ÈáäËøôÊÆµ‰ª£Á†Å
```
def fltrust(model_updates, param_updates, clean_param_update):
    cos = torch.nn.CosineSimilarity(dim=0)
    g0_norm = torch.norm(clean_param_update)
    weights = []
    for param_update in param_updates:
        weights.append(F.relu(cos(param_update.view(-1, 1), clean_param_update.view(-1, 1))))
    weights = torch.tensor(weights).to(args.device).view(1, -1)
    weights = weights / weights.sum()
    weights = torch.where(weights[0].isnan(), torch.zeros_like(weights), weights)
    nonzero_weights = torch.count_nonzero(weights.flatten())
    nonzero_indices = torch.nonzero(weights.flatten()).flatten()

    print(f'g0_norm: {g0_norm}, '
          f'weights_sum: {weights.sum()}, '
          f'*** {nonzero_weights} *** model updates are considered to be aggregated !')

    normalize_weights = []
    for param_update in param_updates:
        normalize_weights.append(g0_norm / torch.norm(param_update))

    global_update = dict()
    for name, params in model_updates.items():
        if 'num_batches_tracked' in name or 'running_mean' in name or 'running_var' in name:
            global_update[name] = 1 / nonzero_weights * params[nonzero_indices].sum(dim=0, keepdim=True)
        else:
            global_update[name] = torch.matmul(
                weights,
                params * torch.tensor(normalize_weights).to(args.device).view(-1, 1))
    return global_update
```






ËÅîÈÇ¶Â≠¶‰π†ÈôêÂà∂Â§ßÂ∞èÁöÑÊîªÂáª






ÂèØ‰∏çÂèØ‰ª•Â∞ÜÈöêËîΩÊîªÂáªÁöÑÊñπÊ≥ïÁ±ªÂûãÂàÜ‰∏∫ÔºöÈôêÂà∂Â§ßÂ∞è„ÄÅÈôêÂà∂ËßíÂ∫¶„ÄÅÈôêÂà∂Á¨¶Âè∑ÁöÑÊîªÂáª




ÊØèÁßçÊîªÂáªÁ±ªÂûãÂÖ∑‰ΩìÈÉΩÊúâÂì™‰∫õ





ÈôêÂà∂Á¨¶Âè∑ÁöÑÊîªÂáªÂÖ∑‰ΩìÊúâÂì™‰∫õÔºü





ÊàëÊÉ≥Ë¶ÅÁöÑÊòØÊ¢ØÂ∫¶Á¨¶Âè∑ÂíåÊ≠£Â∏∏ÂÆ¢Êà∑Á´ØÁõ∏Âêå‰ªéËÄåÊØîËæÉÈöêËîΩÁöÑÊîªÂáª





ÈòÖËØªËøôÊÆµ‰ª£Á†ÅÔºå‰Ω†Âè™ÈúÄË¶ÅËß£ÈáäËøôÊÆµ‰ª£Á†ÅÊúâÂÖ≥fltrustÁöÑÈÉ®ÂàÜ
```
def train_and_aggregate(self, global_lr, round, root_path):
        # === trained local models ===
        trained_models = dict()
        param_updates = list()
        trained_params = list()

        # path = os.path.join(
        #     os.getcwd(),
        #     f"./{root_path}/participants/{args.dataset}_{args.attack_mode}.csv",
        # )

        # print(f"round: {round}")
        # with open(path, "a") as file:
        #     file.write(f"{round}: ")
        #     file.write(" ".join(map(str, self.participants)) + "\n")
        # Ëé∑ÂæóÂΩìÂâçÂÖ®Â±ÄÊ®°ÂûãÁöÑÂèÇÊï∞
        target_model_params = dict()
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                target_model_params[name] = param.clone().detach().requires_grad_(False)
        target_model_params_list = [
            param for _, param in self.model.named_parameters() if param.requires_grad
        ]
        print(f"participants: {self.participants}")
        for client_id in self.participants:
            local_model = self.helper.local_model
            for name, param in local_model.named_parameters():
                if param.requires_grad:
                    param.data.copy_(target_model_params[name])
            trained_local_model = self.clients[client_id].local_train(
                local_model,
                self.helper,
                self.current_round,
                (self.current_round in self.poison_rounds),
            )
            trained_local_model_params_list = [
                param
                for _, param in trained_local_model.named_parameters()
                if param.requires_grad
            ]
            if args.aggregation_rule.lower() == "fltrust":
                param_updates.append(
                    parameters_to_vector(trained_local_model_params_list)
                    - parameters_to_vector(target_model_params_list)
                )
            elif args.aggregation_rule.lower() == "flame":
                trained_param = (
                    parameters_to_vector(trained_local_model_params_list).detach().cpu()
                )
                trained_params.append(trained_param)
            for name, param in trained_local_model.named_parameters():
                if not param.requires_grad:
                    continue
                if name not in trained_models:
                    trained_models[name] = param.data.view(1, -1).clone()
                else:
                    trained_models[name] = torch.cat(
                        (trained_models[name], param.data.view(1, -1).clone()), dim=0
                    )

        model_updates = dict()
        for (name, param), local_param in zip(
            target_model_params.items(), trained_models.values()
        ):
            model_updates[name] = local_param.data - param.data.view(1, -1)  # ÂπøÊí≠Êú∫Âà∂

            if args.attack_mode.lower() in [
                "mr",
                "dba",
                "flip",
                "edge_case",
                "neurotoxin",
                "combine",
            ]:
                if "num_batches_tracked" not in name:
                    for i in range(args.participant_sample_size):
                        if self.clients[self.participants[i]].malicious:
                            mal_boost = 1
                            if args.is_poison:
                                if args.mal_boost:
                                    if args.attack_mode.lower() in [
                                        "mr",
                                        "flip",
                                        "edge_case",
                                        "neurotoxin",
                                        "combine",
                                    ]:
                                        mal_boost = (
                                            args.mal_boost / args.number_of_adversaries
                                        )
                                    elif args.attack_mode.lower() == "dba":
                                        mal_boost = args.mal_boost / (
                                            args.number_of_adversaries
                                            / args.dba_trigger_num
                                        )
                                else:
                                    if args.attack_mode.lower() in [
                                        "mr",
                                        "flip",
                                        "edge_case",
                                        "neurotoxin",
                                        "combine",
                                    ]:
                                        mal_boost = (
                                            args.participant_sample_size
                                            / args.number_of_adversaries
                                        )
                                    elif args.attack_mode.lower() == "dba":
                                        mal_boost = args.participant_sample_size / (
                                            args.number_of_adversaries
                                            / args.dba_trigger_num
                                        )
                            model_updates[name][i] *= mal_boost / args.global_lr

        # ‰øùÂ≠òÊ®°ÂûãÊ¢ØÂ∫¶
        path = os.path.join(
            os.getcwd(),
            f"./{root_path}/model_updates/{args.dataset}_{args.attack_mode}_{round}.pkl",
        )
        with open(path, "wb") as file:
            pickle.dump(model_updates, file)

        # Âà§Êñ≠ÂâçÂêé‰∏§Ë°åÊòØ‰∏çÊòØÁõ∏Á≠â
        # first_layers = trained_models.values()[0]
        # is_all_rows_same = (first_layers[0] == first_layers).all(dim=1).all()
        # print(f"all rows are same: {is_all_rows_same}")

        # === aggregate ===
        global_update = None
        if args.aggregation_rule.lower() == "secfft":
            global_update = secfft(model_updates)
        elif args.aggregation_rule.lower() == "avg":
            global_update = avg(model_updates)
        # elif args.aggregation_rule.lower() == 'fedcie':
        #     global_update = Server.fedcie(model_updates, previous_model_update, last_model_params)
        elif args.aggregation_rule.lower() == "roseagg":
            global_update = roseagg(model_updates, current_round=self.current_round)
        elif args.aggregation_rule.lower() == "foolsgold":
            global_update = foolsgold(model_updates)
        elif args.aggregation_rule.lower() == "flame":
            current_model_param = (
                parameters_to_vector(target_model_params_list).detach().cpu()
            )
            global_param, global_update = flame(
                trained_params, current_model_param, model_updates
            )
            vector_to_parameters(global_param, target_model_params_list)
            return
        elif args.aggregation_rule.lower() == "fltrust":
            if self.current_round > 500:
                lr = args.local_lr * args.local_lr_decay ** (
                    (self.current_round - 500) // args.decay_step
                )
            else:
                lr = args.local_lr
            model = copy.deepcopy(self.model)
            for name, param in model.named_parameters():
                if param.requires_grad:
                    param.data.copy_(target_model_params[name])

            optimizer = torch.optim.Adam(
                model.parameters(),
                lr=1e-3,
                betas=(0.9, 0.98),
                eps=1e-6,
                weight_decay=0.2,
            )
            epochs = self.helper.params["retrain_no_times"]
            criterion = torch.nn.CrossEntropyLoss()
            text_inputs = [
                f"This is a photo of a {label}" for label in self.helper.classes
            ]
            for _ in range(epochs):
                for inputs, labels in self.root_dataset:
                    optimizer.zero_grad()
                    clip_inputs = self.helper.processor(
                        text=text_inputs,
                        images=inputs,
                        return_tensors="pt",
                        padding=True,
                        do_rescale=False,
                    ).to(args.device)
                    labels = labels.to(args.device)
                    outputs = model(**clip_inputs)
                    logits_per_image = outputs.logits_per_image
                    loss = criterion(logits_per_image, labels)
                    loss.backward()
                    optimizer.step()
            model_param_list = [
                param for _, param in model.named_parameters() if param.requires_grad
            ]
            clean_param_update = parameters_to_vector(
                model_param_list
            ) - parameters_to_vector(target_model_params_list)

            global_update = fltrust(model_updates, param_updates, clean_param_update)
        elif args.aggregation_rule.lower() == "rlr":
            global_update = robust_lr(model_updates)

        # === update the global model ===
        with torch.no_grad():
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    param.data = param.data + global_lr * global_update[name].view(
                        param.size()
                    )
```





```
    print(model_updates.keys())
    print(type(model_updates.keys()))
```
```
dict_keys(['base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.0.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.1.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.1.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.2.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.2.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.3.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.3.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.4.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.4.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.5.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.5.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.6.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.6.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.7.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.7.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.8.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.8.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.9.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.9.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.10.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.10.mlp.fc2.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.mlp.fc1.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.mlp.fc1.lora_B.default.weight', 'base_model.model.vision_model.encoder.layers.11.mlp.fc2.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.11.mlp.fc2.lora_B.default.weight', 'base_model.model.visual_projection.lora_A.default.weight', 'base_model.model.visual_projection.lora_B.default.weight'])
<class 'dict_keys'>
```
Ëß£Èáädict_keysÔºå‰ª•ÂèäÂ¶Ç‰Ωï‰ªé‰∏≠ÂèñÂÄº







```
def fltrust_original(model_updates, param_updates, clean_param_update):
    cos = torch.nn.CosineSimilarity(dim=0)
    g0_norm = torch.norm(clean_param_update)
    weights = []
    for param_update in param_updates:
        weights.append(F.relu(cos(param_update.view(-1, 1), clean_param_update.view(-1, 1))))
    weights = torch.tensor(weights).to('cuda:0').view(1, -1)
    weights = weights / weights.sum()
    weights = torch.where(weights[0].isnan(), torch.zeros_like(weights), weights)
    nonzero_weights = torch.count_nonzero(weights.flatten())
    nonzero_indices = torch.nonzero(weights.flatten()).flatten()

    print(f'g0_norm: {g0_norm}, '
          f'weights_sum: {weights.sum()}, '
          f'*** {nonzero_weights} *** model updates are considered to be aggregated !')

    normalize_weights = []
    for param_update in param_updates:
        normalize_weights.append(g0_norm / torch.norm(param_update))

    global_update = dict()
    for name, params in model_updates.items():
        if 'num_batches_tracked' in name or 'running_mean' in name or 'running_var' in name:
            global_update[name] = 1 / nonzero_weights * params[nonzero_indices].sum(dim=0, keepdim=True)
        else:
            global_update[name] = torch.matmul(
                weights,
                params * torch.tensor(normalize_weights).to('cuda:0').view(-1, 1))
    return global_update
```

ÂÜô‰∏Ä‰∏™ÂáΩÊï∞`fltrust(roundsNum: List[int], dirPath: str, modelPath: str)`Ôºå‰æãÂ¶Ç`fltrust([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN', '../NormalRun/FL_Backdoor_CV/saved_models/Revision_1/fltrust_NEUROTOXIN_09141511-fmnist')`
ÂÖ∂‰∏≠roundsNumÊòØË¶ÅÈÄâÂèñÁöÑËΩÆÊ¨°ÔºåÊØè‰∏™ËΩÆÊ¨°Êúâ10‰∏™ÂÆ¢Êà∑Á´ØÔºåÊàëÈÄâÊã©`[15, 16, 17, 18, 19]`Ëøô5‰∏™ËΩÆÊ¨°ÁöÑËØùÔºåÁõ∏ÂΩì‰∫éÊääËøô5‰∏™ËΩÆÊ¨°ÊãºÊé•Ëµ∑Êù•ÔºåËßÜ‰∏∫Âè™Êúâ‰∏Ä‰∏™ËΩÆÊ¨°ÔºåÂπ∂‰∏îËøô‰∏™ËΩÆÊ¨°Êúâ50‰∏™ÂÆ¢Êà∑Á´Ø„ÄÇÊØè‰∏™ËΩÆÊ¨°‰∏≠0-2Âè∑ÊòØÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØÔºå3-9Âè∑ÊòØËâØÊÄßÂÆ¢Êà∑Á´Ø„ÄÇÊãºÊé•ÁöÑÊó∂ÂÄôÊ≥®ÊÑèÊääÊÅ∂ÊÑèÁöÑÊãºÊé•Âà∞‰∏ÄÂùóÔºåËâØÊÄßÁöÑÊãºÊé•Âà∞‰∏ÄÂùóÔºåÂèòÊàê0-14Âè∑ÊòØÊÅ∂ÊÑèÔºå15-49Âè∑ÊòØËâØÊÄß„ÄÇ

ËØ∑‰Ω†Ë°•ÂÖ®`fltrust`ÂáΩÊï∞Ôºå‰Ω†‰πüÂèØ‰ª•‰øÆÊîπ`fltrust_original`ÂáΩÊï∞ÔºåËøîÂõû‰∏§‰∏™Êï∞ÊçÆÔºö
1. fltrustÁöÑÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØÁöÑËØÜÂà´ÁªìÊûúÔºàËøîÂõûÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØÁöÑÁºñÂè∑ÂàóË°®Ôºâ„ÄÇËã•fltrustÂπ∂Ê≤°ÊúâÊòéÁ°ÆÊåáÂá∫Âì™‰∫õÊòØËâØÊÄßÂÆ¢Êà∑Á´ØÔºåÂì™‰∫õÊòØÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØÔºåÂàô‰æùÊçÆfltrustÁöÑÂéüÁêÜÂÜô‰∏Ä‰∏™ËØÜÂà´ÂáΩÊï∞„ÄÇ
2. 50‰∏™ÂÆ¢Êà∑Á´ØÁöÑËØÑÂàÜ„ÄÇÂèØ‰ª•ÊòØ1x50ÁöÑÊï∞ÁªÑÔºå‰πüÂèØ‰ª•ÊòØ50x50ÁöÑÊï∞ÁªÑ„ÄÇ

ÊàëÁöÑÊï∞ÊçÆÊñá‰ª∂ÁªìÊûÑÂ¶Ç‰∏ãÔºö
1. `dirPath`‰æãÂ¶Ç`../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN`Êñá‰ª∂Â§π‰∏ãÊúâ3‰∏™Â≠êÊñá‰ª∂Â§πÔºåÂàÜÂà´ÊòØ`clean_param_updates`„ÄÅ`model_updates`„ÄÅ`participants`„ÄÇÂÖ∂‰∏≠`clean_param_updates`Êñá‰ª∂Â§π‰∏ãÊòØÊØè‰∏™ÂéüÂßãËΩÆÊ¨°ÁöÑ`clean_param_update`ÔºåÂëΩÂêçÊòØ`fmnist_NEUROTOXIN_0.pkl`„ÄÅ`fmnist_NEUROTOXIN_1.pkl`„ÄÅ......ÔºõÂÖ∂‰∏≠`model_updates`Êñá‰ª∂Â§π‰∏ãÊòØÊØè‰∏™ÂéüÂßãËΩÆÊ¨°ÁöÑ10‰∏™ÂÆ¢Êà∑Á´ØÊõ¥Êñ∞ÔºåÂëΩÂêçÊòØ`fmnist_NEUROTOXIN_0.pkl`„ÄÅ......ÔºõÂÖ∂‰∏≠`participants`Êñá‰ª∂Â§π‰∏ãÊúâ`participants.csv`Ëøô1‰∏™Êñá‰ª∂ÔºåÂèØ‰ª•Áõ¥Êé•Ë∞ÉÁî®`getParticipants(roundNum: int, dirPath: str) -> np.ndarray:`ÂáΩÊï∞Ëé∑ÂèñÂèÇ‰∏éËÄÖÂàóË°®Ôºå‰æãÂ¶Ç`participants_thisRound = getParticipants(roundNum, dirPath)`ÔºåËøôÊ†∑ÁöÑËØùmodel_updatesÊØè‰∏™ÈîÆÂÄºÂØπÂ∫îÁöÑ`10xÂèÇÊï∞‰∏™Êï∞`ÁöÑÊï∞ÁªÑ‰∏≠ÔºåÁ¨¨`i`‰∏™`ÂèÇÊï∞‰∏™Êï∞`Êï∞ÁªÑÂØπÂ∫îÂÆ¢Êà∑Á´ØÁºñÂè∑‰∏∫`participants_thisRound[i]`„ÄÇ

Â¶ÇÊûú‰Ω†ÈúÄË¶ÅÂì™‰∫õÂÜÖÂÆπÊàñËÄÖÊï∞ÊçÆÔºåËØ∑ÂèäÊó∂ÂëäËØâÊàë„ÄÇ
Â¶ÇÊûú‰Ω†ÊúâÂì™‰∫õ‰∏çÁêÜËß£ÁöÑÂú∞ÊñπÔºåËØ∑ÂèäÊó∂ÂêëÊàëËØ¢ÈóÆ„ÄÇ
Âú®‰Ω†ÂÆåÂÖ®ÁêÜËß£ÊàëÁöÑÊÑèÂõæÂêéÔºåÂú®‰Ω†ÂèñÂæóË∂≥Â§üÁöÑ‰ø°ÊÅØÂêéÔºåÂÜçÂºÄÂßãÁºñÂÜô‰ª£Á†Å„ÄÇ






GPTÁöÑÂõûÁ≠îÔºåÈùûÊàëÊâÄÈóÆÔºåÂæàÂáÜÁ°Æüëá
```
ÊÑüË∞¢‰Ω†Êèê‰æõËØ¶ÁªÜÁöÑ‰ø°ÊÅØÔºÅ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞ÂÆåÊàêËøô‰∏™‰ªªÂä°ÔºåËÆ©ÊàëÂÖàÊÄªÁªì‰∏Ä‰∏ãÊàëÂØπ‰Ω†ÁöÑÈúÄÊ±ÇÁöÑÁêÜËß£ÔºåÂπ∂Á°ÆËÆ§ÊàëÊòØÂê¶ÂÆåÂÖ®ÁêÜËß£‰∫ÜË¶ÅÊ±Ç„ÄÇ

ÁêÜËß£Ê¶ÇË¶Å
ËæìÂÖ•ÂèÇÊï∞:

roundsNum: List[int]: ‰∏Ä‰∏™ÂåÖÂê´Ë¶ÅÈÄâÂèñËΩÆÊ¨°ÁºñÂè∑ÁöÑÂàóË°®ÔºåÊØè‰∏™ËΩÆÊ¨°Êúâ 10 ‰∏™ÂÆ¢Êà∑Á´Ø„ÄÇ‰æãÂ¶ÇÔºå[15, 16, 17, 18, 19] Ë°®Á§∫ÈÄâÊã©ËΩÆÊ¨° 15 Âà∞ 19„ÄÇ
dirPath: str: ‰∏Ä‰∏™ÁõÆÂΩïË∑ØÂæÑÔºåÂåÖÂê´‰∏â‰∏™Â≠êÊñá‰ª∂Â§πÔºåÂàÜÂà´ÊòØ clean_param_updates„ÄÅmodel_updates„ÄÅparticipants„ÄÇ
modelPath: str: ‰∏Ä‰∏™Áî®‰∫éÂ≠òÂÇ®Ê®°ÂûãÊàñÁõ∏ÂÖ≥Êï∞ÊçÆÁöÑË∑ØÂæÑ„ÄÇ
Êï∞ÊçÆÁªìÊûÑ:

Âú® dirPath ‰∏ãÊúâ‰∏â‰∏™Â≠êÊñá‰ª∂Â§πÔºö
clean_param_updatesÔºöÂåÖÂê´ÊØè‰∏™ËΩÆÊ¨°ÁöÑ clean_param_updateÔºåÊñá‰ª∂ÂëΩÂêçÊ†ºÂºè‰∏∫ fmnist_NEUROTOXIN_0.pkl„ÄÅfmnist_NEUROTOXIN_1.pkl Á≠â„ÄÇ
model_updatesÔºöÂåÖÂê´ÊØè‰∏™ËΩÆÊ¨° 10 ‰∏™ÂÆ¢Êà∑Á´ØÁöÑÊ®°ÂûãÊõ¥Êñ∞ÔºåÊØè‰∏™Êñá‰ª∂ÂåÖÂê´‰∏Ä‰∏™ 10xÂèÇÊï∞‰∏™Êï∞ ÁöÑÊï∞ÁªÑ„ÄÇ
participantsÔºöÂåÖÂê´‰∏Ä‰∏™ participants.csv Êñá‰ª∂ÔºåÂèØ‰ΩøÁî® getParticipants(roundNum, dirPath) ÂáΩÊï∞Êù•Ëé∑ÂèñÁâπÂÆöËΩÆÊ¨°ÁöÑÂèÇ‰∏éËÄÖÂàóË°®„ÄÇ
ÊãºÊé•Êï∞ÊçÆ:

ÈÄâÂèñÁöÑËΩÆÊ¨°Ôºà‰æãÂ¶ÇÔºå5 ‰∏™ËΩÆÊ¨°ÔºåÊØèËΩÆ 10 ‰∏™ÂÆ¢Êà∑Á´ØÔºâÊãºÊé•Êàê‰∏Ä‰∏™Êñ∞ÁöÑ‚ÄúËΩÆÊ¨°‚ÄùÔºåËøô‰∏™Êñ∞ËΩÆÊ¨°ÂåÖÂê´ 50 ‰∏™ÂÆ¢Êà∑Á´ØÔºåÂÖ∂‰∏≠Ââç 15 ‰∏™Ôºà0-14 Âè∑ÔºâÊòØÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØÔºåÂêé 35 ‰∏™Ôºà15-49 Âè∑ÔºâÊòØËâØÊÄßÂÆ¢Êà∑Á´Ø„ÄÇ
ÈúÄË¶ÅË°•ÂÖ®ÁöÑ fltrust ÂáΩÊï∞:

ËØ•ÂáΩÊï∞ÈúÄË¶ÅË∞ÉÁî® fltrust_original Êù•ËøõË°åËÅöÂêàÔºåÂπ∂Ê†πÊçÆ fltrust ÂéüÁêÜÊù•ËØÜÂà´ÊÅ∂ÊÑèÂÆ¢Êà∑Á´Ø„ÄÇ
ÂáΩÊï∞ËøîÂõû‰∏§‰∏™ÁªìÊûúÔºö
ÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØÁöÑËØÜÂà´ÁªìÊûúÔºàÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØÁöÑÁºñÂè∑ÂàóË°®Ôºâ„ÄÇ
50 ‰∏™ÂÆ¢Êà∑Á´ØÁöÑËØÑÂàÜÔºàÂèØ‰ª•ÊòØ 1x50 ÁöÑÊï∞ÁªÑÊàñ 50x50 ÁöÑÊï∞ÁªÑÔºâ„ÄÇ
Â¶Ç‰ΩïËØÜÂà´ÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØÔºö

‰ΩøÁî® fltrust ËÅöÂêàÁÆóÊ≥ïÁöÑÁªìÊûúÔºåÊ†πÊçÆËÅöÂêàÊùÉÈáçÊàñÁõ∏‰ººÂ∫¶Êù•Âà§Êñ≠Âì™‰∫õÂÆ¢Êà∑Á´ØÊòØÊÅ∂ÊÑèÁöÑ„ÄÇ
ÈúÄË¶ÅÁöÑÁ°ÆËÆ§
Âú®ÊàëÂºÄÂßãÁºñÂÜô‰ª£Á†Å‰πãÂâçÔºåÊàëÊúâÂá†‰∏™ÈúÄË¶ÅÁ°ÆËÆ§ÁöÑÂú∞ÊñπÔºö

fltrust ‰∏≠ÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØËØÜÂà´ÁöÑÊ†áÂáÜÔºöÂú® fltrust_original ÂáΩÊï∞‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî®‰ΩôÂº¶Áõ∏‰ººÂ∫¶Âíå ReLU ÊøÄÊ¥ªÊù•ËÆ°ÁÆóÂÆ¢Êà∑Á´ØÊõ¥Êñ∞ÁöÑÊùÉÈáçÔºåÊâÄÊúâÊùÉÈáçÂä†Âíå‰∏∫ 1„ÄÇÊàëÁöÑÁêÜËß£ÊòØÔºåÂ¶ÇÊûúÊüê‰∫õÂÆ¢Êà∑Á´ØÁöÑÊùÉÈáçÈùûÂ∏∏‰ΩéÔºà‰æãÂ¶ÇÔºå‰Ωé‰∫éÊüê‰∏™ÈòàÂÄºÔºâÔºåÊàë‰ª¨ÂèØ‰ª•ËÆ§‰∏∫ÂÆÉ‰ª¨ÊòØÊÅ∂ÊÑèÁöÑ„ÄÇËØ∑Á°ÆËÆ§Ëøô‰∏ÄÁÇπÔºåÊàñËÄÖÊèê‰æõÊõ¥Â§öÂÖ≥‰∫éÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØËØÜÂà´Ê†áÂáÜÁöÑÁªÜËäÇ„ÄÇ

Êï∞ÊçÆËØªÂèñÂíåÂêàÂπ∂ÁöÑÁªÜËäÇÔºöÊòØÂê¶ÂèØ‰ª•ÂÅáËÆæÊØè‰∏™ clean_param_update Âíå model_update Êñá‰ª∂ÁöÑÊ†ºÂºèÊòØ‰∏ÄËá¥ÁöÑÔºåÂπ∂‰∏îÂèØ‰ª•‰ΩøÁî® pickle.load Áõ¥Êé•Âä†ËΩΩÊàêÂº†ÈáèÊàñÊï∞ÁªÑÊ†ºÂºèÔºü

ËÅöÂêàÂêéÁöÑËØÑÂàÜÊ†ºÂºèÔºöÂØπ‰∫é 50x50 ÁöÑËØÑÂàÜÁü©ÈòµÔºåÊòØÂê¶Â∏åÊúõ‰ª•‰ΩôÂº¶Áõ∏‰ººÂ∫¶Áü©ÈòµÁöÑÂΩ¢ÂºèÊòæÁ§∫ÊØè‰∏™ÂÆ¢Êà∑Á´Ø‰∏éÂÖ∂‰ªñÂÆ¢Êà∑Á´ØÁöÑÁõ∏‰ººÂ∫¶ÔºüÂ¶ÇÊûúÊòØÔºåÈÇ£‰πàÂ∫îÂü∫‰∫éÂì™‰∫õÊï∞ÊçÆËÆ°ÁÆóËøô‰∫õÁõ∏‰ººÂ∫¶Ôºü

ËØ∑Á°ÆËÆ§Ëøô‰∫õÈóÆÈ¢òÔºå‰ª•‰æøÊàëÁªßÁª≠ÁºñÂÜô‰ª£Á†Å
```





‰Ω†ÁöÑÁêÜËß£ÈùûÂ∏∏Ê≠£Á°ÆÔºÅ‰ª•‰∏ãÊòØÈíàÂØπ‰Ω†ÁöÑÁñëÈóÆËøõË°åÁöÑÂõûÁ≠îÔºö
1. fltrust‰∏≠ÊÅ∂ÊÑèÂÆ¢Êà∑Á´ØËØÜÂà´ÁöÑÊ†áÂáÜÔºöfltrust‰ºº‰πéÂπ∂Ê≤°ÊúâÁªôÂá∫ÊòéÁ°ÆÁöÑËÅöÁ±ªÊñπÂºèÔºåÂõ†Ê≠§‰Ω†ÂèØ‰ª•Ëá™ÂÆö‰πâËÅöÁ±ªÊñπÂºèÔºåÊé®Ëçê‰ΩøÁî®`KMeans`ËøõË°åËÅöÁ±ª„ÄÇ
2. Êï∞ÊçÆËØªÂèñÂíåÂêàÂπ∂ÁöÑÁªÜËäÇÔºöÊØè‰∏™ clean_param_update Âíå model_update Êñá‰ª∂ÁöÑÊ†ºÂºèÊòØ‰∏ÄËá¥ÁöÑÔºå‰ª•‰ΩøÁî® pickle.load Áõ¥Êé•Âä†ËΩΩÊàêÂº†ÈáèÊàñÂ≠óÂÖ∏Ôºå‰æãÂ¶ÇÔºö
```
def loadPkl(roundNum: int, dirPath: str) -> Dict[str, torch.Tensor]:  # ÊääpklÂèòÊàê{'key1': [[user3], [user5]]}
    # dirPath = '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-12_23-26-43-avg-fmnist_NEUROTOXIN/'
    pklPrefix = os.path.basename(os.path.normpath(dirPath)).split('-')[-1]
    pklName = os.path.join(dirPath, f'model_updates/{pklPrefix}_{roundNum}.pkl')
    with open(pklName, 'rb') as f:
        update: Dict[str, torch.Tensor] = pickle.load(f)
    return update
model_updates = loadPkl(roundsNum[th], dirPath)
keys = list(model_updates.keys())
print(keys)
firstKey = keys[0]
firstValue = model_updates[firstKey]
print(type(firstValue))
print(firstValue.shape)
```
ÁöÑËøêË°åÁªìÊûú‰∏∫
```
['base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', ...]
<class 'torch.Tensor'>
torch.Size([10, 12288])
```
ÂÖ∂‰∏≠`firstValue[i]`ÁöÑclientÁºñÂè∑ÊòØ`participants_thisRound[i]`ÔºåÂÖ∂‰ªñvalues‰πü‰∏ÄËá¥„ÄÇ
3. ËÅöÂêàÂêéÁöÑËØÑÂàÜÊ†ºÂºèÔºöÂèØ‰ª•‰ΩøÁî®‰ΩôÂº¶Áõ∏‰ººÂ∫¶ËøõË°åËÆ°ÁÆóÔºåÂÖ≥‰∫éÂü∫‰∫éÂì™‰∫õÊï∞ÊçÆÔºåËØ∑‰Ω†Âú®fltrustÁöÑËÆ°ÁÆóËøáÁ®ã‰∏≠ÔºåÂØªÊâæ‰∏Ä‰∏™ÂêàÈÄÇÁöÑÊï∞ÊçÆËøõË°åËÆ°ÁÆó„ÄÇ







Êä•Èîô`FileNotFoundError: [Errno 2] No such file or directory: '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN\\clean_param_updates\\model_updates/clean_param_updates_15.pkl'`

ÂÖ∂ÂÆûÁúüÊ≠£ÁöÑË∑ØÂæÑÊòØ`../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN/clean_param_updates/clean_param_updates_15.pkl`

‰Ω†Â§öÂÜô‰∫Ü‰∏™`model_updates`





Êä•Èîô
```
malicious_clients, cosine_similarity_matrix = fltrust([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN', '../NormalRun/FL_Backdoor_CV/saved_models/Revision_1/fltrust_NEUROTOXIN_09141511-fmnist')

# foolsgoldMaliciousIndex, foolsgoldScore2 = foolsgold([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-13_23-15-48-foolsgold-fmnist_NEUROTOXIN')
# print(foolsgoldMaliciousIndex)
# print(foolsgoldScore2)
# datas = [foolsgoldScore2] * 12
# plot_detection_heatmaps_3x4(*datas)
```



```
# fltrust
# https://github.com/LetMeFly666/SecFFT/blob/706bb287c3b00f6143e2190edc74714ed88f3532/getResult/FL_Backdoor_CV/roles/aggregation_rules.py#L301-L340

def fltrust_original(model_updates, param_updates, clean_param_update):
    cos = torch.nn.CosineSimilarity(dim=0)
    g0_norm = torch.norm(clean_param_update)
    weights = []
    for param_update in param_updates:
        weights.append(F.relu(cos(param_update.view(-1, 1), clean_param_update.view(-1, 1))))
    weights = torch.tensor(weights).to('cuda:0').view(1, -1)
    weights = weights / weights.sum()
    weights = torch.where(weights[0].isnan(), torch.zeros_like(weights), weights)
    nonzero_weights = torch.count_nonzero(weights.flatten())
    nonzero_indices = torch.nonzero(weights.flatten()).flatten()

    print(f'g0_norm: {g0_norm}, '
          f'weights_sum: {weights.sum()}, '
          f'*** {nonzero_weights} *** model updates are considered to be aggregated !')

    normalize_weights = []
    for param_update in param_updates:
        normalize_weights.append(g0_norm / torch.norm(param_update))

    global_update = dict()
    for name, params in model_updates.items():
        if 'num_batches_tracked' in name or 'running_mean' in name or 'running_var' in name:
            global_update[name] = 1 / nonzero_weights * params[nonzero_indices].sum(dim=0, keepdim=True)
        else:
            global_update[name] = torch.matmul(
                weights,
                params * torch.tensor(normalize_weights).to('cuda:0').view(-1, 1))
    return global_update

def fltrust(roundsNum: List[int], dirPath: str, modelPath: str):
    clientPerRound = 10  # ËøôÈáåÂ∞±ÂÖàÂÜôÊ≠ª‰∫Ü
    maliciousPerRound = 3
    # gradients = [0] * len(roundsNum) * clientPerRound
    # maliciouses = []
    gradientsList = []  # ÈáåÈù¢Â≠òÊîæÊØè‰∏ÄËΩÆÁöÑÊ¢ØÂ∫¶ÔºåÊúÄÂêéÂÜçËÅöÂêà
    for th, roundNum in enumerate(roundsNum):
        participants_thisRound = getParticipants(roundNum, dirPath)
        # get model updates
        pklName = f'../NormalRun/FL_Backdoor_CV/saved_models/Revision_1/fltrust_NEUROTOXIN_09141511-fmnist/fltrust_{roundNum}.pth'
        # print(pklName)
        with open(pklName, 'rb') as f:
            model_updates: Dict[str, torch.Tensor] = torch.load(f)
        # print(model_updates)
        pklName = f'../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN/model_updates/fmnist_NEUROTOXIN_{roundNum}.pkl'
        with open(pklName, 'rb') as f:
            param_updates: Dict[str, torch.Tensor] = pickle.load(f)
        param_updates = torch.cat(list(param_updates.values()), dim=1)
        # print(param_updates[list(param_updates.keys())[0]].shape)  # torch.Size([10, 12288])
        pklName = f'../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN/clean_param_updates/fmnist_NEUROTOXIN_{roundNum}.pkl'
        with open(pklName, 'rb') as f:
            clean_param_update: torch.Tensor = pickle.load(f)
        
        # global_update = fltrust_original(model_updates, [param_updates[key] for key in param_updates.keys()], clean_param_update)
        global_update = fltrust_original(model_updates, param_updates, clean_param_update)

        # print(clean_param_update.shape)  # torch.Size([2674688])
        thisGradients = [0] * clientPerRound


        gradientsList.append(thisGradients)
        break
malicious_clients, cosine_similarity_matrix = fltrust([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN', '../NormalRun/FL_Backdoor_CV/saved_models/Revision_1/fltrust_NEUROTOXIN_09141511-fmnist')
```
Êä•Èîô
```
{
	"name": "AttributeError",
	"message": "'CLIPModel' object has no attribute 'items'",
	"stack": "---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
File c:\\ProgramData\\Anaconda3\\envs\\clip_lora\\lib\\site-packages\\peft\\peft_model.py:737, in PeftModel.__getattr__(self, name)
    736 try:
--> 737     return super().__getattr__(name)  # defer to nn.Module's logic
    738 except AttributeError:

File c:\\ProgramData\\Anaconda3\\envs\\clip_lora\\lib\\site-packages\\torch\
n\\modules\\module.py:1729, in Module.__getattr__(self, name)
   1728         return modules[name]
-> 1729 raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")

AttributeError: 'PeftModel' object has no attribute 'items'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
File c:\\ProgramData\\Anaconda3\\envs\\clip_lora\\lib\\site-packages\\peft\\tuners\\lora\\model.py:356, in LoraModel.__getattr__(self, name)
    355 try:
--> 356     return super().__getattr__(name)  # defer to nn.Module's logic
    357 except AttributeError:

File c:\\ProgramData\\Anaconda3\\envs\\clip_lora\\lib\\site-packages\\torch\
n\\modules\\module.py:1729, in Module.__getattr__(self, name)
   1728         return modules[name]
-> 1729 raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")

AttributeError: 'LoraModel' object has no attribute 'items'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
c:\\Users\\admin\\Desktop\\LLM\\wb2\\Codes\\getResult\\getUpdates.ipynb Cell 10 line 1
----> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a> malicious_clients, cosine_similarity_matrix = fltrust([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-14_15-11-15-fltrust-fmnist_NEUROTOXIN', '../NormalRun/FL_Backdoor_CV/saved_models/Revision_1/fltrust_NEUROTOXIN_09141511-fmnist')
      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a> # foolsgoldMaliciousIndex, foolsgoldScore2 = foolsgold([15, 16, 17, 18, 19], '../NormalRun/FL_Backdoor_CV/resultWithTime/2024-09-13_23-15-48-foolsgold-fmnist_NEUROTOXIN')
      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a> # print(foolsgoldMaliciousIndex)
      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a> # print(foolsgoldScore2)
      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a> # datas = [foolsgoldScore2] * 12
      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a> # plot_detection_heatmaps_3x4(*datas)

c:\\Users\\admin\\Desktop\\LLM\\wb2\\Codes\\getResult\\getUpdates.ipynb Cell 10 line 5
     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>     clean_param_update: torch.Tensor = pickle.load(f)
     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=56'>57</a> # global_update = fltrust_original(model_updates, [param_updates[key] for key in param_updates.keys()], clean_param_update)
---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a> global_update = fltrust_original(model_updates, param_updates, clean_param_update)
     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=59'>60</a> # print(clean_param_update.shape)  # torch.Size([2674688])
     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=60'>61</a> thisGradients = [0] * clientPerRound

c:\\Users\\admin\\Desktop\\LLM\\wb2\\Codes\\getResult\\getUpdates.ipynb Cell 10 line 2
     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>     normalize_weights.append(g0_norm / torch.norm(param_update))
     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a> global_update = dict()
---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a> for name, params in model_updates.items():
     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>     if 'num_batches_tracked' in name or 'running_mean' in name or 'running_var' in name:
     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/LLM/wb2/Codes/getResult/getUpdates.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>         global_update[name] = 1 / nonzero_weights * params[nonzero_indices].sum(dim=0, keepdim=True)

File c:\\ProgramData\\Anaconda3\\envs\\clip_lora\\lib\\site-packages\\peft\\peft_model.py:741, in PeftModel.__getattr__(self, name)
    739 if name == \"base_model\":  # see #1892: prevent infinite recursion if class is not initialized
    740     raise
--> 741 return getattr(self.base_model, name)

File c:\\ProgramData\\Anaconda3\\envs\\clip_lora\\lib\\site-packages\\peft\\tuners\\lora\\model.py:360, in LoraModel.__getattr__(self, name)
    358 if name == \"model\":  # see #1892: prevent infinite recursion if class is not initialized
    359     raise
--> 360 return getattr(self.model, name)

File c:\\ProgramData\\Anaconda3\\envs\\clip_lora\\lib\\site-packages\\torch\
n\\modules\\module.py:1729, in Module.__getattr__(self, name)
   1727     if name in modules:
   1728         return modules[name]
-> 1729 raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")

AttributeError: 'CLIPModel' object has no attribute 'items'"
}
```