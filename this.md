*Input-Aware Dynamic Backdoor Attack* 382次引用，传统的后门攻击方法通常使用统一的触发器模式，即所有恶意样本使用相同的触发器。这种固定模式虽然有效，但容易被当前的防御方法检测到并减轻其影响。许多防御技术通过寻找这些固定触发器来识别和缓解后门攻击。为了提高攻击的隐蔽性，文章提出了一种输入感知动态后门攻击（Input-Aware Dynamic Backdoor Attack）。在这种方法中，触发器是根据每个输入动态生成的，而不是固定不变的。这意味着不同的输入图像会有不同的触发器，从而打破了现有防御方法的基础假设，使攻击更难被检测到。 1）触发器生成器：文章设计了一个基于自编码器（autoencoder）的触发器生成器，它根据输入图像生成相应的触发器。生成的触发器具有显著的多样性，确保不同输入图像的触发器彼此不同。2）交叉触发测试：为了进一步提高触发器的隐蔽性，研究者引入了一种新的测试方法，称为交叉触发测试（cross-trigger test）。通过该测试，确保为一个输入生成的触发器无法在其他输入上重用，从而进一步提高了攻击的隐蔽性。3）训练目标函数：文章结合了分类损失（classification loss）和多样性损失（diversity loss），以确保生成的触发器既能有效激活后门，又能在不同输入间保持足够的差异性。  实验结果表明，这种动态后门攻击在MNIST、CIFAR-10和GTSRB等标准数据集上取得了接近100%的攻击成功率，并且在使用现有最先进的防御方法时仍然能够成功绕过检测。文章还证明了该方法在图像正则化和网络检查工具（如GradCam）下的稳健性，这进一步验证了这种攻击的隐蔽性。  动态后门，变来变去，多次结合起来才能更好地看出目的。

*Efficient and persistent backdoor attack by boundary trigger set constructing against federated learning*  之前的backdoor方法通常从训练数据集中随机选择触发候选样本，这种做法容易扰乱样本分布，并模糊它们之间的边界，导致主要任务的准确性下降。此外，这些方法使用的触发器通常是手工制作且未经过优化，导致后门映射关系较弱，攻击成功率较低。  为了解决这些问题，本文提出了一种灵活的后门攻击方法，称为触发样本选择与优化（Trigger Sample Selection and Optimization, TSSO）。这一方法受到神经网络分类模式的启发，利用自编码器（Autoencoders）和局部敏感哈希（Locality-Sensitive Hashing）来选择在类边界处的触发候选样本，从而实现精确注入。此外，TSSO通过全局模型和历史结果迭代优化触发器的表示，从而建立一个稳健的映射关系。 文章在四个经典数据集上评估了TSSO方法，特别是在非独立同分布（non-IID）的设置下，TSSO在更少的训练轮次中取得了更高的攻击成功率，并延长了后门攻击的效果。即使在扩展性测试中，部署防御措施的情况下，TSSO仍然能够在只有4%恶意客户端（中毒率为1/640）的情况下达到超过80%的攻击成功率。这展示了TSSO在后门攻击中的高效性和持久性。 相当于是把单次的攻击变地和正常的训练很类似，因此难以识别。

*Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning* CCFA，研究指出，在联邦学习中，由于后续的正常更新，后门攻击的效果会逐渐减弱，这表现为攻击成功率在多轮迭代中显著下降，最终可能完全失效。为了量化这种现象，文章引入了一个新的指标——攻击持久性（Attack Persistence），用于衡量后门攻击效果的衰减程度。在前人研究未能广泛探讨如何提高攻击持久性的背景下，作者提出了FCBA方法。该方法通过聚合更多的触发信息，生成更完整的后门模式，从而在全局模型中更好地植入后门。经过训练的后门模型对后续的正常更新具有更强的抗性，使得测试集上的攻击成功率更高。作者在三个数据集上对这一方法进行了测试，并在不同的设置下评估了两种模型的表现。结果显示，FCBA的持久性优于现有最先进的联邦学习后门攻击方法。在GTSRB数据集上，经过120轮攻击后，FCBA的攻击成功率较基线提升了50%以上。 核心思想是通过聚合更多的触发信息，生成更复杂和完整的后门模式，从而使得后门在全局模型中植入得更深、更持久。  开源。

*FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information* 提出了一种名为FedRecover的方法，用于在联邦学习（Federated Learning, FL）系统中从中毒攻击中恢复全局模型。FedRecover方法的关键在于利用历史信息来估计客户端的模型更新，而不是在恢复过程中要求客户端重新计算和通信这些更新。这一方法的目的是减少恢复过程中的计算和通信开销，同时保持恢复后的全局模型的准确性。  文章中没有提到是否存储了所有客户端历史上的每次梯度（占据空间过多的问题）。

*Model Poisoning Attacks to Federated Learning via Multi-Round Consistency* 多轮的攻击。PoisonedFL通过引入多轮次一致性（multi-round consistency）和动态攻击幅度调整这两个关键组件，显著提高了攻击效果。该方法不依赖于真实客户端的信息，并且对服务器部署的防御机制具有很强的适应性。 1）多轮次一致性：PoisonedFL通过确保恶意客户端在多个训练轮次中的模型更新方向一致，即使在个别轮次中攻击效果被削弱，累积的攻击效果仍然能够显著地偏移全局模型。2）动态攻击幅度调整：为了避免恶意更新被防御机制完全过滤掉，PoisonedFL动态调整攻击幅度。根据过去轮次的攻击效果，调整恶意更新的强度，以实现攻击的隐蔽性与有效性之间的平衡。  *但是*，这篇文章和我们要检测的攻击正好相反，我们要检测的攻击是那种迂回式攻击，这篇文章的图二说别的攻击可能是“迂回”的，而PoisonedFL结合多轮直奔目标。